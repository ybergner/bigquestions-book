# When and How Will You Die?

> It is difficult to make predictions, especially about the future. 
>
> --- Niels Bohr (probably)

In our first Big Question, we began to look at individual differences between people or what statisticians call variation within a population. If there is no variation---like in the bizarro world where everyone orients their toilet paper in the "under" orientation---then there is nothing to talk about, at least not statistically speaking. There is, however, considerable variation in health outcomes and human lifespan. Lots to talk about there. In our next Big Question, we ask "when and how will you die?" and "what, if anything, can you do about it?"

What kind of question is, "when and how will you die?" Well, according to some of my colleagues, it’s a morbid question. Feelings aside, we might say that it sounds like a prediction question, since it’s about the future. So to explore this big question, we will need to understand what it means in general to make a forecast about some future event. We’ll also find it useful to distinguish between predictions that are or are not explanatory. Most efforts in health sciences attempt to explain relationships between behavioral and genetic factors and health outcomes. In particular, they try to understand causal effects. So in this chpater, we will also try to understand causal explanations more generally.

## Not Quite Death, but, um... Rain?

Perhaps its a good idea to warm up, before we face the grim reaper. What does it mean to say there’s a 30% chance of rain tomorrow in New York? Does it mean that it will definitely rain in 30% of the city (say, Brooklyn), but not in the other 30%? Or that it will rain for 30% of the day (say, from 8am-3pm). Here are some possibilities to consider:

a) It will definitely rain in some parts of the city but not in all of them
b) It will definitely rain for some part of the day in all of the city
c) It will definitely rain for some part of the day in some of the city
d) It may or may not rain anywhere in the city at any point in the day.


Read here for an [explanation of what meteorologists *probably* mean](http://wxbrad.com/why-a-50-chance-of-rain-usually-means-a-100-chance-of-confusion/)


### Stochastic vs Deterministic variables

Sometimes when I say definitely, I mean probably. Like if I say, I'm definitely going to do something about all of this clutter on my desk. But when I really mean business, I say deterministically. It definitely sounds more serious.

Meteorologists---scientists who model the weather---cannot tell us deterministically about weather events. A **deterministic** description of an event would be something like, if I let go of the umbrella I am holding in my hand, it will fall to the ground. If A then B. No exceptions. Weather events are **stochastic**. They have an element of randomness, like tossing a coin or rolling a die. So, just as we can say that a coin has a 50% chance of coming up heads---assuming it is a fair coin---we can make statements like there is a 30% chance that it will rain tomorrow. 


### Ensembles

One way to think about the 30% chance of rain is to imagine that our experience in the world is one possibility in a multiplicity of possible worlds. See, I told you this idea of multiple alternate universes was going to be important!  Imagine that there are 10 possible worlds, indistinguishable from ours in terms of the laws of physics, and that tomorrow it will in fact rain in 3 of them. To the great being-who-knows-all-things, which 3 may well be known. However, to us mortals who merely live in the world, we don’t know which one of these possible worlds is the one we live in. Nevertheless we are capable of imagining these different potential outcomes. As you just did.

It didn't have to be 10 worlds, of course. That was arbitrary. If we imagined thirty worlds, it could rain in nine of them, as I've represented in Figure \@ref(fig:thirty-worlds). I did this by making thirty circles and coloring in 9 of them at random. Since I like to pull back the curtain every once in a while, I will even show you the code I use to generate this simple figure.

```{r thirty-worlds, echo=TRUE, fig.cap='Rain (filled, blue dots) in 9 out of 30 possible worlds. It does not rain (hollow circles) in the other worlds.', fig.height=3}

norain <- cbind(rep(1:10,3), rep(1:3, each=10))  # start with a 10 x 3 grid of points
rainworlds <- norain[sample(1:nrow(norain), 9),]  # choose (sample) nine at random, using the sample() function in R
plot(norain, xlab="", ylab="", ylim = c(1,3), axes = FALSE, asp = 1) # plot the points
points(rainworlds, pch=19, col="lightblue") # color in the nine
```


### Degree of belief

There is another way to think about 30% as a probability. Suppose a meteorologist said to you, I’m 30% sure it is going to rain tomorrow. And you say back, "Oh, you mean that, say there are really 1000 alternate universes out there, that in roughly 300 of them, it will rain tomorrow?" And the meteorologist says, "I have no idea what you're talking about. There is only one universe, and I'm not totally sure what will happen tomorrow, but I put the chances of rain at 30% *[walks away slowly towards the door]*."

For your meteorologist friend, 30% represents a degree of belief. Importantly, the degree of belief is subjective. Here it is attributed to a meteorologist, which might make you take it more seriously than if your Uncle Bob said the same thing (unless Uncle Bob is actually a meteorologist). Anyway, degree of belief is subjective. Which doesn’t mean it’s arbitrary or just a matter of opinion. When it comes to forecasts, some people or some forecasting models are going to be right more often than others. More on that later.


### Decisions


Aside from subjectivity, which is a thorny topic among statisticians, there is really no *practical* difference between the interpretation of 30% probability as a frequency of occurrence in an ensemble of possible worlds or as a degree of belief about this world. It won't change what you do about it.

If you take this forecast of rain seriously, you have decisions to make. It could be whether or not to take an umbrella with you when you leave the house tomorrow, or whether to cancel your plans to have a barbecue outside. These decisions may not seem very high stakes. The worst case scenario is that you (and others at your barbecue) get wet. But other decisions you have to make on a daily basis can have more serious consequences for your health or even your life. You often have to make those decisions based on probabilistic and maybe subjective information.


## Death

End of warm-up. It's time to talk about when you will die.

I highly recommend this data visualization called [Years You Have Left to Live, Probably](https://flowingdata.com/2015/09/23/years-you-have-left-to-live-probably/). Here is a screenshot, although it's not nearly as interesting when you can't interact with the simulation and watch the little balls drop.

```{r years-screen, out.width='90%', fig.cap='Screenshot of interactive data visualization'}
include_graphics("images/YYHLTLScreenshot1.png")
```


<!-- **REMOVE BEFORE FINAL I've attempted to embed the actual javascript widget here, but it isn't working.** -->
<!-- ```{r years-frame, fig.cap='Interactive data visualization', screenshot.opts=list(delay=20,zoom=2), dev='png', cache=TRUE, fig.align='center', fig.height=6} -->
<!-- include_url("https://flowingdata.com/projects/2015/life/") -->
<!-- ``` -->

<!-- ```{r years-frame2, fig.cap='Interactive data visualization 2', fig.align='center', fig.height=6} -->
<!-- include_app("https://flowingdata.com/projects/2015/life/") -->
<!-- ``` -->


This visualization does a number of things. The most salient feature is probably the dropping balls. Each one represents a possible future outcome. This is exactly like an ensemble of alternate universes. As you watch the balls drop, you think to yourself, "ah, nice, I lived to be 92" and then moments later "ooh, harsh! I died at 39!" 

As the simulation runs, it also accumulates data in bins at the bottom, labeled "0 to 9", "10 to 19", and so on. Remember that these bins represent ranges of years-you-have-left-to-live, not age-at-death. This may be confusing, because age-at-death is indeed what is shown along the x-axis of the figure. Also, right below the x-axis, and corresponding to age-at-death is a set of gray bars that grow as the balls drop. In the screenshot, the simulation has been running for a little while, so that the following counts have been accumulated.

```{r}
bins <- c("0 to 9", "10 to 19", "20 to 29", "30 to 39", "40 to 49", "50 or more")
counts <- c(1,1,3,3,13,72)
kable(data.frame(bin=bins, counts=counts), booktabs=TRUE) %>%  kable_styling(bootstrap_options = "striped", full_width = F)


```
Notice that by the time this screenshot was taken, 93 balls had dropped. The visualization took the counts, converted them into proportions of total counts (e.g., 72/93 = 0.774; 3/93 = 0.33), and represented each of these proportions as a probability, expressed as a percent (e.g., 77%; 3%).

Another thing that you will notice if you play around a bit is that as the balls drop, the probabilities change. In the beginning, when the number of samples (balls dropped) is small, the numbers change rapidly and sometimes by a large amount. However, after a couple of hundred samples, the changes are much smaller.

By watching the balls drop on this simulation (which I, for one, find mesmerizing), you may actually be meditating on some profound ideas in statistics. Every time you restart the simulation, you begin the sampling process. Each sample is a **draw** from some distribution of possible life outcomes. Your future life bounces around in this distribution from sample to sample. And in the beginning, at small samples, the distribution itself seems unstable. For example, if you put in 24 as the current age and start the simulation in slow mode, the estimated probability of living 40-49 more years fluctuates a lot. However, as you accumulate samples, the shape of the distribution literally comes into view as a pattern among the gray bars just below the x-axis. As the sample size increases, the probabilities becomes more stable. Eventually, if you let it run long enough, you end up with the same values, regardless of how things started out.

### How does the death (simulation) work?

The Flowing Data visualization is based on data collected in "life tables", which can be found online from sources like the National Center for Health Statistics and the Social Security Administration. Different life tables are produced every year, as life expectancy continues to evolve with changes in health science and nutrition. Figure \@ref(fig:life-duration) plots data for age-at-death (for Americans) as of 2010. There is a bar for each age from 0 to 120, and the height of each bar represents a count of deaths at that age per 100,000 people.


```{r life-duration, echo=FALSE, fig.cap='How long Americans were living in 2010'}

lifetableNCHS <- read.csv("data/lifetable.csv")
lifetableMale <- read.csv("data/lifetable-male-2010.csv")
lifetableFemale <- read.csv("data/lifetable-female-2010.csv")
barplot((lifetableFemale$dx+ lifetableMale$dx)/2, ylab="death count / 100000", xlab="age", names.arg=lifetableFemale$x)
# 
# dt <- (lifetableFemale$dx+ lifetableMale$dx)/2
# lab <- rep("", length(dt))
# lab[seq(1, length(dt), by=5)] <- lifetableMale$x[seq(1, length(dt), by=5)]
# barplot(dt, names.arg=lab)


```


If you're like me, the first thing you notice in Figure \@ref(fig:life-duration) is that little spike at age 0, like a rattle sticking up at the end of a rattle snake's tail. It shows us that roughly 5 out of 1000 babies don't make it to their first birthday. After that, your odds get considerably better for a while.

Another feature that you may detect is that the distribution of age-at-death is not symmetric. It has a long tail to the left. Distributions like this are also called left-skewed. 
<!-- As is well covered in standard texts (e.g., Chapter 2 of OpenIntro Statistics), this means that certain measures of center, such as mean and median age-at-death, may give different answers. -->

So how does age-at-death relate exactly to the years you have left to live? Life tables are a bit of a strange thing. First of all, they are not tables of "raw data" for a sample of 100,000 people. Rather, they represent a summary of data from many more deaths. According to the SSA [source](https://www.ssa.gov/OACT/HistEst/PerLifeTables/LifeTableDefinitions.pdf), "the life table represents a hypothetical cohort of 100,000 persons born at the same instant who experience the rate of mortality represented by qx, the probability that a person age x will die within one year, for each age x throughout their lives."
 
Most of us don't think about our lives in terms of questions like, are we going to die this year? But that is kind of how the life table works. The life table is a set of numbers---including deaths-at-age-x and expected-years-left-to-live-at-age-x---that are all derived from one initial set of numbers which represent *the probability that a person age x will die within one year*. If you're curious what that initial set of numbers looks like, I've plotted it in Figure \@ref(fig:die-this-year).

```{r die-this-year, echo=FALSE, fig.cap='stuff'}

plot(lifetableFemale$qx, ylab="probability of dying within one year", xlab="age")


```


Looking at Figure \@ref(fig:die-this-year), you can say that the probability of dying within one year gets higher as you grow older, which comes as a surprise to no one. If you're under 65, say, that probability doesn't even feel that high. It's less than 0.01 or 1%. The probability that you will die *this year* only passes 50% after age 100. That sounds fine!

Well, don't get too optimistic. Your chances of dying every year may be small, but every year is another draw. If your chances of dying were 1 out of 2000, then in 2000 universes, you died in one of them.  In the other 1999, you live on to another year, but then you have to press your luck again. This happens every year, and the chances slowly get worse.

But what if you wanted to know your chances, at birth, of dying in your 60s, that is between 60-69. For now, we will try to answer this question using only the life table and assuming that we know nothing else about you. The rows of the life table corresponding to this age range are these

```{r survival, echo=FALSE}

kable(lifetableNCHS[61:70,], booktabs=TRUE, row.names = FALSE, caption='Life Table')

```

This is a lot of numbers. Recall that each qx is the mortality rate for age x, the probability of dying within one year of age x. So should you add up the qx-values for each age in the interval 60 to 69? Maybe pause here to think about this question for a momemnt before reading on.

You can die at 62 and you can die at 64, but you can't die at both ages. In that sense, it was okay to add the probabilities of these events because they are **disjoint**, i.e., they can't both happen and you are interested in whether any one of them does happen. However, if you add up these probabilities, you will still over-estimate the probability for a different reason. Can you guess what you've left out? You've left out the fact that these probabilities assume that you have already made it to 60, and there's a chance (at birth) that you won't.  

You want to add up the following probabilities: 

```
(Probability of making it to 60 and then dying at 60) + 
(Probability of making it to 61 and then dying at 61) + 
... +
(Probability of making it to 69 and then dying at 69) + 
```

How do you figure out the probability of making it to 60 without dying? One year at a time... 

Note that in this case you need ALL of the following to be true: NOT dying at 0 AND NOT dying at 1 AND ... NOT dying at 59. The probability of each event (not dying in each year) is independent, and the probability that all of them happen is the product of the individual probabilities.

```
Probability of NOT dying at 0 * Probability of NOT dying at 1 * ... * Probability of NOT dying at 59
```

Since in any given year, you either die or don't die, these two probabilities must add up to 1, so having gotten to any age x, the probability of surviving it is (1-qx). Now we can take the product of (that is, multiply) all of the survival probabilities (1 - qx) for each x from 0 up to age 59. (I will include the code here. The data table I have loaded from the National Center for Health Statistics is called "lifetableNCHS").


```{r, echo=TRUE}
prod(1-lifetableNCHS[1:60,"qx"])

```

You may notice that this probability had already been calculated for you in the life table, but it had been presented slightly differently as column lx, which is the number of persons (in a cohort of 100,000) surviving to exact age x. If we multiply our rate by 100000, we get 887458.8, which (up to a rounding error) is the same as the number in Table \@ref(tab:survival). 

Okay, so now we are ready to complete the probability calculation. Recall we wanted to add up ten things: Probability of making it to 60 and then dying at 60, etc. We know that the probability of making it to age x is the same as the value of column lx in the table divided by 100,000. And the probability of dying is qx. So we need to multiply these two numbers in each row and add them up.

The result is `r round(sum(lifetableNCHS[61:70,"lx"]/100000 * lifetableNCHS[61:70,"qx"]),4)`. An American child born in 2010 has a 10.5% chance of dying in their 60s (and a 20.7% chance of dying in their 70s). 

So, we've figured out how to do that. And we're almost ready to move on, but it is worth noticing something. The product of the value qx and lx in each row of the life table is the value dx, which is the number of deaths at age x (or between x and x+1). So when we multiplied and added before, we were really just adding up the number of deaths (dx) at ages 60-69 and dividing by 100,000. 

Now hopefully that makes sense to you that this should give us the answer we were originally looking for, namely what are the chances, at birth, of dying in your 60s. We could have looked at our hypothetical cohort of 100,000 people all born at the same time and asked: how many of them will die in their 60s. Well, that would be the sum of the dx-values, namely `r sum(lifetableNCHS[61:70,"lx"])`. It wouldn't be a probability, though, unless we divided it by the total number of people (100,000).

So we've shown that we can answer our particular question two different ways:

A) Computing the total probability of your making it to 60 and then dying at 60 *or* making it to 61 and dying at 61 *or* making it to 62 and dying at 62 etc. up to age 69.

or
 
B) Computing the overall proportion, out of 100,000 people, who die in their 60s.


A = B in this case. An important property of mathematical sciences is that you can arrive at the same answer in different ways. Maybe that sounds like a waste of time, but I view it as one of the most reassuring things about math. If you try something two different ways, and you do *not* get the same answer, then something is probably up. 

## Some facts about Probabilities

A lot of books will try to get a lot of basic facts about probability out of the way up front. (See, for example, OpenIntro Stats, chapter 3). There is a sound logic to setting up foundations like that. But in this book, I've taken the strong position that ideas should be driven by questions. So I've tried to reason through the example above without setting up any foundations. Nevertheless it's a good time to recap some of what we established about probabilities.

- When possibilities are disjoint, or mutually exclusive, the probability that either one of them happens is the sum

<!-- $$P(A \mbox{ or } B) = P(A) + P(B)$$ -->
```
P(A or B) = P(A) + P(B)
```

An example of this was dying at age 62 or dying at age 64.

- A special case of this addition rule applies when one or the other MUST happen. For example, in logic, either something happens or it doesn't happen. Either A or NOT A. Since these possibilities are disjoint:

```
P(A) + P(not A) = 1
P(not A) = 1 - P(A)
```

An example of this was the probability that you do not die at age 0. The last fact we used is 

- The probability rule for **independent** processes the BOTH occur is the product of the individual probabilities.

```
P(A and B) = P(A) * P(B)
```

We used that to figure out how you survive by not dying every year. Notice that I've snuck in the word independent (well, I snuck it in boldy, so it wasn't that sneaky). There is an intuitive reason why this is important. 

Remember bizarro world where the toilet paper orientation and peanut butter preference were deterministically related, and specifically everyone is either under-chunky or over-smooth? If I told you that 60% of the total population prefers smooth, then what proportion of the total population prefers smooth AND likes to over-hang? Also 60%. What proportion prefers smooth AND under-hangs? 0! 

Since, in bizarro world, toilet paper orientation and peanut butter preference are NOT independent,

```
P(tp = over and pb = smooth) does NOT equal to P(tp = over) * P(pb = smooth)
```

This will become even more clear in the next section.


## Conditional Probabilities

Recall that we would NOT have gotten the right answer to the probability of dying in your 60s if we added up the mortality rates qx for all ages x in [60-69]. Rather, we had to multiply these numbers first by the probability of living to age x. Another way to say this is that the mortality rate qx was actually a **conditional probability.**




```{r}

sum(lifetableNCHS[61:70,"qx"])

```




```{r life-durationMF, echo=FALSE, fig.cap='How long Americans were living in 2010'}

barplot(lifetableFemale$dx, ylab="death count / 100000", xlab="age", col=rgb(0,1,0,0.2), names.arg=lifetableFemale$x)
barplot(lifetableMale$dx, col=rgb(1,0,0,0.2), add=T)


```














## Bayes Rule with disease


## Causality

Does eating meat cause heart disease? Does smoking cause lung cancer? What does it mean to say A causes B? First of all, this may sound like a philosophical question, and indeed the philosopher David Hume shed some important light on the question of how we conceive of causation. But this is a course on probabilistic thinking, not philosophy. So we are going to take a more pragmatic approach and focus on how we use the concept of causation in everyday life. Nevertheless, it helps to first recall our distinction between deterministic and stochastic processes. If I hit a porcelain tea cup hard with hammer and the tea cup breaks, we can safely say that hitting the teacup with a hammer caused the cup to break. We don’t really feel the need to say that if you hit a teacup hard with a hammer, there is a 99.9997% chance that it will break. Even if that’s actually true. And we don’t feel the need to define "hard" in this case either. We use an example like a teacup and hammer when we want to focus on the big picture and not the details. And the big picture here says that hitting a teacup with a hammer deterministically causes the teacup to break. If we don’t hit In the case of the physics of hammers and teacups, we feel that we know this much is true. 

What about buying a lottery ticket? Does buying a lottery ticket cause one to win the lottery? Well, you certainly are not guaranteed to win the lottery if you buy a ticket. In fact, your chances will be very low. But you can’t possibly win if you don’t buy a ticket. So, strictly speaking, buying a ticket or not does influence the probability of buying a ticket.

We’ve now seen two examples

In the first case (hammer and teacup):
If A (hammer hits teacup) then definitely B (teacup breaks)
If not A (hammer does not hit teacup) then definitely not B (teacup does not breaks)




Teacup breaks
Teacup doesn’t break
Hammer hits teacup
Always*
Never
Hammer does not hit teacup
Never*
Always
*pretty much; we’re not splitting hairs here.

In the second case (lottery ticket):
If A (buy lottery ticket) then maybe B (win lottery) and maybe not B (do not win lottery)
If not A (do not buy lottery ticket) then definitely not B (do not win lottery)

Now, let’s pause for a moment and think about the question we started with: does smoking cause cancer? Does it fit either of these two cases?

Unfortunately the question about smoking does not. It belongs to a yet another case. 

In the third case (smoking):
If A (smoke) then maybe B (cancer) and maybe not B (no cancer)
If not A (do not smoke) then maybe B (cancer) and maybe not B (no cancer)

Now we’re not saying that the chances of cancer are the same whether you smoke or not. That remains an open question so far as our present argument goes. But even thus far, we can see that the smoking causality question, posed this way, invites some more questions.
How big a difference does there have to be between the cancer rates for smokers and non-smokers for us to be convinced that there is an association between smoking and cancer?
After all, there are a lot of other factors including behavior and genetic factors
Why do we believe that the association between smoking and cancer is a causal relationship (specifically, smoking causes cancer)?
