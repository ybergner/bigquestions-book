\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[openany]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Carpe Datum},
            pdfauthor={Yoav Bergner},
            colorlinks=true,
            linkcolor=Maroon,
            filecolor=Maroon,
            citecolor=Blue,
            urlcolor=blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\usepackage[normalem]{ulem}
% avoid problems with \sout in headers with hyperref:
\pdfstringdefDisableCommands{\renewcommand{\sout}{}}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{subfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{float}
\usepackage[vmargin=3cm,rmargin=3.5cm,lmargin=3cm]{geometry}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{awesomebox}
\usepackage{footnote}
\usepackage{environ}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Carpe Datum}
\providecommand{\subtitle}[1]{}
\subtitle{Data Science for Life's Big Questions}
\author{Yoav Bergner}
\date{}

\begin{document}
\maketitle

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\begin{quote}
``{[}T{]}he most important questions of life are for the most part only problems of probability. It may even be said, strictly speaking, that almost all our knowledge is only probabilistic.''

--- Pierre-Simon Laplace
\end{quote}

\hypertarget{caveat}{%
\section*{Caveat}\label{caveat}}
\addcontentsline{toc}{section}{Caveat}

This book is an incomplete draft of a work in progress being developed as lecture notes for an online course. Content is provisional, contingent, and possibly wrong, but always well intended.

\hypertarget{guiding-principles-in-this-book}{%
\section*{Guiding principles in this book}\label{guiding-principles-in-this-book}}
\addcontentsline{toc}{section}{Guiding principles in this book}

\hypertarget{question-driven}{%
\subsection*{Question-driven}\label{question-driven}}
\addcontentsline{toc}{subsection}{Question-driven}

Because the presentation of topics in this book is question-driven rather than method-driven, this coursebook has some idiosyncracies. Some topics that might be considered rather basic may be omitted, while some topics that are typically considered as advanced will get (a simplified) treatment.

\hypertarget{no-proofs}{%
\subsection*{No proofs}\label{no-proofs}}
\addcontentsline{toc}{subsection}{No proofs}

As a mathematical subject, statistics is often taught with derivation and proof using definitions, simple assumptions, and the logic of algebra and calculus. Mathematical formulas are the standard language of statistics. This approach to learning is powerful if the math supports rather than gets in the way of understanding. However, for many learners, the math obscures rather than clarifies, and another way--using demonstrations and simulations--might enable understanding, as Johnson \& Johnson once said, without tears.

Now, a demonstration is not a proof. That said, repeated experiments can be convincing even in the absence of proof. For example, I can prove to you that if you take any whole number (e.g., 1, 2, 3, 7, 21, 118, 8675309), multiply it by 9, and then sum the individual digits of that resulting product, that the sum itself will be a multiple of 9

\begin{verbatim}
Example: 
7 * 9 = 63; 6 + 3 = 9.
21 * 9 = 189; 1 + 8 + 9 = 18. 
\end{verbatim}

An elegant and simple proof can be constructed (hint: by induction), but if you try it out yourself enough times, you won't \emph{need} the proof to be convinced. Twice, by the way, might not be enough. You could, however, write a computer program to test this equality a thousand times using a thousand random whole numbers. This is sometimes called a computer simulation.

Now problems like this one are often used to teach proof technique rather than to encode cute number-facts in memory. And indeed, for training statisticians, a rigorous mathematical presentation is important. So, for that matter, is computer simulation. For most users of this book, intuition and understanding are the priority, and the ability to derive formulas is not necessary. We will, in due course, bring out some computer simulations.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/existence_proof_2x} 

}

\caption{Hopefully you are in the right place. Credit   [xkcd.com](https://xkcd.com/1856/)}\label{fig:xkcd-preamble}
\end{figure}

\hypertarget{how-many-kinds-of-people-are-there}{%
\chapter*{How Many Kinds of People Are There?}\label{how-many-kinds-of-people-are-there}}
\addcontentsline{toc}{chapter}{How Many Kinds of People Are There?}

\begin{quote}
There are 10 kinds of people in this world.
Those who understand binary code and those who don't.
--- seen on a T-shirt
\end{quote}

\hypertarget{things-are-about-to-get-meta-right-from-the-start}{%
\section*{Things are about to get meta right from the start}\label{things-are-about-to-get-meta-right-from-the-start}}
\addcontentsline{toc}{section}{Things are about to get meta right from the start}

I'm going to start off this first chapter in a book about data science with an unsubstantiated claim. My claim is this: People love to categorize themselves and others. They love to take quizzes online that tell you ``what kind of person you are'' in some way or another. They love to make statements that begin with, ``there are two kinds of people in this world\ldots{}'' and so on. Ok? That's my claim. It's a bit of a mouthful.

Now, I just made a claim in support of which data \emph{can absolutely} be brought to bear. But I won't use data to support it. What? Why not, for crying out loud?! This is a book about data science!!! The reason is this: this book encourages you to think critically and skeptically about all kinds of ideas, claims, and questions. It tries to show you how to talk about these ideas precisely and not succumb to fallacies and bad intuition. But while trying to develop these skills, it is important to know when we are in turbo critical thinking mode (that's a technical term\footnote{Just kidding; it's not really a technical term.}) and when we're not. Sometimes, we need to be able to say common-sense things and not have to support them.

What \emph{exactly} am I even saying in my claim, you might be thinking? What do you mean by, ``people love to'' do X, where X, like \_\_\_\_\_\_ {[}``blank''{]}, is a stand-in for some of the specific things I mentioned. That everybody does X? Most people? That people who do X derive pleasure above some pleasure threshold, thus designating ``love'' as opposed to ``like?'' You see, I could have tried to make my claim more precise. And I could have found polls and published reports that estimate just how many people have, by choice, taken some kind of person-category-test-thing, or posted funny jokes about ``two kinds of people.'' But I'm just letting my claim stand as a common-sense claim. Just like if I said, people love going to the movies. I wouldn't feel the need to cite a scientific study to support that claim.\footnote{\url{https://digg.com/2019/movie-genre-popularity-1910-to-2018-data}}

Now, if someone is making what to \emph{them} appears to be a common-sense claim but to you appears false or at least non-obvious, you have a few options. You can challenge the assumption and ask for evidence. Or you can accept the assumption, \emph{for argument's sake}, to see where this is going. Hopefully, my claim feels common-sense enough to you too (i.e., we have that in common). If not, I'll just ask you to follow along to see where this is all going\ldots{}

\hypertarget{categories}{%
\chapter{Categories, counts, and kinds}\label{categories}}

\hypertarget{two-kinds-of-people}{%
\subsection*{Two Kinds of People}\label{two-kinds-of-people}}
\addcontentsline{toc}{subsection}{Two Kinds of People}

``There are two kinds of people\ldots{} which one are you?'' questions have become something of an internet meme, particulary with the categorizations represented graphically or pictorially. There is a whole \href{https://2kindsofpeople.tumblr.com/}{blog devoted to them by João Rocha}. The images in Figure \ref{fig:tp-fig} probably need no explanation, as they concern the great \href{https://en.wikipedia.org/wiki/Toilet_paper_orientation}{toilet paper orientation} debate.



\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/Toilet_paper_orientation_overunder} 

}

\caption{The great debate. Source \href{https://commons.wikimedia.org/wiki/File:Toilet_paper_orientation_over.jpg}{Wikimedia Commons User:Elya}}\label{fig:tp-fig}
\end{figure}

Toilet paper orientation is a distinguishing \textbf{test question} that separates people into one of two ``kinds'' (or ``types'' or ``categories''; sometimes English has several words that are used interchangeably). A fancy word for this ``splitting into two'' is dichotomy (die-COT-uh-mee), from the Greek. A \textbf{dichotomous question} has two possible answers. Here, you choose one way to orient the roll or the other. Let's call this roll choice ``over'' (shown on left) or ``under'' (shown on right). Perhaps you have debated which is better with a friend or family member. In any case, armed with this particular test question, we can go out and collect some data.

\begin{table}[!h]

\caption{\label{tab:tp-table}How people roll}
\centering
\begin{tabular}[t]{lr}
\toprule
 & count\\
\midrule
over & 23\\
under & 17\\
\bottomrule
\end{tabular}
\end{table}

I went ahead and asked 40 people in Washington Square Park in New York City which kind of person they were, and the results are shown in Table \ref{tab:tp-table}. This being a book about data science, you might think I'm going to start calculating proportions right away, for example by saying that 57.5\% of New Yorkers are over-hangers.
Nope. Although you should be able to figure out that proportion conversion, it is not the point I want to focus on right now.

That point I want to focus on is that, based on our data, there \emph{are} indeed two kinds of people here. If, for example, everyone in the world were an under-hanger (heaven forbid), then I couldn't very well say that there were two kinds of people in this world. At least not with regard to toilet paper orientation. It would be like if I presented you with the data in Table \ref{tab:dumb-table}. Looking at that, I can't very well convince you that there are two kinds of people.

\begin{table}[!h]

\caption{\label{tab:dumb-table}Kinds of people in Washington Square}
\centering
\begin{tabular}[t]{lr}
\toprule
  & count\\
\midrule
human & 40\\
not human & 0\\
\bottomrule
\end{tabular}
\end{table}

That all seems pretty obvious, in part because I made up a \emph{tautology} in the second example there. Being a human being is automatically associated with everyone who can be a \emph{kind of person}.

But what if I had gotten exactly the same results for the toilet paper question? What if the data looked like Table \ref{tab:tp-redux}. In this \textbf{alternate universe}, everyone I ask in Washington Square is an under-hanger. Yes, it's one of those scary alternate universes, like the Twilight Zone. Anyway, does that mean that there is only one kind of person when it comes to toilet paper orientation? Well\ldots{}not necessarily. After all, this was just a \textbf{sample} of people in Washington Square. It was not the whole \textbf{population} of Washington Square, even, let alone New York City, let alone the world.

\begin{table}[!h]

\caption{\label{tab:tp-redux}How people roll (alternate universe)}
\centering
\begin{tabular}[t]{lr}
\toprule
  & count\\
\midrule
under & 40\\
over & 0\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{samples-and-populations-a-statistical-caveat}{%
\subsection*{Samples and Populations: A Statistical Caveat}\label{samples-and-populations-a-statistical-caveat}}
\addcontentsline{toc}{subsection}{Samples and Populations: A Statistical Caveat}

Samples and populations are sort of a big deal in statistics and data science, where these words have somewhat specialized meanings. Consider the following utterances:

\begin{verbatim}
The population of New York City is 8.6 million
The population of New York City is ethnically diverse
\end{verbatim}

How can both of these statements make sense? In common usage, population often refers to the number or count of people, in a town, area, or country. Among statisticians and data scientists, population refers to a set or collection under consideration. It doesn't have to be a set of people. It could be a set of rats, non-governmental organizations, or domestic flights originating in Chicago. But let's suppose the population does refer to a set of people. The \emph{number} of those people is just one summary about the population, also known as the total \textbf{count}. The proportion of over-hangers is another summary of the population, as is the most-common birth-month. The two statements above would be more consistent if the first were rephrased to be: the population (set of people) of New York City \sout{is} numbers 8.6 million.

If we always had access to all of the members in a population (the set or collection under consideration), the field of statistics wouldn't exist. We would just know a bunch of facts about, say, everyone in the whole world. And that would be that. While it is true that data are becoming more and ubiquitous, don't start betting on the demise of statistics. Even if we did have complete data for everyone in the world today, our population of interest might extend to the world as it will be next month, next year, or ten years from now. That is, we might want to make predictions about the future. In which case, we would want to draw \emph{inferences} and to generalize from the data we have on hand---our sample---to data we don't have---the rest of the (i.e., the future) population. Making inferences from samples to populations will always be a compelling and challenging problem.

Since Washington Square is the center of my universe, that's where I sample. Even if we agreed that our population of interest were confined to Washington Square, we would still find it difficult to collect data on everyone there. There are a lot of them, many of them are on skateboards, and new people keep leaving and entering the park. It turns out, that's okay. We don't actually have to reach everybody to be able to do data science. However, we need to understand that when we sample 40 particular people in Washington Square, we might not get the same exact answers as if we had sampled 40 \emph{different} people. The sampling process introduces an element of \textbf{uncertainty} into our process.

Coming back to our toilet paper debate, if we did find zero over-hangers in one sample, it doesn't guarantee that the number of over-hangers will also be zero in the next. The number may vary from sample to sample. Uncertainty does not, however, mean that the information derived from one sample is useless. In fact, soon we'll see that we can actually learn a lot from a sample simply by recognizing that sample values will vary. We can simulate samples on a computer to see how much they will vary. And then, using our simulations, we will be able to give probabilistic answers to questions like, ``what are the chances that there really are no over-hangers in Washington Square?''

\hypertarget{a-psychological-caveat}{%
\subsection*{A Psychological Caveat}\label{a-psychological-caveat}}
\addcontentsline{toc}{subsection}{A Psychological Caveat}

The section above explored a statistical caveat about drawing conclusions from samples. But another caveat that applies here is more psychological. I have assumed, for this argument, that a person's answer to the toilet paper question is a fairly stable thing and not just a transient state-of-mind. That is, if I asked you tomorrow or next week, your answer would be the same as if I asked you today. For the most part. I'm not saying you can't ever decide to change your mind. But it wouldn't make sense to describe a person as an over-hanger if there were no stability at all to their answer. A transient state-of-mind, by contrast, could be asking someone if they are hungry. Everyone is hungry some of the time, but not all of the time. And it wouldn't make any sense, based on that line of questioning, to imply that there are two kinds of people.

\hypertarget{summarizing-data}{%
\subsection*{Summarizing data}\label{summarizing-data}}
\addcontentsline{toc}{subsection}{Summarizing data}

When I presented my survey results to you in Table \ref{tab:tp-table}, notice that I did not present you with the raw data, but rather with a summary of the data. The particular summary I used was called ``counts'', that is, a total count of how many people responded ``over'' or ``under.'' The raw data, in contrast, would have contained each individual response I collected, labled either with a name of the individual, or perhaps with some other unique identifier (such as a random number), or---if I don't need to keep track of particular individuals---with just a row number. Something like this, if we examine at the first six responses rather than all 40 of them. Raw data:

\begin{table}[!h]
\centering
\begin{tabular}{ccc}
\toprule
row & randomID & response\\
\midrule
1 & 9246 & under\\
2 & 1478 & over\\
3 & 8831 & under\\
4 & 8194 & over\\
5 & 4178 & under\\
\addlinespace
6 & 4243 & under\\
\bottomrule
\end{tabular}
\end{table}

Counts is an example of a \textbf{summary statistic}, which is a fancy term for a number that is derived from the raw data. The count summary is as simple as it gets. It is literally the number of times that each response appears. We might note as well that,

\begin{verbatim}
count(under) + count(over) = total number of responses.
\end{verbatim}

This mathematical statement is true because there are only two possible responses. If there were more than two responses, then I would need to add the counts for each possible response.

Note that the \emph{proportion} of ``over'' responses is also a summary statistic (which is just the counts of ``over'' divided by the total number of responses). Another summary statistic could be the ratio of ``over'' responses (counts) to ``under'' responses. For example, one way people use summary statistics in reporting data is through statements like, ``twice as many people prefer chunky peanut butter to smooth.''

\hypertarget{no-mean-feat}{%
\subsubsection*{No mean feat}\label{no-mean-feat}}
\addcontentsline{toc}{subsubsection}{No mean feat}

\begin{quote}
Whenever someone reports a mean (another word for average) value of some set of data, that is also a summary statistic. Does it make sense to construct an average from responses that are either ``over'' or ``under''? No, it doesn't. That's because \{over, under\} is a categorical response, and you can't average over categories. Unless you're trying to make fun of statistics with a puerile joke. Different versions of this joke appear: ``the average American has one tit and one testicle.'' At the risk of explaining the joke too much, here goes: Tits and testicles can certainly be treated as numerical data, and hence can be averaged. This joke hinges on the fact that the existence of testicles (or tits) is associated with a person's sex, which is categorical and not numerical. Assuming that half of all Americans are female (roughly true), we can't say that the average American is half male and half female. The real ``punch'' of this joke is to suggest that summary statistics about averages are just a bunch of nonsense. What do you think?
\end{quote}

This is about as much as we need to say about summary statistics for the time being. But they'll be back.

\hypertarget{checkpoint}{%
\subsection*{Checkpoint}\label{checkpoint}}
\addcontentsline{toc}{subsection}{Checkpoint}

While focusing on the great toilet paper debate, we've managed to establish some important fundamental ideas.

\begin{itemize}
\item
  Dichotomous questions split people into two kinds, but only as long as it is actually possible for both answers to occur.
\item
  In an alternate universe, people might give different answers than they do in this one. (Seriously, this is an important idea).
\item
  Even when we casually refer to \emph{people}, we may have a particular set of people, a population, in mind. Data about this population are likely to come from a sample, rather than from the whole population, and this fact introduces some uncertainty into claims about the whole population. Data science to the rescue!
\item
  If we have types of people in mind, our questions ought to elicit fairly stable answers
\item
  Clearly, we can ask people questions that prompt them to choose between more than two categories. But ``two types of people'' questions are more fun.\footnote{That was another unsubstantiated claim.} I mean there are so many of them! So\ldots{} does that mean that there really are two types of people? To answer this, we will need to get into another great debate.
\end{itemize}

\hypertarget{dimensions}{%
\chapter{Dimensions}\label{dimensions}}

\begin{quote}
``I always said if I had one breakfast to eat before I die, it would be Wonder Bread toasted, with Skippy Super Chunky melted on it, slices of overripe banana and fresh crisp bacon.''

--- \href{https://nypost.com/2008/07/26/mayors-last-meal-is-a-killer/}{Michael Bloomberg}
\end{quote}

Former NYC mayor Michael Bloomberg is a chunky peanut butter kind of person. Are you? As peanut butter comes in ``smooth'' and ``chunky'' varieties (also known as creamy and crunchy, respectively), this question is also a dichotomous one. However, if we add this test question to our question pool, in addition to the one about toilet paper orientation, we will soon find that having two two-kinds-of-people questions begins to imply more than two kinds of people. Wait, what?

See, back when I went to talk to the people in Washington Square, I also asked them about the great peanut butter debate. As you can see from Table \ref{tab:pb-counts}, smooth came out slightly ahead.

\begin{table}[!h]

\caption{\label{tab:pb-counts}How people spread}
\centering
\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedleft}X}
\toprule
 & counts\\
\midrule
chunky & 17\\
smooth & 23\\
\bottomrule
\end{tabu}
\end{table}

But this second question did not erase the first question about toilet paper. In fact the first few rows of our data from Washington Square are displayed below. Each row, representing one person, now has two columns, labeled ``roll'' (for toilet paper) and ``spread'' (for peanut butter):

\begin{verbatim}
##    roll spread
## 1 under chunky
## 2  over chunky
## 3 under smooth
## 4  over chunky
## 5 under smooth
## 6 under chunky
\end{verbatim}

You may have noticed that among the first six people for whom I have shown data, none of them answered both over and smooth. But such response pairs exist. In fact, if we count each combination as it occurs--that is, under-chunky, over-chunky, under-smooth, and over-smooth--we get the results shown in Table \ref{tab:tpxpb}. There are four combinations, because we have two questions with two possibilities (dichotomies) for each.

Before you read on, it's a good time to ask yourself if you can answer the following questions (answers in the footnote): (a) if there were two questions with three categories each, how many combinations could be observed? (b) if there were three dichotmous questions, how many combinations could be observed?\footnote{(a) If the categories for each question are A, B, and C, we can get AA, AB, AC, BA, BB, \ldots{} etc. We multiply the number of categories as many times as we have questions. So 3*3 = 9. (b) This time we have three questions, and for each one we have two options, so there are 2*2*2=8 possible combinations.}

\begin{table}[!h]

\caption{\label{tab:tpxpb}Two questions}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & chunky & smooth\\
\midrule
over & 10 & 13\\
under & 7 & 10\\
\bottomrule
\end{tabular}
\end{table}

Table \ref{tab:tpxpb} is an example of a kind of table that is so common in data science, it has its own name. Three of them, in fact. It is sometimes called a cross table (or crosstab), or a \textbf{two-way table} (makes sense), but most commonly it is known as a \textbf{contingency table} (wha? I'll explain later) I'm sorry that there are three names for the same thing. Really I am.

\begin{quote}
If you're like me, you can't resist paying some attention to the values in the Table \ref{tab:tpxpb}. For example, you might notice that one of the cells of the table (over AND smooth) has the highest number of people in it. We can say that this is the \textbf{modal} category, referring to the \textbf{mode}, which is the most common value in a distribution of values. We have four possible values in this example.
\end{quote}

Ok, now things are about to get deep. This first module is ``How Many Kinds of People are There?'' And we've now explored how using two two-kinds questions leads to four types. You've probably figured out yourself that you have to multiply the number of categories in each of the questions, and that tells you how many ``buckets'' you can have overall. But still, there are different ways to arrive at a certain number of buckets.

\begin{table}[!h]

\caption{\label{tab:newpb}PB preference}
\centering
\begin{tabular}[t]{lr}
\toprule
 & counts\\
\midrule
chunky & 13\\
don't care & 3\\
hate all & 4\\
smooth & 20\\
\bottomrule
\end{tabular}
\end{table}

Consider Table \ref{tab:newpb} in contrast to \ref{tab:tpxpb}. We've now given people four choices to express their peanut butter preference. In addition to chunky and smooth, they can also choose to say that they hate all peanut butter or don't care. We now have four kinds of people. But since we make the determination of what kind of person you are using just one question, we say that there is one \textbf{dimension} (in this case, peanut butter preference) along which people can be divided into four groups. In Table \ref{tab:tpxpb}, there were two dimensions, a dimension of peanut butter and a dimension of toilet paper. Notice that this word, dimension, is used in much the same way as when we refer to geometric space as being two-dimensional (e.g., a drawing on flat sheet) or three-dimensional (e.g., a solid object, or sometimes a drawing that creates the illusion of looking at a solid object.) The three dimensions of space are often labeled something like (x, y, z). Here, our two dimensions could be labeled (pb, tp). The order doesn't matter. We are merely indicating that there are two different variables used in categorizing our data (people, in this case). To summarize, in Table \ref{tab:tpxpb}, we have two dimensions and four kinds. In Table \ref{tab:newpb}, we have \emph{one} dimension and four kinds.

So far so good: two questions, two dimensions, right? Well\ldots{} maybe. We already saw that if a question does not actually divide people into kinds, because only one answer appears, then it doesn't really count. It is not a dimension, because it is not really a variable. It does not vary; it is constant. In our contingency table representation, this might look like the left side of Table \ref{tab:tpxpb-alt}. In an alternate universe, no one prefers smooth to chunky. Another way to say it is that the peanut butter question is not \textbf{informative} because it has no \textbf{variance}. Everyone in our sample is the same.

\begin{table}[!h]
\caption{\label{tab:tpxpb-alt}Two questions (alternate universes)}

\centering
\begin{tabular}[t]{lrr}
\toprule
  & chunky & smooth\\
\midrule
over & 23 & 0\\
under & 17 & 0\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & chunky & smooth\\
\midrule
over & 0 & 23\\
under & 17 & 0\\
\bottomrule
\end{tabular}
\end{table}

But now consider the alternate universe on the right of Table \ref{tab:tpxpb-alt}. In that case, everyone who is an over-hanger of toilet paper prefers smooth peanut butter, and everyone who is an under-hanger prefers chunky. If this is the case, there are only two kinds of people, at least in our sample. Those who over-hang \emph{and} prefer smooth and those who under-hang \emph{and} prefer chunky. But does it make sense to say there are two dimensions? We did ask two different questions!

You might reason about it the following way: in our sample, if I ask anyone just one of the two questions--about either toilet paper or peanut butter--then I immediately know the answer they would give to the other one. Another way to say this is that the answer to one question completely \emph{determines} the answer to the other, and thus the relationship between these questions (really, the answers) is \textbf{deterministic}. I don't actually have to ask two questions, other than to establish in the first place that I didn't have to. And since I only get information from one question, there is only one dimension.

\hypertarget{independence-association-and-contingency}{%
\section*{Independence, Association, and Contingency}\label{independence-association-and-contingency}}
\addcontentsline{toc}{section}{Independence, Association, and Contingency}

\begin{quote}
This section title sounds like a philosophy book by the late Richard Rorty.
--- inner voice
\end{quote}

We just spent a little bit of time in an alternate universe, a bizarro world in which knowing how someone prefers to orient their toilet paper tells you what style of peanut butter they like, and \emph{vice versa}. Notice that this knowing-about relationship is symmetric, and that in fact, the two representations as shown in Table \ref{tab:tpxpb-alt2way} are informationally equivalent.

\begin{table}[!h]
\caption{\label{tab:tpxpb-alt2way}Alternate universe (two equivalent ways)}

\centering
\begin{tabular}[t]{lrr}
\toprule
  & chunky & smooth\\
\midrule
over & 0 & 23\\
under & 17 & 0\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & over & under\\
\midrule
chunky & 0 & 17\\
smooth & 23 & 0\\
\bottomrule
\end{tabular}
\end{table}

In our regular universe, however, this relationship was not observed. In Table \ref{tab:tpxpb}, all four possible combinations occur. When knowledge about a person's answer to one question provides information about their answer to another question, we say that the two answers are \textbf{contingent} upon one another. This is the reason we called the two-way table a contingency table in the first place, although it is still called that even when two answers are not contingent. Go figure. Contingent is another word for \textbf{dependent}. To make matters worse, we \emph{also} often say that the two responses are \textbf{associated}.

In our bizarro world scenario, one answer completely determines the other. This \textbf{deterministic} relationship is one extreme in the spectrum of association/dependence/contingency. It expresses a certainty in knowing the answer to one question if we know the answer to another. At the other extreme, if the two responses are not at all associated/dependent/contingent, then we say that they are \textbf{independent}. To say that two responses are independent is to assert that knowing one of them does not give you any information about what the other one might be. This would have been my intuition, at least, about toilet paper and peanut butter. Somewhere in the middle, we might say that one answer gives you some information, but not certainty about another answer. Whether two answers are independent or mildly associated with one another is an empirical question, which means we should try to answer it with data. In bizarro world, where they were deterministically related, we might reasonably want to know why. Could there be a gene that controls both toilet paper orientation and peanut butter preference at the same time?

\hypertarget{latent-factors-and-measurement}{%
\section*{Latent Factors and Measurement}\label{latent-factors-and-measurement}}
\addcontentsline{toc}{section}{Latent Factors and Measurement}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/two-two-kinds} 

}

\caption{Two more two-kinds questions}\label{fig:apps-windows}
\end{figure}

Figure \ref{fig:apps-windows} (\href{https://2kindsofpeople.tumblr.com/}{source}) shows two more two-kinds of people graphics from João Rocha's blog. I bet that you can identify yourself with one of the two images in each pair. I certainly can. But ask yourself, given our discussion above, do you think the choices a person would identify in each case above are independent or not independent (e.g., contingent, associated, dependent)?

In contrast to the toilet paper and peanut butter questions, which at least appear to be about totally different things, these two dichotomies have something similar going on in each of them. The choice on the left is about organizing your desktop browser, either in tabs or as separate windows. The choice on the right is about organizing apps in your smartphone, either loose or in folders. We might say that both of them get at a tendency to organize your digital environment. Call it digitidness (short for digital tidiness). This tendency, we may imagine, might even carry over into non-digital environments, like your actual desk, bookshelf, or filing cabinet.

What we've done here is to try to explain the association between responses to the two questions (assuming that there is, i.e.~that they are not independent) by appeal to some underlying \textbf{latent factor}. We say a factor is latent (meaning hidden) because we don't observe digitidiness itself directly, but we only observe tidy browsers or smartphone app folders. Perhaps you can think of another candidate factor besides digitidiness. In any case, we might propose that both of the two two-kinds questions in Figure \ref{fig:apps-windows} are in effect indirect \textbf{measurements} of the same factor. If so, this could explain why the two answers would be associated.

\begin{quote}
Notice that a \textbf{factor} is also a dimension, in the sense we used before. We could have said ``latent dimension'', but we tend to use the word factor when we are drawing attention to the specific nature of the dimension rather than just counting. We also sometimes use the word \textbf{trait}. At least in psychology, trait tends to be reserved for stable psychological factors. Thus ``stress'' can be a factor but not a trait, whereas ``social anxiety'' may be a trait, if it is persistent. In this case, digitidiness might be considered a trait (and thus also a factor and a dimension).
\end{quote}

Contrast this with toilet roll orientation, which we can observe directly just by looking in someone's bathroom. (We assume that they are telling the truth when they answered our questions, but we could in principle verify it.) It was only in the bizarro world when toilet roll orientation and peanut butter preference were perfectly related that we started to wonder if there maybe \emph{was} an underlying genetic factor. Genetic factors were once not directly observable either, but we assumed them for explanatory value. Today we can of course observe specific genetic variation, although there are still many gaps in our understanding of the relationship between genes and observed behaviors.

Consider some data again, in two possible worlds, shown in Table \ref{tab:tabsxapps}. On the left, we have the deterministic scenario we saw before. As before, we identified this situation as having two kinds of people and really just one dimension. In contrast to before, where we had no real explanation for this coincidence, we attribute it now to some factor, like digital tidiness.

\begin{table}[!h]
\caption{\label{tab:tabsxapps}Possible data for digital tidiness}

\centering
\begin{tabular}[t]{lrr}
\toprule
  & app-folders & apps-loose\\
\midrule
browser-tabs & 21 & 0\\
browser-windows & 0 & 19\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & app-folders & apps-loose\\
\midrule
browser-tabs & 16 & 6\\
browser-windows & 5 & 13\\
\bottomrule
\end{tabular}
\end{table}

But now consider the possible results in the table on the right. Since all four possible quadrants have non-zero counts, we see that knowing whether someone organizes their browser using tabs does not completely (i.e., \emph{deterministically}) specify whether or not they put their apps into folders. On the other hand, one answer \emph{does seem to be associated} with the other. Notice that the values are still much higher in the diagonal ``buckets'' that we think of as indicating the presence or absence of digitial tidiness. These are the tabs-folders bucket (tidy) or the windows-loose bucket (not tidy). We say that the tidiness factor appears to explain much of the observed range, or \textbf{variance}, in responses to the two questions. But it doesn't explain all of it, since there are people (11 out of 40, in this case) who don't fall into one of these buckets.

This situation on the right is probably more realistic. After all, very few things in this world are absolute (unlike in bizarro world). So now the big question re-emerges: are there two kinds of people or four? One dimension, or two? It's sort of\ldots{}like\ldots{}in between\ldots{}?

\textbf{Golda says}: Although digitidiness explains a lot of what we see in our data, it doesn't explain it all. I believe that desktop tidiness and mobile tidiness are different, if related, tendencies. For example, when we use mobile phones, we're typically on-the-go and have less time. If we knew more about the people in our sample, we might see that these discrepancies in the organization of apps and tabs actually relate to other aspects of their lives. So, I say there are two dimensions.

\textbf{Sidney says}: Digitidiness is the only real factor here, but people may not always be consistent in these particular behaviors. Also some people are only sort-of-tidy, and apply this tidiness unevenly and randomly. These two-kinds of people choices don't leave room for shades of gray, so that's what we're seeing in the mixed categories where people are tidy in one environment and not in another. But ultimately there is really just one dimension here.

\textbf{What do you think?}

\hypertarget{test-indep}{%
\chapter{Tests of independence: A first look starring alternate universes}\label{test-indep}}

So far, we have looked at some data in two-way tables and judged them by inspection. That is, we looked at them and said, these two responses \emph{appear} to be deterministically related, or they appear to be associated but not deterministically, or they appear to be independent. But we acknowledged that our data come from a sample, and that on another day (on in another universe with rules the same as this one), we might have observed slightly different data.

It is worth noting that a single dataset often can't tell us for sure whether two variables are independent or associated (aka dependent/contingent), and whether or not an association is a deterministic one. (Two variables cannot be deterministically independent; that would be a self-contradiction.)

In this chapter, we'll take a first look at how a particular two-way table \emph{might} have come about, and thus how it might have come out differently. Let's suppose that we observe the data summarized here:

\begin{table}[!h]

\caption{\label{tab:indep-sim}Another alternate universe}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & chunky & smooth\\
\midrule
over & 17 & 6\\
under & 5 & 12\\
\bottomrule
\end{tabular}
\end{table}

It looks like certain pairs of outcomes are coming up more than we would expect ``by chance'' if the answers were not associated. Equivalently, it looks like there is an association between these variables, but let's investigate further. Let's take the cell with the largest number of people in it, or the modal value. In this case, the mode occurs for people who answer over and chunky. Which was 17 out of 40.

Now consider what contributes to making this particular value what it is. For example, what would make it bigger? Well, if there were more over-hangers than under-hangers, that would tend to increase the number of people who could be \emph{both} over-hangers and chunky-spreaders. Similarly, if there were more chunky-spreaders. Also, if there were just more people in our sample, then of course we could have more people in this cell of the table. And all of the above arguments could work in reverse, if we lowered any of those particular numbers.

What if, for argument's sake, we fixed the total number of people, the number over-hangers among them, and the number of chunky-spreaders among them. We're going to keep these values constant while letting the individual table elements vary. The technical term for this, by the way, is fixing the \textbf{marginal} values. This is because the sums of the columns (22 and 18) and the sums of the rows (23 and 17), when written at the bottom and side of the table, are called margins (just like the margins of a page). We could still have fixed the marginal values and have different values in the table. For example,

\begin{table}[!h]
\caption{\label{tab:unnamed-chunk-8}Yet more alternate universes!}

\centering
\begin{tabular}[t]{lrr}
\toprule
  & chunky & smooth\\
\midrule
over & 19 & 4\\
under & 3 & 14\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & chunky & smooth\\
\midrule
over & 13 & 10\\
under & 9 & 8\\
\bottomrule
\end{tabular}
\end{table}

Notice that the row and column sums are the same. But on the left we would say an association between the variables appears even more clear, whereas on the right, it appears less obvious. Can you see why?

Recall that the actual data summarized in a two-way table originally came from some observation sample. There were 40 people sampled, and they were each asked two questions. Before we tabulated them, the data might have looked something like this:

\begin{table}[!h]
\centering
\begin{tabular}{l|l}
\hline
Peanutbutter & Toiletpaper\\
\hline
Smooth & Under\\
\hline
Chunky & Over\\
\hline
Chunky & Under\\
\hline
Chunky & Over\\
\hline
Smooth & Under\\
\hline
\end{tabular}
\end{table}

Now suppose we wrote every person's response to the peanut butter question on an index card, shuffled them randomly, and then re-distributed them. We then did the same for the toilet paper question. We did this one question at a time, separately. Finally, we ask our sample to show us their cards, and we treat these paired observations as a new data set. We generate a new two-way table from the results. If we do all this, then we will have kept the total number of people the same, as well as the total number of ``over'' answers and ``chunky'' answers. Moreover, since we shuffled responses to each question separately, there is absolutely no way for someone's randomly assigned answer to one question (the index card they got for peanut butter) to influence their randomly assigned answer to the other (the index card for toilet paper). The answers should, by definition, be independent.

\hypertarget{rise-of-the-machines}{%
\section*{Rise of the machines}\label{rise-of-the-machines}}
\addcontentsline{toc}{section}{Rise of the machines}

Well, we can do this shuffling experiment very easily on a computer. And we can very easily repeat it 1000 times. So, the question we might then ask is, how many times out of 1000 such \textbf{simulations} do we get a value for over and chunky that is as large as (or bigger even) than 17?

The code to do this, and to visualize the outcome, is below. (You do not need to understand this code yet, but some of it might make sense to you).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulated_overchunky =}\StringTok{ }\KeywordTok{c}\NormalTok{() }\CommentTok{#initialize empty vector}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{)\{}
  \CommentTok{# the sample() function does the shuffling}
\NormalTok{  shuffle_data}\OperatorTok{$}\NormalTok{Peanutbutter =}\StringTok{ }\KeywordTok{sample}\NormalTok{(shuffle_data}\OperatorTok{$}\NormalTok{Peanutbutter) }
\NormalTok{  shuffle_data}\OperatorTok{$}\NormalTok{Toiletpaper =}\StringTok{ }\KeywordTok{sample}\NormalTok{(shuffle_data}\OperatorTok{$}\NormalTok{Toiletpaper)}
  \CommentTok{# the table() function does the tabulating}
\NormalTok{  simulated_overchunky[i] =}\StringTok{ }\KeywordTok{table}\NormalTok{(shuffle_data)[}\StringTok{"Chunky"}\NormalTok{,}\StringTok{"Over"}\NormalTok{]}
\NormalTok{\}}

\CommentTok{# plot the results using a histogram}
\KeywordTok{hist}\NormalTok{(simulated_overchunky, }
     \DataTypeTok{main =} \StringTok{""}\NormalTok{,}
     \DataTypeTok{xlab =} \StringTok{"Count"}\NormalTok{, }
     \DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{20}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\DecValTok{17}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{, }\DataTypeTok{col=}\DecValTok{3}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{carpedatum_files/figure-latex/shuffle-test-1} 

}

\caption{Counts of ``over and chunky'' after 1000 shuffling simulations}\label{fig:shuffle-test}
\end{figure}

Figure \ref{fig:shuffle-test} is histogram, or a bar plot of count data. We will cover histograms in detail the next chapter, so don't worry too much if some of this is still confusing. The main thing to notice is that green dotted line at 17, and this: in our 1000 shuffling experiments, we got values greater than or equal to 17 only 5 times. (You probably can't \emph{see} this in the histogram, but if you run the code, you can ask R to report this: \texttt{length(which(simulated\_overchunky\ \textgreater{}=\ 17))}) The rest of the time, the values were smaller, and typical values were around 13.

This simulation we just carried out is a way of validating our intuition, from inspection of Table \ref{tab:indep-sim}, that 17 was a high value \emph{if toilet paper orientation and peanut butter preference} were indeed independent. What we showed is that, if indeed the two variables were independent, that such values would be observed only rarely. Since this value was observed on our first day out in the park (in this scenario), we begin to doubt independence. If on the other hand, we had observed 13 over-chunky people (out of 40, and with the same marginal values as before), then we would say that this observation seems quite consistent with independence. Because values near 13 occur very often in our shuffling simulation.

\hypertarget{hypothesis-testing-just-happened}{%
\section*{Hypothesis testing just happened}\label{hypothesis-testing-just-happened}}
\addcontentsline{toc}{section}{Hypothesis testing just happened}

What we just did is in fact an example of a \textbf{hypothesis test}. In the statistical framework of hypothesis testing, we have a \textbf{null hypothesis} (usually, a skeptical position) and an \textbf{alternative hypothesis}. Here our null hypothesis was that the two variables represented in our response data are independent. Our alternative hypothesis was that people who are over-hangers are truly more likely to be chunky-spreaders. We didn't state this hypothesis at the outset. But it was implied by the fact that we were investigating this ``high'' value in the contingency table. If you take a regular statistics class, however, you will see that, when it comes to \emph{inference}, it is quite important to take care in constructing your null and alternative hypotheses. For now, we just want to get the main idea.

The main idea was that we simulated what data might look like under the null hypothesis (independence) and checked if our observations were consistent with that or not. Consistent (with the null) results would mean that we cannot ``reject'' the null hypothesis, while highly inconsisent results give us justification to ``reject the null'' in favor of the alternative. In this example, we would probably reject the independence assumption (null) in favor of the alternative. The independence assumption was consistent with values around 13 plus or minus 3 (i.e., the range of 10-16). But 17 is unusually high.

\hypertarget{caveats}{%
\section*{Caveats}\label{caveats}}
\addcontentsline{toc}{section}{Caveats}

\textbf{Rare events still do happen, rarely.} What our simulation showed us is that high values do still happen by chance. Even if the variables were independent, if we sent 1000 people out in the world to collect data from samples of 40 people each, \emph{some} of those people would observe large values that \emph{look} associated. This is to be expected. This is why it is important, if we are seeking the truth, that we not game the system by checking over and over again until we get an answer that we like. This would be like rolling double-sixes on the fourteenth try and then saying ``ha! see, I told you I was lucky.''

\textbf{What about the other cells in the table?} You might be wondering how we can do this whole simulation just for the one cell in the table that corresponded to over-chunky. You're right to wonder this (if you are). In time, we will see that we can do independence-tests (or other hypothesis tests) more democratically by examining all of the cells in the table and how they differ from what we would expect under the independence (null) hypothesis. For now, though, note the following. Each of our variables is dichotomous, and we fixed the row and column marginals (totals). So if over-chunky is high, and total count of chunky is held fixed, that means that under-chunky must be low. Our test would have gone the same way if we sought to explain that cell. So, too, must over-smooth be low (lower than it would be under independence assumptions).

\hypertarget{sampling-simulations-and-randomness}{%
\section*{Sampling, simulations, and randomness}\label{sampling-simulations-and-randomness}}
\addcontentsline{toc}{section}{Sampling, simulations, and randomness}

In the last section, we used the \texttt{sample()} function in R to shuffle some values in our simulation. That is, starting with a data set of 40 responses (for example, 23 ``over'' responses and 17 ``under'' responses), we wanted to reassign these responses randomly among the people in our observation sample. What if we wanted to simulate the original set of answers in the first place? It turns out we can also use the computer without collecting any data at all.

To reproduce the exact example above, it would suffice to write down a series of values like this (over, over, over, over, \ldots{}, over, under, under, \ldots{} , under), 23 overs and 17 unders, and then shuffle them. Boom. The first shuffle is our simulated data. But that procedure wouldn't work if we wanted a sample of 3, would it? If we want to be able to simulate data with different sample sizes, we need another approach.

\hypertarget{tossing-virtual-coins}{%
\subsection*{Tossing (virtual) coins}\label{tossing-virtual-coins}}
\addcontentsline{toc}{subsection}{Tossing (virtual) coins}

Let's simplify things for a minute and assume that in actuality, people are equally likely to be over-hangers or under-hangers. Or that for whatever dichotomous question we want to ask, the likelihood of either responses is 50\%.
If you want to simulate an event that has a 50\% chance (or probability of 0.5), you can toss a coin. If we want to randomly assign 10 people to one of two groups, say the Sharks and the Jets, we can do the following. First, decide that Heads we choose Sharks (this is, of course, arbitrary). Then, for each person, flip a coin. If heads, then assign them to Sharks. If tails, assign them to Jets.

Another way to do this is to put two poker chips into a hat, say a red chip (for Sharks) and a blue chip (for Jets). Then blindly reach into the hat and pick out one chip, record the value, and then put it back and shake the hat before the next draw.

Pulling a chip out of a hat or tossing a coin are equivalent manifestations of the \emph{ideal} process of sampling a random event with a 50\% chance. This is actually a profound idea. The coin toss or hat draw are concrete, mechanical processes. Each has two possible outcomes (heads/tails or red/blue), and, for all intents and purposes, the outcomes in both cases are equally likely. We human beings have learned to abstract from both of these mechanical processes to an idea of a \textbf{stochastic process} which is the same thing as a random process and the exact opposite of a deterministic process. This abstract process requires that we have a \textbf{state space}, which is just a \textbf{set of possible outcomes} (mathematicians refer to some sets as spaces). As long as the set has two possibilities, it doesn't really matter whether we label them heads/tails, over/under, red/blue, Sharks/Jets, or 0/1, because the label can always be assigned to mean whatever we want it to mean (just as we decided heads corresponds to Sharks). In addition to the state space, we abstract the idea of the \textbf{sampling process}. This is what ``picks'' out one of the possible outcomes. Like the coin-flip or the hat-draw. And it is this sampling process where the probability of the outcome appears.

We can in fact simulate this sampling process on the computer in R. In fact we can do it several different ways. Here, first, is a more or less direct translation of what our mechanical process would do. The key enabling function here is called \texttt{sample()}, which works just like pulling chips out of a hat. We will tell \texttt{sample()} what's in the hat by defining a set (a vector, in R) called \texttt{coinstates}. Then we will sample once from the hat for each person.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{8675309}\NormalTok{) }\CommentTok{# I'll explain this later }

\NormalTok{coinstates <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"H"}\NormalTok{,}\StringTok{"T"}\NormalTok{)}
\NormalTok{numPeople <-}\StringTok{ }\DecValTok{10}

\CommentTok{# start a blank list to hold my team assignments}
\NormalTok{teamassignments <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}

\CommentTok{# now, go one by one; this is called a programming "loop"}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{numPeople) \{}
  \CommentTok{## toss coin and observe}
\NormalTok{  coinresult <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(coinstates, }\DecValTok{1}\NormalTok{) }\CommentTok{# the 1 means sample once}
  \CommentTok{# associate outcome with team assignemnt}
  \ControlFlowTok{if}\NormalTok{ (coinresult }\OperatorTok{==}\StringTok{ "H"}\NormalTok{) \{}
\NormalTok{    team <-}\StringTok{ "Sharks"}
\NormalTok{  \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (coinresult }\OperatorTok{==}\StringTok{ "T"}\NormalTok{) \{}
\NormalTok{    team <-}\StringTok{ "Jets"}
\NormalTok{  \}}
  \CommentTok{# record team assignment}
\NormalTok{  teamassignments <-}\StringTok{ }\KeywordTok{c}\NormalTok{(teamassignments, team)}
\NormalTok{\}}

\NormalTok{teamassignments}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "Jets"   "Sharks" "Jets"   "Sharks" "Jets"   "Sharks" "Sharks" "Sharks"
##  [9] "Jets"   "Sharks"
\end{verbatim}

Voila. The \texttt{sample} function did the apparent work of the coin flip or, equivalently, picking a ``Heads'' or ``Tails'' out of a hat containing both. Just as good, right? Notice that (a) we only got four Jets, not five, and (b) sometimes we got three Sharks in a row. That's randomness for you.

\begin{quote}
Technically, it is pseudo-randomness. Notice the first line of the code where I set the ``seed'' for the random number generator. What this does is make sure that every time I run this code I get the same results. If that seems not random to you, I don't blame you. But here are some important things to keep in mind. First, even though using this random seed makes my experiment repeatable, I still have no idea what the results are going to be until I run it the first time. Second, if I change the seed, even by a tiny amount, then I will once again have no idea. Try it yourself. Note that if you do not declare a random seed, then the computer will effectively choose one for you, perhaps by using the exact computer time in milliseconds. This way the results will still be different every time you run it. If you want to read (a little bit) more about randomness, see the Appendix.
\end{quote}

If you're thinking we could have removed the coin from this process, you're right. Coins are not essential to this process. We could imagine a mechanical device that drops marbles into jars directly, provided that we believed the marble dropping process was equivalent to the coin flip in terms of 50/50 chances. In R, here is a faster way. We just sample out of a ``hat'' containing each of the team names.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{8675309}\NormalTok{) }\CommentTok{# same seed, on purpose}

\NormalTok{teamnames <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Sharks"}\NormalTok{,}\StringTok{"Jets"}\NormalTok{)}
\NormalTok{numPeople <-}\StringTok{ }\DecValTok{10}

\NormalTok{teamassignments <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(teamnames, numPeople, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{teamassignments}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "Jets"   "Sharks" "Jets"   "Sharks" "Jets"   "Sharks" "Sharks" "Sharks"
##  [9] "Jets"   "Sharks"
\end{verbatim}

Well, that was simpler. Notice also that by using the same random seed, I got the same team assignments, even though the code was completely different. (By the way, the {[}1{]} and {[}9{]} here are just index--count--values that R writes out when the data flow across multiple rows. So you know that the first value on the second row is in fact the 9th element of the vector.)

There's another difference here, which is that instead of going one by one for each person, I made the selection all at once by passing the number of people to the sample function instead of asking for one sample. Because I'm sampling \texttt{numPeople} times and there are only two outcomes (``in the hat''), I have to add the argument \texttt{replace\ =\ TRUE} when I call the \texttt{sample()} function. This means that after I look at the result, I put it back in the hat (I replace it), and then draw again. Otherwise, I could sample only as many times as I have choices.

Sampling \emph{without replacement} is suitable if, as in the independence test from earlier, all I wanted was to put a set of choices in random order. As we said, this is like shuffling a deck of cards by dropping the deck into a big bag, shaking it, and then pulling the cards out one by one. No replacement.

\hypertarget{but-some-observations-do-occur-more-than-others}{%
\subsection*{\texorpdfstring{But some observations \emph{do} occur more than others}{But some observations do occur more than others}}\label{but-some-observations-do-occur-more-than-others}}
\addcontentsline{toc}{subsection}{But some observations \emph{do} occur more than others}

To complete the functionality of our data-generating machine, we need a way to extend this sampling process to outcomes that are not equally likely. What if we want to base a simulation on the observed proportion that 23 out of 40 people chose ``over'' and only 17 chose ``under'', but we only want to sample 10 people?

Once more, I will show you two ways to do this. The first way is to put all 40 virtual cards into our virtual hat, and sample 10 times (with or without replacement? what do you think?). The second way involves abandoning our connection to these virtual cards and replacing them with the mathematical abstraction of a probability.

First, the virtual cards:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2125551212}\NormalTok{) }\CommentTok{# I can choose any see I want}

\NormalTok{tp_cards <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\StringTok{"over"}\NormalTok{, }\DecValTok{23}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\StringTok{"under"}\NormalTok{, }\DecValTok{17}\NormalTok{))}
\NormalTok{numPeople <-}\StringTok{ }\DecValTok{10}

\NormalTok{simulated_tp <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(tp_cards, numPeople, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{simulated_tp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "over"  "over"  "under" "under" "under" "under" "under" "over"  "under"
## [10] "over"
\end{verbatim}

Hopefully this code seems pretty straightforward. Why did I use \texttt{replace=TRUE}? I didn't need to do that, since I was only drawing 10 samples from a hat with 40 cards. However, by replacing the card I drew each time, I made sure that the chances were the same for each virtual person in my sample.

Suppose that I did not replace the samples each time. And suppose that, by dumb luck, I drew five ``unders'' in a row on my first five draws (this sequence did actually happen on draws 3-7 above). The sixth draw is now coming from a hat with 23 overs and 12 unders. That draw definitely has different chances of coming up under, and, importantly, it is not independent of what happened before. In real-world sampling, I do not expect the answer from the sixth person I ask to depend on the answers given by the previous five. So it is essential that my simulation have this same property. (Of course, it is true that the proportions in the hat changed even after the very first draw, assuming I don't replace the cards, but I used the sixth draw to make it more obvious.)

\begin{quote}
Think about this: Now that we have added replacement to the sampling process, if we change the sample size to 40 (\texttt{numPeople\ \ \textless{}-\ 40}), are we guaranteed to get 23 overs and 17 unders?
\end{quote}

Now for the second way. I don't really need to produce 40 cards at all. I just need to recognize that 23/40 or 0.575 is my target probability of ``over'' on every single sample. (17/40 or 0.425 is the probability of ``under'' automatically). I can still do everything with the good old \texttt{sample()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2129981212}\NormalTok{) }\CommentTok{# I can choose any seed; I like using phone numbers.}
\NormalTok{numPeople <-}\StringTok{ }\DecValTok{10}
\NormalTok{simulated_tp <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"over"}\NormalTok{,}\StringTok{"under"}\NormalTok{), numPeople, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{prob=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.575}\NormalTok{, }\FloatTok{0.425}\NormalTok{))}
\NormalTok{simulated_tp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "under" "over"  "over"  "over"  "under" "under" "under" "under" "under"
## [10] "over"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2129981212}\NormalTok{) }\CommentTok{# if I want to same results}
\NormalTok{simulated_tp <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"over"}\NormalTok{,}\StringTok{"under"}\NormalTok{), numPeople, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{prob=}\KeywordTok{c}\NormalTok{(}\DecValTok{23}\OperatorTok{/}\DecValTok{40}\NormalTok{, }\DecValTok{17}\OperatorTok{/}\DecValTok{40}\NormalTok{))}
\NormalTok{simulated_tp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "under" "over"  "over"  "over"  "under" "under" "under" "under" "under"
## [10] "over"
\end{verbatim}

There it is. For good measure, I included the probability expression using decimals and fractions, to prove that it doesn't matter to R. I did, however, reset the random seed, otherwise I would not have gotten the same results.

\hypertarget{recap}{%
\section*{Recap}\label{recap}}
\addcontentsline{toc}{section}{Recap}

In this chapter, we used computer simulation to overcome one of the limitations of a single data collection with a small sample. We introduced the R function \texttt{sample()} which can be used to either shuffle a bunch of data in random order or to select (i.e.~sample) some data from a larger set. We saw a couple of different ways of doing this, some in which we literally create the large set and some in which we use probabilities instead that ``act like'' we have a larger set. More importantly to the original goal, using the R function \texttt{sample()} we saw that we can create many replications of simulated data collections. We can use these replications to see what answer-pair observations are consistent with independent answering processes and what observations seem highly unlikely if the answers are truly independent.

Notice that we keep using this language of likely/unlikely, consistent/inconsistent, etc. None of our results ``prove'' independence or association between question pairs. That's just not possible with stochastic/random processes and finite samples. We need to be able accept this condition of uncertainty. Note, though, that if we get larger and larger samples, there are some statements that we can make with more and more confidence. Statistics does not eliminate uncertainty, but it can be used to put precise bounds on it.

\hypertarget{shades}{%
\chapter{(An Infinite Number of) Shades of Gray (or Brown)}\label{shades}}

We've taken the two-kinds-of-people idea pretty far already. But it's time to acknowledge the elephant in the room. Not every question about attributes, preferences, or behaviors can be answered in such an either/or manner. Digitidiness might be one of those things. Consider the following dialogue:

\begin{quote}
Stacy: ``There are liberal and conservative kinds of people, Trang. Which one are you?''
Trang: ``Well, you know I'm not sure I'm exactly one or the other. I think I'm somewhere in the middle.''
\end{quote}

Although we often use them as \textbf{discrete categories}, the words liberal and conservative might be better thought of as endpoints of \textbf{continuous scale}. In fact, they might even apply to different \emph{dimensions} of political thought with respect to social issues or economic issues. If you think about it, it's not hard to come up with other examples of ``categories'' that really just describe one end or another of a continuous scale. Yes, there are short people and tall people, but everyone has a height, and a lot of people are ``about average.'' Height is just a number on some scale. So it wouldn't necessarily make sense to put people into the categories of tall or short.

This may be an old-timey analogy in the age of digital streaming, but think of the knobs on a stereo receiver. Some of them click between categories, like the input-selector (phono, radio, aux). And some of them turn smoothly through continous values, like volume and tone. Categorical, or discrete, variables are the clicky knobs.

In the great toilet paper debate, we were able to identify two kinds of people based on two possible responses to the question of roll orientation. Two answers; two kinds. If instead of discrete categories, we have a number on a continuous scale, does that mean that there can't be ``kinds'' of people anymore? To answer this question, we'll need to understand what exactly we're talking about when we characterize people using a continuous scale.

\hypertarget{poopiness}{%
\section*{Poopiness}\label{poopiness}}
\addcontentsline{toc}{section}{Poopiness}

Consider poopiness. And consider a scale where some people are really poopy (close to poopiness = 1), some aren't poopy at all (close to 0), and many are somewhere near the middle. That's not a very quantitative description. I used the words ``some'' and ``most'', but I didn't give you counts like I did in Table \ref{tab:tp-table} about toilet roll orientation. I will try to do that in just a minute. Meanwhile, notice that my scale here runs from 0 to 1, which I will also sometimes write as {[}0,1{]}. When it comes to height, we have well-established scales, like inches or centimeters, and we can use measuring sticks. But when it comes to liberalism or poopiness, the scale does not necessarily refer to something we can see directly. Nevertheless we can use the scale to compare people and to see how a whole bunch of people ``measure up.'' I've set the scale to {[}0,1{]}, because it is a common scale, but it could have run from 1 to 10, for example, without significantly changing anything in what I'm about to say.

If I showed you the poopiness data for a sample of people, the list would look something like Table \ref{tab:poopy-counts}. As before, in this table each row stands for one person. To protect their identities, everyone is identified only by a number (e.g., 0083), which is shown in the first column. In the second column is each person's poopiness value.

\begin{table}[!h]

\caption{\label{tab:poopy-counts}Don't ask me how I got these numbers.}
\centering
\begin{tabular}[t]{lr}
\toprule
  & poopiness\\
\midrule
0040 & 0.319\\
0140 & 0.703\\
0033 & 0.401\\
0107 & 0.544\\
0031 & 0.538\\
\addlinespace
0100 & 0.657\\
\bottomrule
\end{tabular}
\end{table}

Poopiness is shown as a decimal number. Part of the reason I've used this scale, instead of 1-100, is to emphasize that the data values can be arbitrarily close to one another. Two values may be different by 0.1 or 0.03, or even 0.000027, if we have enough precision in our data to say such a thing. These data are called \textbf{numerical} or \textbf{quantitative} as opposed to \textbf{categorical}. There are actually 148 values in the data set, but I've only shown the first six in Table \ref{tab:poopy-counts}.

It's not as easy to make sense of a bunch of decimal values like this as it is to look at simple counts of categories (like 17 for chunky, 23 for smooth). However, this sense-making problem has been solved by representing the same data using dot plots, stacked dot plots, frequency tables, and histograms, which you can read all about in any standard textbook (for example OpenIntro Statistics, Chapter 2). I'm going to go straight into the \textbf{frequency table} and \textbf{histogram}, which you may indeed have seen before. These are the most commonly used representation for data of this kind.

Again, it is a bit awkward to count how many people have poopiness value of exactly 0.473. Maybe there is one, maybe none. How would we interpret that answer, anyway? Instead, we can group values into ranges, or ``bins'', e.g.~0-0.05, 0.05-0.1, 0.1-0.15, etc. so that the ranges together span the entire possible range, which in this case is {[}0,1{]}. We can then count how many of our data fall into each bin.\footnote{Technically, each range is a semi-open interval, e.g. (0.1,0.15{]}, so that any values exactly equal to 0.1 can only be included in one bin and not the ones on either side.} This table of counts is typically called a frequency table. Frequency is just another word for counts.

\begin{table}[!h]

\caption{\label{tab:poopy-freq}Frequency Table for Poopiness}
\centering
\begin{tabular}[t]{lr}
\toprule
Range & Frequency\\
\midrule
0 - 0.05 & 0\\
0.05 - 0.1 & 0\\
0.1 - 0.15 & 0\\
0.15 - 0.2 & 0\\
0.2 - 0.25 & 2\\
\addlinespace
0.25 - 0.3 & 9\\
0.3 - 0.35 & 12\\
0.35 - 0.4 & 9\\
0.4 - 0.45 & 9\\
0.45 - 0.5 & 14\\
\addlinespace
0.5 - 0.55 & 14\\
0.55 - 0.6 & 9\\
0.6 - 0.65 & 19\\
0.65 - 0.7 & 9\\
0.7 - 0.75 & 13\\
\addlinespace
0.75 - 0.8 & 16\\
0.8 - 0.85 & 7\\
0.85 - 0.9 & 5\\
0.9 - 0.95 & 1\\
0.95 - 1 & 0\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth]{carpedatum_files/figure-latex/poopy-hist-1} 

}

\caption{Histogram of Poopiness}\label{fig:poopy-hist}
\end{figure}

A histogram is a bar plot of counts for values that fall into certain numerical ranges. So it's a bar plot of the data in Table \ref{tab:poopy-freq}. But oftentimes you'll just see the histogram without the frequency table. It is worth noting that the information contained in the frequency table is equivalent to the information contained in the histogram, but it is sometimes easier to get a general idea of what is going on in the data by looking at the histogram.

Consider the range of poopiness values from 0.40-0.45. Our data set has 9 values in this range, as you can see in Table \ref{tab:poopy-freq}, so the height of the bar above this range of values on the x-axis (horizontal axis) is 9. I've colored it in pink only to help you see what I'm referring to. The y-axis in Figure \ref{fig:poopy-hist} is labeled ``Frequency'', as in the table. Some more jargon: the numerical values that separate the bins are called ``breaks.'' In Figure \ref{fig:poopy-hist}, the breaks occur are at increments of 0.05.

\begin{quote}
Question: Given that there are 20 possible bins in the histogram in Figure \ref{fig:poopy-hist}, but only some of them have non-zero counts, are there 20 kinds of people (in terms of poopiness) or 15 kinds of people?
\end{quote}

Trick question? You bet. The breaks (and thus bins) in a histogram are arbitrary. I can choose any breaks I want, as long as all of the data points fall into exactly one bin. (I can't just exclude some bins, though. That would be cheating.) The histograms in Figure \ref{fig:poopy-hist-alt} are both perfectly valid histograms. One of them has four bins, and one of them has only two bins.

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth]{carpedatum_files/figure-latex/poopy-hist-alt-1} 

}

\caption{Other Histograms of Poopiness}\label{fig:poopy-hist-alt}
\end{figure}

It's tempting to take the counts on the right of Figure \ref{fig:poopy-hist-alt} and declare that there are two kinds of people. After all, this gets us back to familiar territory. Ta-dah!

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth]{carpedatum_files/figure-latex/poopy-two-kinds-1} 

}

\caption{This is a terrible, horrible, no-good, very-bad thing to do.}\label{fig:poopy-two-kinds}
\end{figure}

As you can tell, because it says so right in the figure caption, this is terrible, horrible, no-good, very-bad thing to do. Why do you think it is a bad thing to do? Choose one:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  The split was made at 0.5 on the poopiness scale, but that is not the average value of poopiness in the data set, which is closer to 0.57, as can be seen in Figure \ref{fig:poopy-hist} (or from the ``raw'' data themselves).
\item
  You should always use at least 5 bins when you have numerical data
\item
  Representations of data should communicate honestly about the nature of the data themselves. In this case, poopiness is not a category.
\end{enumerate}

What I did here was take a numerical/quantitative value (poopiness) and mis-represent it as a categorical value. I did it by \emph{dichotomizing} it, i.e., by splitting off everyone above 0.5 and labeling them as ``poopy''. I could have alternately split at the mean or median value and labeled the resulting two groups as ``low poopiness'' and ``high poopiness.'' But this would still have been a mis-representation. It would hide the fact that poopiness comes in a continuous range of values.

\begin{quote}
ASIDE (\emph{delivered in a hushed voice}): I won't be able to convince you of this now, but it turns out that if you do this---if you dichotomize numerical data---you will BREAK STATISTICS! Ok, that sounds a bit dramatic. But in all seriousness, one of the jobs of statistics is to understand associations between different variables, such as poopiness and, say, earning potential. If you treat poopiness (or other variables) as discrete when they are really continuous, you may very well get the wrong answers. As the man down the street from where I used to live often muttered to himself while waving his arms in the air, THAT IS AN ABSOLUTE IRONCLAD MATHEMATICAL FACT. No, but in all seriousness, there is a very good paper on exactly this subject \citep{maccallum2002}.
\end{quote}

Dang it! you say. You've taken me down this rabbit hole of poopiness for too long. How many kinds of people are there? Are you saying that if one looks at properties that are described by numbers instead of categories, that are no kinds of people at all? Is it all just shades of gray (or brown)?

\hypertarget{mixtures}{%
\subsection*{Mixtures}\label{mixtures}}
\addcontentsline{toc}{subsection}{Mixtures}

Remember Figure \ref{fig:poopy-hist}? (Don't click it!) Here it is again so you don't have to scroll back. Data scientists like to say this picture shows you the \textbf{distribution} of poopiness in our sample. Statisticians use the word distribution in a more formal way that is best put off until we actually need it. We don't need it yet.

\begin{center}\includegraphics[width=0.9\linewidth,]{carpedatum_files/figure-latex/poopy-hist2-1} \end{center}

What if I told you that there ARE two kinds of people; you just can't see them unless I give you special glasses (or more information). If I gave you special glasses (or information), you would see this:

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,]{carpedatum_files/figure-latex/poopy-hist-mixture-1} 

}

\caption{A mixture of poopiness}\label{fig:poopy-hist-mixture}
\end{figure}

\emph{By what dark magic have you colorized the data!} you say. Or, perhaps you just said, hm, interesting.
In Figure \ref{fig:poopy-hist-mixture}, I've made a histogram with bars in two different colors, light green and pink. The colors are slightly transparent so that you can see both the green and pink distributions in their entirety even though they overlap. That's what the brownish bars mean. You're looking at the overlap of the green and pink bars, not another set of bars. Now, if you compare this histogram closely with the original, colorless histogram above, you'll see that the bin ranges are the same (width=0.05), and the the counts of green and pink bars add up to the total values that we had before. If there are green people and pink people, or in any case two different kinds of people, and if their poopiness is distributed as shown in Figure \ref{fig:poopy-hist-mixture}, then the poopiness of the mixture of these two groups of people will look just like Figure \ref{fig:poopy-hist}.

Ok, but that doesn't explain how you would know that there are two groups. If I didn't tell you. That's because \emph{you wouldn't necessarily know. You would need to have more information}. Now you might suspect something if you saw a distribution that looked like this:

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,]{carpedatum_files/figure-latex/poopy-hist-mixture-suspicious-1} 

}

\caption{A suspicious mixture of poopiness}\label{fig:poopy-hist-mixture-suspicious}
\end{figure}

In Figure \ref{fig:poopy-hist-mixture-suspicious}, the distribution has a double-hump like a Bactrian camel. In spite of that, it is not called a Bactrian distribution--which would make me happy--but a \textbf{bimodal} distribution. The point that I'm trying to make here is that a bimodal distribution makes you suspect that there could actually be (at least) two groups mixed together in our data.

But the original data for poopiness did not look bimodal. I suggested to you that you would need more information to determine if there are two groups. And so, I present you with\ldots{} Crappiness!

\hypertarget{crappiness}{%
\section*{Crappiness}\label{crappiness}}
\addcontentsline{toc}{section}{Crappiness}

For each of the subjects in our poopiness data set, we have also collected data on their crappiness. Crappiness is also reported as a numerical value ranging from {[}0,1{]}. It's sort of like poopiness, but different. Here are some values:

\begin{verbatim}
##      poopiness crappiness
## 0040     0.319      0.564
## 0140     0.703      0.415
## 0033     0.401      0.729
## 0107     0.544      0.374
## 0031     0.538      0.853
## 0100     0.657      0.316
\end{verbatim}

And here\ldots{}(drum roll please)\ldots{} is a histogram of crappiness!

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,]{carpedatum_files/figure-latex/crappy-hist-1} 

}

\caption{Histogram of Crappiness}\label{fig:crappy-hist}
\end{figure}

Hmm. I bet you were hoping that the crappiness data would look obviously bimodal, but it's not obvious. Nevertheless, hopefully you trust that I wouldn't lead you on a wild goose chase for no reason. Perhaps you can even see it coming. If we look at poopiness and crappiness separately, there is no clue that there might be distinct groups of people in our data set. But if we look at them together\ldots{} there is.

When we looked at categorical data for two two-kinds-of-people questions, we made 2x2 contingency tables. We also used the word ``dimension'', for example to say that we were describing people along two dimensions (recall: toilet paper and peanut butter). Now that we are looking at numerical data (poopiness and crappiness), we can also use two dimensions, as in a two-dimensional scatterplot, to examine both variables at once.

A scatterplot is just a name for a data plot, in which the position of each data point corresponds to its coordinates along more than one dimension. We often refer to two-dimensional coordinate systems as (x,y), where x is the horizontal axis and y is the vertical axis. Technically, in this case our coordinate system is (poopiness, crappiness). These are the names of our dimensions. But we will still often refer back to the idea of an x- or y-axis. This scatterplot is shown in Figure \ref{fig:poopy-crappy}. Each point represents data from one person, with their poopiness value on the x-axis and crappiness on the y-axis.

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,]{carpedatum_files/figure-latex/poopy-crappy-1} 

}

\caption{Scatterplot of Crappiness vs Poopiness}\label{fig:poopy-crappy}
\end{figure}

Alas, oh data! Your bimodal nature has revealed itself in the higher-dimensional plane!

How many kinds of people are there? When it comes to poopiness and crappiness, people exhibit a continuous range of values, so we can't neatly put them into buckets. Neither poopiness nor crappiness appear to be bimodally distributed on their own. However, when examined together, as in the scatterplot in Figure \ref{fig:poopy-crappy}, a pretty suggestive pattern emerges in the data. There are two \textbf{clusters} of points, one group of which is lower in poopiness but higher in crappiness than the other. Interestingly, though, in both groups poopiness and crappiness tend to increase together. That is, they appear to be associated, not independent.

I do not mean to imply that clusters of points can always be found if we have data along many dimensions. That is certainly not always the case. The present example was concocted (I admit it!) to show that groups \emph{can} emerge, even in numerical data. Cluster analysis \citep{kaufman2009} refers to set of data-science methods all about looking for the existence of groups in multidimensional data.

\hypertarget{check-your-understanding}{%
\subsection*{Check your understanding}\label{check-your-understanding}}
\addcontentsline{toc}{subsection}{Check your understanding}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Based on the scatterplot in Figure \ref{fig:poopy-crappy} and the grouped-by-color histogram for poopiness in Figure \ref{fig:poopy-hist-mixture}, describe what the equivalent grouped-by-color histogram for crappiness would look like. Would it look the same or different? Explain.
\end{enumerate}

\hypertarget{cut-scores-and-abnormality}{%
\chapter{Cut Scores and Abnormality}\label{cut-scores-and-abnormality}}

\begin{quote}
Because that's not what normal people do.
--- things my spouse says
\end{quote}

You'll recall that I previously warned against possible negative consequences of setting arbitrary cut points to dichotomize a data set---that is, turning numerical data on a continuous scale into two categories by using a cutoff value. But now consider the following scenarios:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  To pass the written test for your a driving learner's permit in California, you must answer at least 38 questions correctly out of 46. That's 82.6\% correct. At 80.4\% (37/46) or below, you fail and have to retake the test on another day.
\item
  A patient's blood test shows levels of ALT (alanine aminotransferase) at 77 units per liter. The lab report labels this as ``abnormally'' high, and the physician is concerned about possible liver damage or disease.
\end{enumerate}

These two examples involve just the kind of dichotomization that I cautioned against, and yet they occur very commonly in practice. So what gives? Is it wrong to use cutoffs this way? Why do people do it?

The short answer is that we often find ourselves in need of a classification (pass or fail; diagnose liver disease or not) but without a perfect classification device. Rather we have only indirect measurements (of knowledge or liver function) in some quantitative measure. Perhaps you once found yourself on the ``border'' between letter grades for a course and were particularly perturbed (or relieved) by the imperfections of such a system. Or you may have found yourself with ``slightly'' abnormal levels in a blood test and wondered whether you should seek further tests.

Both the California department of motor vehicles and the physician in our scenarios need to make a decision based on imperfect evidence. They want to be able to say that the person's test results show that they are ready to get behind the wheel of a car, in one scenario, or suffering from liver problems in the other. But all they can really do is express this belief using a \textbf{probability}. This probabilistic judgement is based on a mathematical \textbf{model} that relates traits like readiness-to-drive or liver-disease to certain test results. Understanding how these models come into existence is one of the learning objectives of this course.

The term \textbf{normal distribution} arose in statistics because the particular bell-shaped distribution occurs so frequently. If poopiness were normally distributed in our sample from before it might look like this.

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,]{carpedatum_files/figure-latex/normal-poopy-1} 

}

\caption{Normal poopiness}\label{fig:normal-poopy}
\end{figure}

Technically speaking, all of the values, including the maximal value of 0.962 that we observe in Figure \ref{fig:normal-poopy} are normal. Poopiness varies in the population. It is impossible to be abnormally poopy, under the circumstances. By definition, some values at the extreme ends of a normal distibution are less likely to occur than values in the middle. But still they may occur rarely. It is only when extreme values (large or small) are associated with other conditions of interest, such as the relationship between elevated ALT and liver disease, that it makes sense to ``flag'' these extreme values.

We say that we \textbf{discretize} continuous variables (i.e., turn them into discrete categorical variables) by using thresholds or cut scores. Passing a test or being flagged for liver disease is usually based on a single cut score. The cut score is a numerical value, and data that fall above or below that value are categorized differently. It is possible to use more than one threshold. For example, in the next chapter we will see that people can be classified as belonging to different generations based on their age, and neighborhoods can be categorized based on their population density.

We started out this course on a quest to answer our first big question: How many kinds of people are there? En route, we have examined both categorical data, such as from two-kinds-of-people questions, and numerical values like poopiness. The toilet paper and peanut butter orientation questions may seem silly and inconsequential to you. I can only imagine what you might think of the poopiness and crappiness dimensions that I completely made up (I admitted it!). However, in the next chapter we will see how some more standard variables are used to profile American people. Next week, we will see more issues of discrete/categorical and continous/numerical multi-dimensional descriptions of people that arise when it comes to personality psychology.

\hypertarget{remember-these-terms}{%
\subsubsection*{Remember these terms?}\label{remember-these-terms}}
\addcontentsline{toc}{subsubsection}{Remember these terms?}

\begin{itemize}
\tightlist
\item
  kind, type, category
\item
  discrete, continuous, numerical, dichotomous
\item
  crosstab, two-way table, contingency table
\item
  association, contingency, dependence
\item
  latent factor, dimension, trait
\item
  measurement, model, histogram, bimodal, cluster
\end{itemize}

\hypertarget{the-modal-american}{%
\chapter{The Modal American}\label{the-modal-american}}

The NPR podcast Planet Money aired an episode in the summer of 2019 called \href{https://www.npr.org/2019/08/28/755191639/episode-936-the-modal-american}{The Modal American}. If you haven't listened to it, you should stop what you're doing and listen to it right now. (Seriously, this will make a lot more sense).

The podcast hosts, Kenny Malone and Jacob Goldstein, set the stage as follows. People sometimes talk about the ``average'' American, but that doesn't really make sense, even as an idea. If you average the traits of all Americans, you don't end up with a real human being.\footnote{We encountered this fact in the form of a joke in Chapter \ref{categories}, specifically \protect\hyperlink{no-mean-feat}{here}} Malone suggests that what people are really thinking about is the \emph{modal} American, i.e., the most common type of American. The kind person you're more likely to bump into on the street than any other. The hosts then consult with economics and data reporter Ben Casselman to help identify who is the modal American.

For the purposes of their investigation, the kinds of American the Planet Money hosts are interested in are determined by groupings of demographic variables like those collected by the US Census. The census does not, to my dismay, include questions about peanut butter preference, toilet paper orientation, digitidiness, or poopiness. Rather, they stick with things like sex, race, age, income, marital-status, etc. It turns out that different people mean different things when the ask, ``how many kinds of Americans are there?'' In any case, this journey to put Americans into buckets leads down some paths that will be very familiar to you by now.

\hypertarget{step-one-categories}{%
\subsection*{Step one: categories}\label{step-one-categories}}
\addcontentsline{toc}{subsection}{Step one: categories}

First of all, some of the variables, like sex, race, and marital status, are categorical to begin with. We can ask people to answer these survey questions, and then we can combine their answers into kinds like married-Hispanic-male or \href{https://www.imdb.com/title/tt0105414/}{single-White-female} types of people.

Some of the variables included, like neighborhood-type, are categorical in the way we think about them (read: rural, suburban, or urban), but are actually derived from numerical variables. In this case, neighborhood type is derived from the population density in the census block. Other variables, like income and, arguably, age, are just plain numerical. But to identify the most common kind of American, our podcast hosts need to put people into categories (like rich and poor--to make things dichotomous--or low-, middle-, and high-income). So they \textbf{discretize} all of the numerical variables into a small number of categories. They ``bin'' the incomes as well as the ages, which they combine into generation labels like ``Generation X'' and ``Baby Boomers.''

\hypertarget{step-two-contingency-table}{%
\subsection*{Step two: contingency table}\label{step-two-contingency-table}}
\addcontentsline{toc}{subsection}{Step two: contingency table}

Having done all this, the next step is to combine the data about all Americans along each of these variables into one big contingency table. It's not a two-way table, because there are more than two variables, but it's still a contingency table. (It is not very common to use terms like four-way or five-way table.)

The choice of variables, and specifically the choice of the number of categories to keep for each one, determines how many kinds of Americans there can be. Having made the choices, it is a simple matter of looking to see which is the most common kind.

Along the way, the idea of dependence, or association, also comes up, but it is not necessarily mentioned by name. Supppose you know the most common age category is Children. And suppose the most common category of marital-status is Married. It does not necessarily follow that married-children is the most common category in a two-way table made from those two variables. Of course, the opposite is true. And this is because age and marital status are associated variables. The older you are, the more likely you are to be (now or previously) married, as opposed to never married. In general, you cannot assume that the most common category in two variables will combine to produce the most common two-variable-category. \textbf{But this would be true if the variables were independent.}

For independent variables, it must be the case that the most common joint category will emerge from the simple combination of the most common individual categories. You could actually use that as a definition of independence.

Take sex and race, for example. In the Planet Money podcast, both of these are actually treated as dichotomous variables (all race categories other than white are combined into one, so it is basically white or not; perhaps you are hearing ``there are two kinds of people in this world\ldots{}''). Since female is the most common sex and white is the most common race category, it has to be the case that white-female comes out as the most common combination. Sex and race are independent.

Perhaps it is reassuring to you that the ideas we have been developing are not just limited to semi-jokey two-kinds of people questions, or Buzzfeed personality tests. But that they apply exactly as before to serious data.

\hypertarget{exercises}{%
\subsection*{Exercises}\label{exercises}}
\addcontentsline{toc}{subsection}{Exercises}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  At around minute 5:15, the hosts describe the what they see looking at the (histogram of) age distribution. What kind of distribution are the hosts describing?
\item
  Why such (over)simplification? Hint: there is a discussion around 9 minutes in.
\item
  The methodology of the Planet Money analysis is described on \href{https://www.npr.org/2019/08/28/755191639/episode-936-the-modal-american}{their website}. From this, determine how many variables there are, how many categories there are for each one, and thus how many kinds of Americans (i.e., how many buckets) there are in total.
\end{enumerate}

\hypertarget{sixteen-personalities-or-five-factors}{%
\chapter{Sixteen Personalities or Five Factors?}\label{sixteen-personalities-or-five-factors}}

Before you read this chapter, you might want to go ahead and take one of the personality tests based on
the Meyers-Briggs Type Indicator (MBTI) categories and/or the five-factor model of personality (also called the Big Five). There is only one ``official'' MBTI, which is a commercial product. However, there are several free alternatives online which use the same typology classification. There are also several variations of the Big Five.

\begin{quote}
Test yourself:

\begin{itemize}
\tightlist
\item
  MBTI-style at \url{16personalities.com} or

  \begin{itemize}
  \tightlist
  \item
    \url{https://openpsychometrics.org/tests/OEJTS/}
  \end{itemize}
\item
  Big Five

  \begin{itemize}
  \tightlist
  \item
    \url{http://www.personal.psu.edu/~j5j/IPIP/}
  \item
    \url{https://bigfive-test.com/}
  \item
    \url{https://openpsychometrics.org/tests/IPIP-BFFM/}
  \item
    General information about these test items: \url{https://ipip.ori.org/}
  \end{itemize}
\end{itemize}
\end{quote}

I will only minimially describe the MBTI and the Five Factor Model (FFM, or Big Five) here, in terms of the topics we have been discussing. There are many resources for learning more about these personality tests. Some are referenced under further reading.

\hypertarget{mbti}{%
\subsection*{MBTI}\label{mbti}}
\addcontentsline{toc}{subsection}{MBTI}

The MBTI will categorize people, based on their responses, dichotomously along each of four dimensions, also called ``scales.'' These are:

\begin{itemize}
\tightlist
\item
  Extraversion-Introversion (E-I)
\item
  Sensation-Intuition (S-N)
\item
  Thinking-Feeling (T-F)
\item
  Judging-Perceiving (J-P)
\end{itemize}

Thus there are sixteen possible combinations, for example ``INTP''. Each person is assigned to one of these sixteen personalities. Many online tests will provide you with a report to help interpret your classification. That is, the four dimensions are understood to come together in some holistic picture of your ``type.''

\hypertarget{big-five}{%
\subsection*{Big Five}\label{big-five}}
\addcontentsline{toc}{subsection}{Big Five}

The term ``Big Five'' is a commonly used term for the five-factor model of personality. Based on responses to questionnaires, people are assigned a numerical score along five dimensions (also called scales or factors!)

\begin{itemize}
\tightlist
\item
  Neuroticism refers to the tendency to experience negative feelings.
\item
  Extraversion is marked by pronounced engagement with the external world.
\item
  Openness to Experience describes a dimension of cognitive style that distinguishes imaginative, creative people from down-to-earth, conventional people.
\item
  Agreeableness reflects individual differences in concern with cooperation and social harmony. Agreeable individuals value getting along with others.
\item
  Conscientiousness concerns the way in which we control, regulate, and direct our impulses.
\end{itemize}

\begin{quote}
Fun fact: both OCEAN and CANOE are mnemonic devices that can help you recall the names of the Big Five dimensions.
\end{quote}

Since the results of a Big Five test, such as the IPIP-NEO, are five numbers, you don't get assigned a personality ``type'' by these tests. Rather, you may be provided with an explanation of what it means to score high (or low) on, say, Extraversion. You may have noticed that extraversion (occasionally spelled ``extroversion'') appears on both the MBTI and the Big Five.

\hypertarget{twenty-questions-about-extraversion}{%
\subsection*{Twenty Questions (about Extraversion)}\label{twenty-questions-about-extraversion}}
\addcontentsline{toc}{subsection}{Twenty Questions (about Extraversion)}

Suppose, for whatever reason, we want to identify a person's extraversion. We may want either (a) to classify them as extraverted or not (i.e., introverted), or (b) to quantify a degree of extraversion, say on a scale of 0-100. Why not then just pose the question in the following way? In the first case:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Choose the one that describes you: Extraverted ~~~~\textbar{} ~~~~Introverted
\end{enumerate}

or, in the second case,

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Locate yourself on the scale: Extraversion 0 . . . . 50 . . . . 100
\end{enumerate}

Personality tests, such as those we've discussed above, do not ask questions like these. Rather, they include many different questions, sometimes twenty or even more, about things like going to parties, making friends, and drawing attention to oneself. Why ask twenty questions instead of just one?

Recall from the great toilet paper debate that it was not necessary to ask twenty questions to know whether you were an over-hanger or an under-hanger. However, when we discussed digitidiness, we suspected that two different questions may have both been getting at the same latent factor. The situation here, in the real-life domain of personality testing, is similar.

\begin{tipblock}

\textbf{Golda asks}: So the idea is to design questions that are smarter than the user? Do we not trust people's abilities to self-categorize? Maybe these tests would be more effective if they allowed people to draw their own conclusions about themselves?

\textbf{Sidney says}: Developers of these questionnaires acknowledge that it is not as easy for people to self-categorize (what do you really mean by introverted? is that a bad thing?) as it is to describe how they feel at parties. Furthermore, category labels may seem socially desirable to the respondent, leading them to make a certain choice for ``appearances.'' So, to some extent yes, psychologists may not always trust people to self-categorize. But I think they would still defend everyone's right to draw their own conclusions from the results of a personality test.

\end{tipblock}

Psychologists \sout{believe that extraversion is an underlying factor} invented the idea of extraversion to explain patterns of behavior, including patterns of responses to questions about how people feel in various situations. Such as enjoyment or lack thereof in being the center of attention. The use of indirect evidence such as questionnaire responses to make inferences about psychological traits is the main task of \textbf{psychological measurement} (also called psychometrics). The main challenge of psychological measurement, perhaps even the reason for its existence as a method and field of study, is that human beings are noisy.

Put another way, you cannot expect a deterministic relationships between a person's response or behavior (how a person feels or acts) in one situation and how they respond or behave in another. Even someone we may want to call an extraverted person is not \emph{always} extraverted. And an extraverted person might not always answer questions about their feelings in the same way. It is also hard to directly observe extraversion or to ask people to simply self-report it. Extraversion manifests itself differently at different times and in different contexts. Whether this noise is due to some mysterious internal process, like a coin flip in your brain, or due to many unnaccountable external factors, like whether you slept poorly that day, we can't really say. What we can say is that human noisiness manifests itself as \textbf{measurement error} when we try to measure things like extraversion using questions and other observations.

Variability in human behavior is one of the factors that may contribute to measurement error. If the same person's behavior varies randomly over time, then our observations at any given time are subject to this variation. But we could also imagine having measurement error sources that come from ambiguously worded questions, which would still lead to errors even if behavior were stable. Say someone is asked if they engage in risky health behaviors. Even if their behavior is very consistent (for argument's sake), different people make have different perceptions of what counts as risky. So in that case measurement error results from (variation in) the interpretation of the question rather than from variation in risk-behavior over time.

The word \textbf{error} makes it sounds like there is a right answer, and that (personality) tests get it wrong. This is, indeed, one view (called true score theory in psychometrics). However, you don't have to believe that there is a right answer. For example, you can believe that human beings have some amount of inherent unpredictability. But just because human behavior is not perfectly predictable, just because observations are not deterministically related, we can still say that---in spite of measurement error---some patterns do remain. The responses to different questions about extraversion, for example, are associated with one another. People who say that they feel comfortable at parties are also likely to say that they make friends easily.

\hypertarget{but-whats-the-point}{%
\subsection*{But what's the point?}\label{but-whats-the-point}}
\addcontentsline{toc}{subsection}{But what's the point?}

Trying to describe people in terms of kinds or numerical scales is complicated. Why do we even bother? It's tempting to say that we just want to understand ourselves better, and that is certainly a reasonable answer. Sometimes, though, we want to predict how someone will act in the future, perhaps in a situation that differs from one that they have faced in the past. In that case, we can't exactly use the past to predict the future, unless we do so by making inferences about underlying traits from past behaviors and then predicting how someone with those particular traits would act in a new context. This purpose drives some uses of tests based on the MBTI and the Big Five, for example by employers or career counselors. However, although the MBTI is often used for these purposes, one should exercise caution in doing so \citep{pittenger1993}. You should certainly not assume that all personality tests do an equally good job of providing information for the desired inferences.

According to the standards of the American Psychological Association \citep{american1999}, whenever psychological tests are used for some specific purpose (e.g., employment, admission to a school or hospital, or even in court) there must be a valid argument for the intended purpose of the test scores. This \textbf{validation argument} will usually involve many facets, including how consistent the results of the test are, whether it is a fair test for all groups of people, whether test scores really are associated with relevant outcomes in the domain of use, and so on. These arguments, and challenges to them, are all part of what we mean by validity.

\hypertarget{recap-1}{%
\section*{Recap}\label{recap-1}}
\addcontentsline{toc}{section}{Recap}

So far, in pursuit of our question ``How many kinds of people are there?'', we have not been overly concerned with specific proportions or probabilities. Our discussion has been rather \emph{metaphysical}. We have tried to understand how differences that we observe among people can be expressed in terms of kinds (categories) or quantities (numbers), which are themselves different \emph{kinds} of data.

We have looked at dichotomous questions of both frivolous (toilet paper orientation or peanut butter preference) and less frivolous (attainment of Bachelors degree, employment status) kinds. Similarly, we've considered numerical variables from poopiness and crappiness to neuroticism and conscientiousness. We've seen that categorical questions can include more categories than two (e.g., peanut butter preference, if we include ``hate all'' and ``don't care'', but also ethnicity or neighborhood-type). We saw that numerical variables can sometimes be discretized and thus converted into categories, such as when income is classified as high or low, when age is converted to ``Generation Z'', or when a score on a personality test assigns you the value ``Perceiving'' as opposed to ``Judging.''

We saw that categorical questions can naturally classify people into types, and that when we combine different kinds of questions, the number of types/kinds/buckets can increase. But we also observed that the number of observed types does not necessarily increase to its mathematical maximum because answers to different questions can be associated. In other words, the number of factors or dimensions can be smaller than the number of questions. (But it can also be equal to the number of questions, in simple cases).

We also saw that numerical variables can be used to classify people. This can happen when we use cut-scores as thresholds and assign people above a threshold a value like ``pass'' or ``abnormal'' or ``introverted.'' The distribution of a numerical variable can be observed using a histogram, and sometimes a histogram is observed to have two bumps, which suggests a mixture of two different groups. But we also saw by plotting points on a coordinate plane of poopiness and crappiness that numerical data can cluster into groups, even when we don't see tell-tale signs of those groups along any single variable. The notion of clustering is all about identifying groups of data points that might designate types. According to some researchers, types can actually be recovered in the five-factor (Big Five) model of personality, which until recently was strongly thought of as \emph{not} having types.

You've probably figured out by now that this course is less about answering the Big Questions than about understanding how they might in principle be answered. What does one have to understand to even formulate an answer? Different people may indeed come to different answers about how many kinds of people there are. Hopefully you feel better equipped to reason rigorously and to discuss more precisely the evidence used to support any particular claim.

\hypertarget{when-and-how-will-you-die}{%
\chapter{When and How Will You Die?}\label{when-and-how-will-you-die}}

\begin{quote}
It is difficult to make predictions, especially about the future.

--- Niels Bohr (probably)
\end{quote}

In our first Big Question, we began to look at individual differences between people or what statisticians call variation within a population. If there is no variation---like in the bizarro world where everyone orients their toilet paper in the ``under'' orientation---then there is nothing to talk about, at least not statistically speaking. There is, however, considerable variation in health outcomes and human lifespan. Lots to talk about there. In our next Big Question, we ask ``when and how will you die?'' and ``what, if anything, can you do about it?''

What kind of question is, ``when and how will you die?'' Well, according to some of my colleagues, it is a morbid question. Feelings aside, we might say that it sounds like a prediction question, since it is about the future. So to explore this big question, we will need to understand what it means in general to make a forecast about some future event. We'll also find it useful to distinguish between predictions that are or are not explanatory. Most efforts in health sciences attempt to explain relationships between behavioral and genetic factors and health outcomes. In particular, they try to understand causal effects. So in the next few chapters, we will also try to understand causal explanations more generally.

\hypertarget{not-quite-death-but-um-rain}{%
\section*{Not Quite Death, but, um\ldots{} Rain?}\label{not-quite-death-but-um-rain}}
\addcontentsline{toc}{section}{Not Quite Death, but, um\ldots{} Rain?}

Perhaps it is a good idea to warm up, before we face the grim reaper. What does it mean to say there's a 30\% chance of rain tomorrow in New York? Does it mean that it will definitely rain in 30\% of the city (say, Brooklyn), but not in the other 70\%? Or that it will rain for 30\% of the day (say, from 8am-3pm). Here are some possibilities to consider:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  It will definitely rain in some parts of the city but not in all of them
\item
  It will definitely rain for some part of the day in all of the city
\item
  It will definitely rain for some part of the day in some of the city
\item
  It may or may not rain anywhere in the city at any point in the day.
\end{enumerate}

Read here for an \href{http://wxbrad.com/why-a-50-chance-of-rain-usually-means-a-100-chance-of-confusion/}{explanation of what meteorologists \emph{probably} mean}

\hypertarget{stochastic-vs-deterministic-relationships}{%
\subsection*{Stochastic vs Deterministic relationships}\label{stochastic-vs-deterministic-relationships}}
\addcontentsline{toc}{subsection}{Stochastic vs Deterministic relationships}

\begin{quote}
Sometimes when I say definitely, I mean probably. Like if I say, I'm definitely going to do something about all of this clutter on my desk. But when I really mean business, I say deterministically. It definitely sounds more serious.
\end{quote}

Meteorologists---scientists who model the weather---cannot tell us deterministically about weather events. Recall that we previously considered deterministic associations between two two-kinds-people questions. We imagined a world where if you knew a person's answer to one question (e.g., are you right- or left-handed?) then you would know for sure their answer to another. Now some of our examples were hypothetical and unrealistic, because, well, people are not in actuality very deterministic. If we want realistic examples, we end up making use of tautologies like ``Are you single? Are you in a relationship?''

But some events in nature are, more or less, deterministically related. An example might be something like, if I let go of the umbrella I am holding, then it will fall to the ground. If A then B. No exceptions (and no strings). You can imagine that I asked two questions: (a) did I let go of the umbrella (at a certain time T)? and (b) immediately after time T, did the umbrella fall to the ground? If you know the answer to one question, then you know the answer to the other.

Weather events are \textbf{stochastic}. As we know all too well from experience, they have an element of chance or randomness, like tossing a coin or rolling a die. So, just as we can say that a coin has a 50\% chance of coming up heads---assuming it is a fair coin---we can make statements like there is a 30\% chance that it will rain tomorrow. Stochastic is another word for random, but I prefer it because the word ``random'' is often used casually to mean weird or unusual (as in, ``that's random!'') Although we can't speak with certainty about random, or stochastic, events, that doesn't mean we can't speak usefully about them. We just need to learn to speak probabilistically.

\hypertarget{ways-of-thinking-about-probabilistic-statements}{%
\section*{Ways of thinking about probabilistic statements}\label{ways-of-thinking-about-probabilistic-statements}}
\addcontentsline{toc}{section}{Ways of thinking about probabilistic statements}

\hypertarget{ensembles}{%
\subsection*{Ensembles}\label{ensembles}}
\addcontentsline{toc}{subsection}{Ensembles}

One way to think about the 30\% chance of rain is to imagine that our experience in the world is one possibility in a multiplicity of possible worlds. See, I told you this idea of multiple alternate universes was going to be important! Imagine that there are 10 possible worlds, indistinguishable from ours in terms of the laws of physics, and that tomorrow it will in fact rain in 3 of them. To the great being-who-knows-all-things, which 3 worlds will see rain may well be known. However, to us mortals who merely live in the world that we know, we don't know which one of these possible worlds is the one we live in. Nevertheless we are capable of imagining these different potential outcomes. As you just did.

It didn't have to be 10 worlds, of course. That was arbitrary. If we imagined thirty worlds, it could rain in 9 of them, as I've represented in Figure \ref{fig:thirty-worlds}. I did this by making thirty circles and coloring in 9 of them at random. Since I like to pull back the curtain every once in a while, I will even show you the R code I use to generate this simple figure.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# start with a 10 x 3 grid of points}
\NormalTok{norain <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{,}\DecValTok{3}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DataTypeTok{each=}\DecValTok{10}\NormalTok{))  }
\CommentTok{# choose (sample) nine at random, using the sample() function in R}
\NormalTok{rainworlds <-}\StringTok{ }\NormalTok{norain[}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(norain), }\DecValTok{9}\NormalTok{),]  }
\CommentTok{# plot the points}
\KeywordTok{plot}\NormalTok{(norain, }\DataTypeTok{xlab=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{""}\NormalTok{, }\DataTypeTok{ylim =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{), }\DataTypeTok{axes =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{asp =} \DecValTok{1}\NormalTok{) }
\CommentTok{# color in the nine}
\KeywordTok{points}\NormalTok{(rainworlds, }\DataTypeTok{pch=}\DecValTok{19}\NormalTok{, }\DataTypeTok{col=}\StringTok{"lightblue"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,]{carpedatum_files/figure-latex/thirty-worlds-1} 

}

\caption{Rain (filled, blue dots) in 9 out of 30 possible worlds. It does not rain (hollow circles) in the other worlds.}\label{fig:thirty-worlds}
\end{figure}

Using this ensemble of possible worlds provides us with a sense-making device for probabilistic statements. Ultimately, it either will or will not rain tomorrow. You can also think of this observation as sampling from the ensemble of possible worlds. As though we put them all of these worlds into a hat and drew one of them. The probability of an event is thus thought of as the frequency with which it occurs.

\hypertarget{degree-of-belief}{%
\subsection*{Degree of belief}\label{degree-of-belief}}
\addcontentsline{toc}{subsection}{Degree of belief}

There is another way to think about 30\% as a probability. Suppose a meteorologist said to you, I'm 30\% sure it is going to rain tomorrow. And you say back, ``Oh, you mean that, say there are really 1000 alternate universes out there, that in roughly 300 of them, it will rain tomorrow?'' And the meteorologist says, ``I have no idea what you're talking about. There is only one universe, and I'm not totally sure what will happen tomorrow, but I put the chances of rain at 30\% \emph{{[}walks away slowly towards the door{]}}.''

For your meteorologist friend, let's call them Mel, 30\% may represent a \emph{degree of belief}. Importantly, the degree of belief is subjective. Here it is attributed to a meteorologist, which might make you take it more seriously than if your Uncle Bob said the same thing (unless Uncle Bob is actually a meteorologist). Anyway, degree of belief is subjective. Which doesn't mean it is arbitrary or just a matter of opinion. When it comes to forecasts, some people or some forecasting models are going to be right more often than others.

This idea of ``being right more often'' helps us connect the degree of belief way of thinking about probability to the ensemble sampling idea of probability. Sure, tomorrow only one universe will be ours to observe. It will either rain or not. If it rains, will you say that Mel the meteorologist did a good job or a bad job? What if it doesn't rain? It's going to be hard to say based on a single observation!

But a few days or weeks from now, Mel (the meteorologist) will come along again and say there is a 30\% chance of rain tomorrow. And again. And again some time later. What we could do is collect all of the times that Mel gave 30\% as their chance of rain and compare the actual occurrence of rain the next day. Suppose that we have 88 such cases to examine, and that it rained in 30 of them. Thats 30/88 or 34\% of the time. While not exactly 30\%, that still seems pretty good for something as complex as the weather!

\begin{quote}
Now maybe, maaaaybe, we could wonder if Mel is in fact under-predicting the chance of rain. Then we could add a bit to their forecasts when deciding what to do about it. I would feel more confident about that if we had a larger sample size. This is because we know that the observed proportions in a stochastic process will only converge to ``true'' proportions when sample sizes get large. We will come back to this in the module about money.
\end{quote}

To recap, out of 88 times that Mel gave a 30\% chance of rain tomorrow, it rained the next day 30 times and didn't rain the next day 58 times. You can see why it's hard to judge the meteorologist based on a single observation, even though we are often personally annoyed when the event (rain/no-rain) does not coincide with the choice we made about whether to wear galoshes.

\hypertarget{decisions}{%
\subsection*{Decisions}\label{decisions}}
\addcontentsline{toc}{subsection}{Decisions}

Aside from subjectivity, which is a thorny topic among statisticians, there is really no \emph{practical} difference between the interpretation of 30\% probability as a frequency of occurrence in an ensemble of possible worlds or as a degree of belief about this world. It won't change what you do about it.

If you take this forecast of rain seriously, you have decisions to make. It could be whether or not to take an umbrella with you when you leave the house tomorrow, or whether to cancel your plans to have a barbecue outside. These decisions may not seem very high stakes. The worst case scenario is that you (and others at your barbecue) get wet. But other decisions you have to make on a daily basis can have more serious consequences for your health or even your life. You often have to make those decisions based on probabilistic and maybe subjective information.

\hypertarget{death}{%
\section*{Death}\label{death}}
\addcontentsline{toc}{section}{Death}

End of warm-up. It's time to talk about when you will die.

I highly recommend this data visualization called \href{https://flowingdata.com/2015/09/23/years-you-have-left-to-live-probably/}{Years You Have Left to Live, Probably}. Here is a screenshot, although it's not nearly as interesting when you can't interact with the simulation and watch the little balls drop.

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,]{images/YYHLTLScreenshot1} 

}

\caption{Screenshot of interactive data visualization}\label{fig:years-screen}
\end{figure}

This visualization does a number of things. The most salient feature is probably the dropping balls. Each one represents a possible future outcome. This is exactly like an ensemble of alternate universes. As you watch the balls drop, you think to yourself, ``ah, nice, I lived to be 92'' and then moments later, ``ooh, harsh! I died at 39!''

As the simulation runs, it also accumulates data in bins at the bottom, labeled ``0 to 9'', ``10 to 19'', and so on. (Recall the discussion of bins, frequency tables, and histograms in Chapter \ref{shades}.) Note that these bins represent ranges of years-you-have-left-to-live, not age-at-death. This may be confusing, because age-at-death is what is shown along the horizonatal, or x-axis, of the figure. Also, right below the x-axis, and corresponding to age-at-death is a set of gray bars that grow as the balls drop. In the screenshot, the simulation has been running for a little while, so that the following counts have been accumulated.

\begin{table}[!h]
\centering\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
bin & counts\\
\midrule
0 to 9 & 1\\
10 to 19 & 1\\
20 to 29 & 3\\
30 to 39 & 3\\
40 to 49 & 13\\
\addlinespace
50 or more & 72\\
\bottomrule
\end{tabular}
\end{table}
\end{table}

Notice that by the time this screenshot was taken, 93 balls had dropped. The visualization took the counts, converted them into proportions of total counts (e.g., 72/93 = 0.774; 3/93 = 0.33), and represented each of these proportions as a probability, expressed as a percent (e.g., 77\%; 3\%).

Another thing that you will notice if you play around a bit is that as the balls drop, the probabilities change. In the beginning, when the number of samples (balls dropped) is small, the numbers change rapidly and sometimes by a large amount. However, after a couple of hundred samples, the changes are much smaller.

By watching the balls drop on this simulation (which I, for one, find mesmerizing), you may actually be meditating on some profound ideas in statistics. Every time you restart the simulation, you begin the sampling process. Each sample is a \textbf{draw} from some distribution of possible life outcomes. Your future life bounces around in this distribution from sample to sample. And in the beginning, when you have only collected a small number of samples, the distribution itself seems unstable. For example, if you put in 24 as the current age and start the simulation in slow mode, the estimated probability of living 40-49 more years fluctuates a lot. However, as you accumulate samples, the shape of the distribution literally comes into view as a pattern among the gray bars just below the x-axis. As the sample size increases, the probabilities becomes more stable. Eventually, if you let it run long enough, you end up with the same values, regardless of how things started out. (This increasing stability at large samples is why I was hesitant to judge Mel the meteorologist until I had more data.)

Although we are now talking about probabilities about your remaining years left to live, the interpretation of probabilities is similar to that in our discussion of rain predictions. In the case of rain, there were only two possibilities, rain or no-rain. (A dichotomy!) In the death simulation, there are six bins, each of which represents a range of years. In the case of rain, we understood the meaning of a 30\% chance (i.e., probability) of rain by imagining a large number of possible worlds, where it rains in 30\% of them. Or one universe where there are a lot of opportunities to make forecasts and check the restults of them. Thus the probability was associated directly with a frequency of something occurring. This is known is as the \textbf{frequentist} interpretation of probability. In the case of death, we say you have a 77\% chance of living 50+ more years if, in a large number of possible worlds, you live 50+ more years in 77\% of them.

You probably realize that we don't get to see all of these alternate universes, even though we can imagine them. Therefore our probability estimates in many cases are based on things that we have observed happen to \emph{other} people. For example, among 100,000 people that we do observe from the moment of birth, suppose 78\% of them lived into or past their 70s. We convert that observed frequency into a probability for you. You could say that we treat the other people we observed as alternate-universe versions of you.

\hypertarget{conditional-death}{%
\chapter{Conditional Death}\label{conditional-death}}

\hypertarget{how-does-the-death-simulation-work}{%
\subsection*{How does the death (simulation) work?}\label{how-does-the-death-simulation-work}}
\addcontentsline{toc}{subsection}{How does the death (simulation) work?}

The Flowing Data animated visualization is based on data collected in ``life tables'', which can be found online from sources like the National Center for Health Statistics (NCHS) and the Social Security Administration (SSA). Different life tables are produced every year, as life expectancy continues to evolve along with changes in health science and nutrition. Figure \ref{fig:life-duration} plots data for age-at-death (for Americans) as of 2010. There is a bar for each age from 0 to 120, and the height of each bar represents a count of deaths at that age per 100,000 people.

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,]{carpedatum_files/figure-latex/life-duration-1} 

}

\caption{How long Americans were living in 2010}\label{fig:life-duration}
\end{figure}

If you're like me, the first thing you notice in Figure \ref{fig:life-duration} is that little spike at age 0, like a rattle sticking up at the end of a rattle snake's tail. It shows us that roughly 5 out of 1000 babies don't make it to their first birthday. After that, your odds get considerably better for a while.

Another feature that you may detect is that the distribution of age-at-death is not symmetric. It has a long tail to the left. Distributions like this are also called left-skewed.

So how does age-at-death relate exactly to the years you have left to live? Life tables are a bit of a strange thing. First of all, they are not tables of ``raw data'' for a sample of 100,000 people. Rather, they represent a summary of data from many more deaths. According to the SSA \href{https://www.ssa.gov/OACT/HistEst/PerLifeTables/LifeTableDefinitions.pdf}{source}, ``the life table represents a hypothetical cohort of 100,000 persons born at the same instant who experience the rate of mortality represented by qx, the probability that a person age x will die within one year, for each age x throughout their lives.''

Most of us don't think about our lives in terms of questions like, are we going to die this year? But that is technically how the life table works. The life table is a set of numbers---including deaths-at-age-x and expected-years-left-to-live-at-age-x---that are all derived from one initial set of numbers which represent \emph{the probability that a person age x will die within one year}. If you're curious what that initial set of numbers looks like, I've plotted them in Figure \ref{fig:die-this-year}.

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,]{carpedatum_files/figure-latex/die-this-year-1} 

}

\caption{Mortality rate per year of age}\label{fig:die-this-year}
\end{figure}

Looking at Figure \ref{fig:die-this-year}, you can say that the probability of dying within one year gets higher as you grow older, which comes as a surprise to no one. If you're under 65, say, that probability doesn't even feel that high. It's less than 0.01 or 1\%. The probability that you will die \emph{this year} only passes 50\% after age 100. That's reassuring, right?

Well, don't get too optimistic. Your chances of dying every year may be small, but every year is another draw from this morbid lottery. If your chances of dying were 1 out of 2000, then in 2000 universes, you died in one of them. In the other 1999, you live on to another year, but then you have to press your luck again. This happens every year, and the chances slowly get worse.

But what if you wanted to know your chances, at birth, of dying in your 60s, that is between 60-69. For now, we will try to answer this question using only the life table and assuming that we know nothing else about you. The rows of the life table corresponding to this age range are these

\begin{table}[!h]

\caption{\label{tab:survival}Life Table}
\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
Age & qx & lx & dx & L & Tx & ex\\
\midrule
60-61 & 0.008732 & 88745.98 & 774.97 & 88358.50 & 2051875 & 23.1\\
61-62 & 0.009335 & 87971.02 & 821.18 & 87560.42 & 1963516 & 22.3\\
62-63 & 0.009983 & 87149.84 & 870.00 & 86714.84 & 1875956 & 21.5\\
63-64 & 0.010715 & 86279.84 & 924.46 & 85817.61 & 1789241 & 20.7\\
64-65 & 0.011568 & 85355.38 & 987.39 & 84861.68 & 1703423 & 20.0\\
\addlinespace
65-66 & 0.012586 & 84367.98 & 1061.84 & 83837.06 & 1618562 & 19.2\\
66-67 & 0.013763 & 83306.15 & 1146.57 & 82732.86 & 1534724 & 18.4\\
67-68 & 0.015057 & 82159.58 & 1237.07 & 81541.05 & 1451992 & 17.7\\
68-69 & 0.016380 & 80922.51 & 1325.52 & 80259.75 & 1370451 & 16.9\\
69-70 & 0.017756 & 79596.98 & 1413.34 & 78890.31 & 1290191 & 16.2\\
\bottomrule
\end{tabular}
\end{table}

This is a lot of numbers. Recall that each qx is the mortality rate for age x, the probability of dying within one year of age x. So should you add up the qx-values for each age in the interval 60 to 69? Maybe pause here to think about this question for a moment before reading on.

Here is a partial answer. You can die at 62 and you can die at 64, but you can't die at both ages. In that sense, it was okay to add the probabilities of these events because they are \textbf{disjoint}, i.e., they can't both happen and you are interested in whether any one of them does happen. However, if you add up these probabilities, you will still over-estimate the probability for a different reason. Can you guess what you've left out?

Here is the rest of the answer. You've left out the fact that these probabilities assume that you have already made it to 60, and there's a chance (at birth) that you won't.

To answer the original question, you want to add up the following probabilities:

\begin{verbatim}
(Probability of making it to 60 and then dying at 60) + 
(Probability of making it to 61 and then dying at 61) + 
... +
(Probability of making it to 69 and then dying at 69) + 
\end{verbatim}

How do you figure out the probability of making it to 60 without dying? It sounds a little bit like a riddle whose answer is ``one year at a time.'' Indeed, to make it to 60 without dying, you need to not die every year for the first 59 years of your life.

Note that, while death can occur in only one year of your life, to survive into your sixties you need ALL of the following to be true: NOT dying at 0 AND NOT dying at 1 AND \ldots{} NOT dying at 59. The probability of each event (not dying in each year) is independent, and the probability that all of them happen is the product of the individual probabilities.

\begin{verbatim}
Probability of NOT dying at 0 * 
Probability of NOT dying at 1 having made it to 1 * 
... * 
Probability of NOT dying at 59 having made it to 59
\end{verbatim}

Since in any given year, you either die or don't die, these two probabilities must add up to 1, so having gotten to any age x, the probability of surviving it is (1-qx). Now we can take the product of (that is, multiply) all of the survival probabilities (1 - qx) for each x from 0 up to age 59. (I will include the code here. The data table I have loaded from the National Center for Health Statistics is called ``lifetableNCHS'').

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{prod}\NormalTok{(}\DecValTok{1}\OperatorTok{-}\NormalTok{lifetableNCHS[}\DecValTok{1}\OperatorTok{:}\DecValTok{60}\NormalTok{,}\StringTok{"qx"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.887458
\end{verbatim}

You may notice that this probability had already been calculated for you in the life table, but it had been presented slightly differently as column lx, which is the number of persons (in a cohort of 100,000) surviving to exact age x. If we multiply our rate by 100000, we get 88745.8, which (up to a rounding error) is the same as the number in Table \ref{tab:survival}.

Okay, so now we are ready to complete the probability calculation. Recall we wanted to add up ten things: Probability of making it to 60 and then dying at 60, etc. We know that the probability of making it to age x is the same as the value of column lx in the table divided by 100,000. And the probability of dying is qx. So we need to multiply these two numbers in each row and add them up.

The result is 0.1056. An American child born in 2010 has a 10.5\% chance of dying in their 60s (and a 20.7\% chance of dying in their 70s).

So, we've figured out how to do that. And we're almost ready to move on, but it is worth noticing something. The product of the value qx and lx in each row of the life table is the value dx, which is the number of deaths at age x (or between x and x+1). So when we multiplied and added before, we were really just adding up the number of deaths (dx) at ages 60-69 and dividing by 100,000.

Now hopefully that makes sense to you that this should give us the answer we were originally looking for, namely what are the chances, at birth, of dying in your 60s. We could have looked at our hypothetical cohort of 100,000 people all born at the same time and asked: how many of them will die in their 60s. Well, that would be the sum of the dx-values, namely 10562. It wouldn't be a probability, though, unless we divided it by the total number of people (100,000).

So we've shown that we can answer our particular question two different ways:

\begin{enumerate}
\def\labelenumi{\Alph{enumi})}
\tightlist
\item
  Computing the total probability of your making it to 60 and then dying at 60 \emph{or} making it to 61 and dying at 61 \emph{or} making it to 62 and dying at 62 etc. up to age 69.
\end{enumerate}

or

\begin{enumerate}
\def\labelenumi{\Alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Computing the overall proportion, out of 100,000 people, who die in their 60s.
\end{enumerate}

A = B in this case. An important property of mathematical sciences is that you can arrive at the same answer in different ways. Maybe that sounds like a waste of time, but I view it as one of the most reassuring things about math. If you try something two different ways, and you do \emph{not} get the same answer even though you should, then something is probably wrong with the way you are thinking about it.

\hypertarget{somefacts}{%
\chapter{Some facts about Probabilities}\label{somefacts}}

In this course, I have taken the strong position that ideas should be driven by questions. So I've tried to reason through the example above before setting up any foundations on the basic rules of probability. A standard introduction to these topics can be found in many books, for example OpenIntro Stats, Chapter 3.

Now is probably a good time to recap some of what we have established about probabilities. We will also introduce the most basic notation \texttt{P(A)} for the probability that event A happens. For example, event A can stand for ``you die at age 64'' or ``it rains in New York tomorrow.''

\begin{itemize}
\tightlist
\item
  When possibilities are disjoint, or mutually exclusive, the probability that either one of them happens is the sum of the individual probabilities
\end{itemize}

\begin{quote}
\begin{verbatim}
P(A or B) = P(A) + P(B)
\end{verbatim}
\end{quote}

An example of this was dying at age 62 or dying at age 64.

\begin{itemize}
\tightlist
\item
  A special case of this addition rule applies when one or the other MUST happen. For example, in logic, either something happens or it doesn't happen. Either A or NOT A. Since these possibilities are disjoint:
\end{itemize}

\begin{quote}
\begin{verbatim}
P(A) + P(not A) = 1
\end{verbatim}
\end{quote}

\begin{quote}
\begin{verbatim}
P(not A) = 1 - P(A)
\end{verbatim}
\end{quote}

An example of this was the probability that you do not die at age 0. We found it by subtracting out the probability that you will die from 1.

The last fact we used is

\begin{itemize}
\tightlist
\item
  The joint probability rule for \textbf{independent} events that BOTH occur is the product of the individual probabilities of each event occurring.
\end{itemize}

\begin{quote}
\begin{verbatim}
P(A and B) = P(A) * P(B)
\end{verbatim}
\end{quote}

We used that to figure out how you survive by not dying every year. Notice that I've snuck in the word independent (well, I snuck it in boldy, so it wasn't that sneaky). There is an intuitive reason why it is important to make a distinction about independent events.

In the last module, we said that two events (we were talking about responses to questions) are independent if knowing about one of them does not give you any information about what the other one might be. But remember bizarro world where the toilet paper orientation and peanut butter preference were deterministically related, and specifically everyone is either under-chunky or over-smooth? I've reproduced this result in Table \ref{tab:tpxpb-reprise}. If I told you that 53\% of the total population prefers smooth, then what proportion of the total population prefers smooth AND likes to over-hang? Also 53\%. What proportion prefers smooth AND under-hangs? 0!

\begin{table}[!h]

\caption{\label{tab:tpxpb-reprise}Bizarro world}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & chunky & smooth\\
\midrule
over & 0 & 23\\
under & 17 & 0\\
\bottomrule
\end{tabular}
\end{table}

In bizarro world, toilet paper orientation and peanut butter preference are NOT independent, because knowing one of them DOES give you information about the other.

\begin{verbatim}
P(tp = over AND pb = smooth) does NOT equal to P(tp = over) * P(pb = smooth)
\end{verbatim}

This will become even more clear in the next section.

\hypertarget{conditional-probabilities}{%
\section*{Conditional Probabilities}\label{conditional-probabilities}}
\addcontentsline{toc}{section}{Conditional Probabilities}

Recall that we would NOT have gotten the right answer to the probability of dying in your 60s if we added up the mortality rates qx for all ages x in {[}60-69{]}. (Exercise: verify this.) Rather, we had to multiply these numbers first by the probability of living to age x. Another way to say this is that the mortality rate qx was actually a \textbf{conditional probability.} It was the probability of dying at age x \emph{on condition that} you have survived to age x. To be absolutely clear, we are measuring x in whole numbers, like birthdays, but we don't mean dying on your xth birthday. Rather, we mean dying anytime between turning age x and turning x+1. We need a special notation to distinguish conditional probabilities. We write,

\begin{verbatim}
qx = P(You die at age x | You survived to age x)
\end{verbatim}

and we read this as ``qx is the probability that you die at age x given that you survived to age x'' or as ``qx is the probability that you die at age x conditional on your surviving to age x.'' These are equivalent, but they differ from

\begin{verbatim}
P(You die at age x)
\end{verbatim}

which is the \textbf{unconditional} probability that you die at age x. This is also different from

\begin{verbatim}
P(You die at age x AND You survived to age x)
\end{verbatim}

which is called the \textbf{joint probablity} of the two events. (Note that the joint probability of events A and B is also often written as P(A, B). The comma functions like the word ``and''.) We calculated exactly this joint probability above when we wanted to add up the probabilities that you die at some point in your 60s. The way we computed the joint probability for each year was by application of this general rule for conditional probabilities

\begin{verbatim}
P(A and B) = P(A|B) P(B)
\end{verbatim}

which we read as ``the probability of both A and B happening is equal to the probability of A conditional on B multiplied by the probability of B.'' Note that this rule \emph{always} holds. That's because what I've called the general rule is equivalently just the definition of conditional probability. For example, I could have written it this way:

\begin{verbatim}
P(A|B) = P(A and B) / P(B)
\end{verbatim}

This is just a rearrangement of the formula, but we have a tendency of seeing whatever is on the left side of an equation as being defined by what is on the right.

As far as death is concerned, the following are all true:

\begin{verbatim}
P(die at x AND survived to x) = P(die at x | survived to x) * P(survived to x)
P(die at x AND survived to x) = qx * P(survived to x)

qx = P(die at x AND survived to x) / P(survived to x)
\end{verbatim}

where in the second line I substituted the mortality rate qx for the conditional probability that defines it. In the last line, you can see how the mortality rate could be estimated from data if you actually observed a whole bunch of people. You would count how many of the die at age, say, 62, and divide that number by the number who survived to age 62. You can also probably see why the following is true:

\begin{verbatim}
P(survived to x | die at x) = 1
\end{verbatim}

That is, if you died at 62 then you must have survived to that age. That may seem too obvious for words, but it helps to show clearly that for conditional probabilities, it is not generally true that P(A\textbar{}B) = P(B\textbar{}A).

Considering toilet paper in bizarro world, we can see explicitly why the rule for joint probabilities of independent events \texttt{P(A\ and\ B)\ =\ P(A)\ *\ P(B)} did not hold. The conditional probability relationship always holds, but independence is a special case. We can see what it is now:

\begin{verbatim}
P(A and B) = P(A|B) P(B) = {only in special cases} = P(A) * P(B)
\end{verbatim}

Thus, when A and B are independent, it must be true that

\begin{verbatim}
P(A|B) = P(A)   # for independent events
\end{verbatim}

which reads as ``the probability of A conditional on B is equal to the probability of A (regardless of B).'' Another way to say this is that no matter what we know about B, it doesn't tell us anything informative about A.

In the real world, this is true about toilet paper and peanut butter.

\begin{verbatim}
P(tp = over | pb = smooth) = P(tp = over) ## real world
\end{verbatim}

But that was NOT true in bizarro world, where knowing peanut butter preference told us EVERYTHING about toilet paper orientation. If A is the probability that a person is an over-hanger, and B is the probability that they prefer smooth peanut butter, then

\begin{verbatim}
P(tp = over | pb = smooth) = 1  ## bizarro world
P(tp = over | pb = chunky) = 0
\end{verbatim}

However, regardless of whether we stay in the real world or in bizarro world, the following will always be true,

\begin{verbatim}
P(tp = over AND pb = smooth) =  P(tp = over | pb = smooth) * P(pb = smooth),
\end{verbatim}

because this is the definition of conditional probability. Now you might notice that if the above is true, then the following should \emph{also} be true:

\begin{verbatim}
P(pb = smooth AND tp = over) =  P(pb = smooth | tp = over) * P(tp = over).
\end{verbatim}

And it is true. For the same reason that this follows from the definition of conditional probability.

The order of the two joint events on the left-hand-side does not matter. There is no difference between P(A and B) and P(B and A). If both A and B happen, they both happen. There is no implied chronology when we write ``A and B'' that A came before B. So, that said, it is also true that

\begin{verbatim}
P(tp = over | pb = smooth) * P(pb = smooth) =  P(pb = smooth | tp = over) * P(tp = over).
\end{verbatim}

By combining the two equations from above. Or, in general

\begin{verbatim}
P(A | B) * P(B) =  P(B | A) * P(A)
\end{verbatim}

Notice that is \emph{not} generally true about conditional probabilities that \texttt{P(A\textbar{}B)\ =\ P(B\textbar{}A)}. Even though it is always true about joint probabilities that \texttt{P(A,B)\ =\ P(B,A)} (I used commas instead of ``and'' here.) Here's an example. All NYU students are New Yorkers (at least, honorary New Yorkers while in town.) But not all New Yorkers are NYU students. The probability of being a New Yorker conditional on (or given that) you are currently an NYU student is 1. However the probability of being an NYU student conditional on being a New Yorker is clearly not 1.

\hypertarget{conditional-death-again}{%
\section*{Conditional death, again}\label{conditional-death-again}}
\addcontentsline{toc}{section}{Conditional death, again}

Earlier I said we would use the life table to answer questions about when you will die assuming nothing else about you. Now, you might be aware that life expectancy is not the same for males and females. Indeed, there are separate life tables for each sex. I've plotted the death column dx from both tables in Figure \ref{fig:life-durationMF}. Females are shown in light blue bars, and males using orange. Unfortunately for the males, their mortality rate is higher not only in their later years, but even in their late teens and twenties. (We'll come back to that when we consider how you will die.)

\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,]{carpedatum_files/figure-latex/life-durationMF-1} 

}

\caption{Deaths by age for male and female (2010)}\label{fig:life-durationMF}
\end{figure}

Suppose I was interested purely in the likelihood (at birth) of living into ones 90s or beyond, conditional on sex. I can find out the proportions from separate life tables for each sex as follows. (These single-sex life tables go from 0 to 119, and the first row is dying before your 1st birthday. I need to look at the 91st row to see death after age 90.):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#proportion of females dying from at ages 80-119}
\KeywordTok{sum}\NormalTok{(lifetableFemale[}\DecValTok{91}\OperatorTok{:}\DecValTok{120}\NormalTok{,}\StringTok{"dx"}\NormalTok{])}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(lifetableFemale[,}\StringTok{"dx"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2446278
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#proportion of males dying from at ages 80-119}
\KeywordTok{sum}\NormalTok{(lifetableMale[}\DecValTok{91}\OperatorTok{:}\DecValTok{120}\NormalTok{,}\StringTok{"dx"}\NormalTok{])}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(lifetableMale[,}\StringTok{"dx"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1347719
\end{verbatim}

I can make a two way table using these proportions. I will base my table on a sample of 1000 females and 1000 males. \textbf{This is not real sample data.} I am just using the life table proportions here to construct an idealized sample.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sex_age80 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{dieBefore90 =} \KeywordTok{c}\NormalTok{(}\DecValTok{755}\NormalTok{,}\DecValTok{865}\NormalTok{), }\DataTypeTok{livePast90 =} \KeywordTok{c}\NormalTok{(}\DecValTok{245}\NormalTok{,}\DecValTok{135}\NormalTok{), }
                        \DataTypeTok{row.names=}\KeywordTok{c}\NormalTok{(}\StringTok{"Females"}\NormalTok{,}\StringTok{"Males"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The table looks like this

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-19}Will you live into your 90s?}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & dieBefore90 & livePast90\\
\midrule
Females & 755 & 245\\
Males & 865 & 135\\
\bottomrule
\end{tabular}
\end{table}

The unconditional probability of living past 90 is (245+135)/2000 = 0.19 or 19\%. This would be your betting chances if a baby were born and you did not know its sex. We can write this as P(live past 90) = 0.19. But if you knew it was born female, then you need to compute P(live past 90 \textbar{} sex = female). How would you do this? Well, you can use the table. Of the 1000 females, 245 live past 90, so the answer is 24.5\%. For males, it is 13.5\%. Sorry, males.

\hypertarget{just-to-make-things-weird}{%
\subsection*{Just to make things weird}\label{just-to-make-things-weird}}
\addcontentsline{toc}{subsection}{Just to make things weird}

Now suppose, for arguments' sake, that the numbers in the life table apply to everyone born in the last 120 years as of today (they don't; life expectancy has changed over the years). If I told you that someone was over 90, but nothing else about them, what is the probability that the person was female at birth (we will make the simplifying assumption that the life table corresponds to sex at birth)? The two-way table I constructed had equal numbers of males and females. We will assume that the birth rates are indeed the same. So the unconditional probability of being born female knowing nothing about a person's status as living or dead is 50\%. However, among those alive in their 90s, 245/(245+135) = 0.64 of them are female. Almost 2 to 1.

Given a two-way table, with variables representing events A and B, it is possible to derive conditional probabilities in both directions. P(A\textbar{}B) might be the probability of living past 90 given sex at birth. P(B\textbar{}A) would then be probability of sex at birth given present age over 90.

In the next chapters, we will start to examine what kinds of things might kill you. This will give us another chance to look at association in two way tables (as distinguished from independence). We will also explore what it means to say that something \emph{causes} early death. In the example above, we saw that sex is associated with early death. Females live longer; males die younger. Would we say that sex causes early death?

\hypertarget{how-you-will-die-causes-or-conditions}{%
\chapter{How You Will Die: Causes or Conditions?}\label{how-you-will-die-causes-or-conditions}}

\hypertarget{causes-of-death-colloquially}{%
\section*{Causes of death, colloquially}\label{causes-of-death-colloquially}}
\addcontentsline{toc}{section}{Causes of death, colloquially}

We now pause our inquiry into when you will die and concentrate for some time on how it might happen. Let us pay another visit to Nathan Yau's series of (interactive) visualizations for Flowing Data. We already discussed \href{https://flowingdata.com/2015/09/23/years-you-have-left-to-live-probably/}{Years You Have Left to Live, Probably}. Another of them is called \href{https://flowingdata.com/2016/01/05/causes-of-death/}{Causes of Death} and the last \href{https://flowingdata.com/2016/01/19/how-you-will-die/}{How you will die} (links work if you download the PDF but do not work within Perusall).

These visualizations are really a treasure trove. So much going on. I've reproduced an image from Causes of Death below, but you really should interact with it on the web.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/CausesOfDeath} 

}

\caption{Screenshot of Causes of Death visualization}\label{fig:causes-viz}
\end{figure}

Take a close look at the image in Figure \ref{fig:causes-viz}. How many variables are represented here? (Pause and think this over.) There are fifteen color bands representing causes of death, but these are not each variables. Rather, they are categories of a single variable: cause-of-death, as operationalized by the Centers for Disease Control and Prevention.\footnote{Operationalization of a variable is a fancy and more specific word for defining it. It doesn't seem necessary to define ``cause of death'' in the common-sense use of the word define. But when we decide how many and which categories to include in our use of the variable, we operationalize this definition. For example, we could operationalize cause of death as ``natural causes'' or ``other''. This would be a dichotomous operationalization. And it leaves a lot to the imagination. The Center for Disease Control is particularly interested in diseases and not in external causes. So it makes sense that all external causes are banded together in this figure. However, don't be fooled. There are many subcategories, including very specific ones like ``Pedal cyclist injured in collision with heavy transport vehicle or bus.''} These include cancer, congenital defects, and external causes (e.g., pianos falling on your head), among others. Along the bottom of the image we see age, which is another variable. Indeed, the most salient aspect of this visualization is how much the relative importance of different causes of death changes over the course of one's life span.

If you die in your 20s, it is likely that you died from external causes. While if you died in your 80s, it is more likely because of a circulatory or respiratory disease. This intuitive sense-making is an example of conditional probabilities. It may not be a two-way table, but Figure \ref{fig:causes-viz} is in many ways analogous to a two-way table. The two variables are age-at-death and cause-of-death. Although we are not putting numbers to the probabilities we can read this image as indicating that

\begin{verbatim}
P(cause = external | age in 20s) > P(cause = circ or resp | age in 20s)
\end{verbatim}

(read, the probability that cause of death is external \emph{given} that age of death is in ones 20s\ldots{}).
While the opposite is true in your 80s:

\begin{verbatim}
P(cause = external | age in 80s) < P(cause = circ or resp | age in 80s)
\end{verbatim}

There is an important difference between this particular figure and a two-way table of counts. This figure shows \emph{relative} proportions of death for each age, not absolute numbers. As we know, and even visualized explicitly in Chapter \ref{conditional-death}, the count of deaths is much higher among adults in their 70s and 80s than among those in their 20s. Figure \ref{fig:causes-viz} does not communicate that. Instead, by ignoring the overall scale, the figure allows us to focus on the shifting importance of different causes.

A two dimensional figure is used here to represent relationships between two variables (age-at-death and cause-of-death), but there are more than two variables here. The additional variables require using the tabs at the top. Sex is here, as well as race. Note that although sex and race categories are all switched between by using tabs, they are not, of course, categories belonging to a single variable. This is something that I slightly dislike about this visualization. The way it is designed, it feels as though one cannot observe, or condition on, sex and race at the same time.

So there are four variables (did you get it right?). We can, in principle, imagine conditioning on any or all of them. For example, we could ask what is the probability of death by endocrine disease for (i.e., given, conditioned on) an African-American male aged 65-70. We might write something like this in probability notation as,

\begin{verbatim}
P(cause = endocrine | race = African-American, sex= male, age = 65-70) = ?
\end{verbatim}

Yau also draws your attention to another feature of this visualization: ``When you select races, you might notice that the smaller groups, American Indian and Asian, appear to be more jagged, whereas cause of death for the larger groups, black and white, appear to be smoother. This is likely due to population size more than anything else. It's a smaller sample, and there's higher variance.''

The sample size factors in here, because CDC data are being used to estimate a proportion. For example, what proportion of 65-year-old deaths are the result of circulatory disease. Suppose we knew the true proportion in the total population. If we took only a sample from this population, we will not always find exactly the same proportion. For small samples, the proportion in each sample will appear to vary a lot. While for large samples, the proportion will vary but in a smaller range. We will revisit this idea again when we discuss making bets.

Yau's final visualization in the trio, How You Will Die, combines elements of causes-of-death and years-you-have-left-to-live. The visualization is dynamic, and the passing of time is like a sped up version of your life. In Figures \ref{fig:how-die-21-1}-\ref{fig:how-die-21-3}, I have reproduced still snapshots taken at three different ``ages'', starting with the simulation settings: Asian female, age 21.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/HowYouWillDie21-61} 

}

\caption{Screenshot 1 of How You Will Die visualization}\label{fig:how-die-21-1}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/HowYouWillDie21-85} 

}

\caption{Screenshot 2 of How You Will Die visualization}\label{fig:how-die-21-2}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/HowYouWillDie21-97} 

}

\caption{Screenshot 3 of How You Will Die visualization}\label{fig:how-die-21-3}
\end{figure}

In this visualization, you can tell that death becomes more probable (and eventually certain) as you get older. The dots fill up slowly (except at the very beginning, when early childhood disease can play a role) and then more quickly. The colors represent causes. In the two-dimensional plane of dots, we get a sense of the randomness of the draw. But we also perceive the relative importance of causes by the areas taken up by purple and red. These are collected into a bar plot on the right side. At the base of each bar, the proportion is reported as a percentage of all deaths by this age.

\hypertarget{hold-my-beer}{%
\section*{Hold my beer}\label{hold-my-beer}}
\addcontentsline{toc}{section}{Hold my beer}

So far, we have conditioned on things that you can't change, like your age, race, or sex-at-birth. But what if I tell you that I'm about to do something really \sout{stupid} risky. Like suppose I told you I was going to eat a bunch of Pop Rocks and drink a lot of Coke, which we all know killed Little Mikey, the child star from the Life cereal commercials?\footnote{Okay, yeah, this is an urban legend: \url{https://www.snopes.com/fact-check/pop-rocks-soda/}}

If I am about to engage in highly risky behavior, you might not want to put the probability that I die this year at the 1\% or 2\% base rate from the life tables. You might think I have a 37\% chance of dying from an exploded stomach. Which, by the way, is an external cause. So you might think that the conditional probability of death \emph{by external causes} should go up, but not the probability of death by respiratory disease.

When there is something that you can change, or manipulate, such as engaging in a particular behavior (e.g., driving, rock climbing, or eating Pop Rocks and Coke), then we can imagine what \emph{might} have happened if you didn't do something that you did (or did something that in reality you didn't do). This is what is known as a \emph{counterfactual} claim, such as ``if I didn't check my email this morning before leaving the house, I would not have missed the train.'' Maybe. But how can we know?

Our knowledge about the world is based on a combination of observations. (I am distinguishing knowledge from belief, but epistemology is a big topic and beyond our scope at the moment!) Some of those observations are of the form where we do not deliberately manipulate conditions in the world, such as treatments, behaviors, or environmental factors. We just observe the joint occurrences of events in samples, such as health outcomes, behaviors, pollution levels, etc. Analysis of these kinds of data are called observational studies. We can also do experiments, sometimes, where we gather a sample of people and assign half of them to do something (e.g., take medicine, or sleep in a cabin) and half of them to do something else (e.g., take a placebo, or watch an informational video). These are experimental studies.

It is often said that experimental studies are the best way to develop knowledge about causation, that is, answers to questions of the form, does A cause B? Does eating meat cause heart disease? Does smoking cause lung cancer? And are experiments the only way to answer such questions?

What does it mean to say A causes B? The philosopher David Hume was one of the first to shed light on the question of how we conceive of causation. In the past twenty years, statistical thinking about causality has also changed a lot. In this course, we are going to take a pragmatic approach and focus on how we use the concept of causation in everyday life. It will be helpful to review our distinction between deterministic and stochastic processes and between associated and independent events.

\hypertarget{non-deterministic-causation}{%
\section*{Non-deterministic Causation}\label{non-deterministic-causation}}
\addcontentsline{toc}{section}{Non-deterministic Causation}

If I hit a porcelain tea cup hard with hammer and the tea cup breaks, we can safely say that hitting the teacup with a hammer caused the cup to break. We don't really feel the need to say that if you hit a teacup hard with a hammer, there is a 99.9997\% chance that it will break. Even if that's actually true. And we don't feel the need to define ``hard'' in this case either. We use an example like a teacup and hammer when we want to focus on the common-sense big picture and not the details. And the big picture here says that hitting a teacup with a hammer deterministically causes the teacup to break. Let us also assert that if we do not hit the teacup, and it just sits there, then it will not spontaneously break. In the case of the physics of hammers and teacups, we feel that we know this much is true.

What about buying a lottery ticket? Does buying a lottery ticket cause one to win the lottery? Well, you certainly are not guaranteed to win the lottery if you buy a ticket. (In fact, your chances will be very low. The subject of making money is the next Big Question). But you can't possibly win if you don't buy a ticket. So, strictly speaking, buying a ticket does influence the probability of winning.

We've now discussed two examples. In the first case (hammer and teacup):

\begin{itemize}
\tightlist
\item
  If A (hammer hits teacup) then definitely B (teacup breaks)
\item
  If not A (hammer does not hit teacup) then definitely not B (teacup does not breaks)
\end{itemize}

In table form:

\begin{longtable}[]{@{}lcc@{}}
\toprule
& Teacup breaks & Teacup doesn't break\tabularnewline
\midrule
\endhead
Hammer hits teacup & Always* & Never\tabularnewline
Hammer does not hit teacup & Never & Always\tabularnewline
\bottomrule
\end{longtable}

*pretty much; we're not splitting hairs here.

In the second case (lottery ticket):

\begin{itemize}
\tightlist
\item
  If A (buy lottery ticket) then maybe B (win lottery) and maybe not B (do not win lottery)
\item
  If not A (do not buy lottery ticket) then definitely not B (do not win lottery)
\end{itemize}

\begin{longtable}[]{@{}lcc@{}}
\toprule
& Win lottery & Do not win lottery\tabularnewline
\midrule
\endhead
Buy lottery ticket & Rarely & Probably\tabularnewline
Do not buy ticket & Never & Always\tabularnewline
\bottomrule
\end{longtable}

What about this question: does smoking cause cancer? Does it fit either of these two cases? Unfortunately the question about smoking does not. It belongs to a yet another case.

In the third case (smoking):

\begin{itemize}
\tightlist
\item
  If A (smoke) then maybe B (cancer) and maybe not B (no cancer)
\item
  If not A (do not smoke) then maybe B (cancer) and maybe not B (no cancer)
\end{itemize}

\begin{longtable}[]{@{}lcc@{}}
\toprule
& Get cancer & Do not get cancer\tabularnewline
\midrule
\endhead
Smoke & Maybe & Maybe\tabularnewline
Do not smoke & Maybe & Maybe\tabularnewline
\bottomrule
\end{longtable}

Now I'm not saying that the chances of cancer are the same whether you smoke or not. That remains an open question so far as our present argument goes. But even thus far, we can see that the smoking causality question, posed this way, invites some more questions.

How big a difference does there have to be between the cancer rates for smokers and non-smokers for us to be convinced that there is an association between smoking and cancer? And if there is an association between smoking and cancer, what would drive us to call this a causal relationship, to say that smoking causes cancer? Could there be a third variable? Could causality go the other way?

At minimum, we may say that there is no causation without association. If two events are independent (that is, they co-occur with frequencies that are consistent with their being independent events), then it does not make sense to say that one causes the other.

\hypertarget{testing-for-an-association-between-two-variables}{%
\section*{Testing for an association between two variables}\label{testing-for-an-association-between-two-variables}}
\addcontentsline{toc}{section}{Testing for an association between two variables}

Let's focus on the first question: How big a difference does there have to be between the cancer rates for smokers and non-smokers for us to be convinced that there is an association between smoking and cancer? Our approach to answering this question will be very similar to the one we used in Chapter \ref{test-indep} where we were considering associations between two-types-of-people questions. The method for establishing association is quite general.

Caveat: we are going to vastly oversimplify the problem here and use made up numbers. Smoking is not a dichotomous variable. People can smoke more or less heavily and for different durations. They may have already quit or be active smokers. For this worked example, we will imagine that we are sampling people between the ages of 50 and 75. We will operationalize smokers as heavy smokers: people who smoked the equivalent of at least 10 cigarettes a day for at least 20 years.

Suppose that we are able to obtain a randomly selected sample of 1000 people in this age group. For each one, the following information is available: a) whether the person is/was a heavy smoker and b) whether the person has ever been diagnosed with cancer. The beginning of our dataset might look something like this:

\begin{longtable}[]{@{}lcc@{}}
\toprule
& Cancer? & Smoke?\tabularnewline
\midrule
\endhead
Person 1 & No & No\tabularnewline
Person 2 & No & Yes\tabularnewline
Person 3 & Yes & No\tabularnewline
Person 4 & No & No\tabularnewline
Person 5 & No & Yes\tabularnewline
Person 6 & Yes & Yes\tabularnewline
\bottomrule
\end{longtable}

As a first step, you tabulate the data and get the following contingency table:

\begin{longtable}[]{@{}lcc@{}}
\toprule
& Cancer: Yes & Cancer: No\tabularnewline
\midrule
\endhead
Smoke: Yes & 46 & 204\tabularnewline
Smoke: No & 93 & 657\tabularnewline
\bottomrule
\end{longtable}

Then, you use the table to estimate the following:

\[P(\text{Cancer}|\text{Smoke})=\frac{46}{46+204}=0.184\]

\[P(\text{Cancer}|\text{Not Smoke})=\frac{93}{93+657}=0.124\]

You might say that these numbers suggest an association (i.e., dependence) between smoking and cancer: Within this sample, a higher proportion of smokers were diagnosed with cancer than non-smokers. But is this enough of a difference to convince you that, if you went out and found 1000 new (random) people, you would still observe a difference of this magnitude?

One way to \emph{start} trying to answer this question is to consider the following thought experiment: imagine that, among all people in the world, there is NOT a higher incidence of cancer among smokers (as compared to non-smokers). If that were the case, you would expect to see

\[P(\text{Cancer}|\text{Smoke})=P(\text{Cancer}|\text{Not Smoke}).\]

Or, written slightly differently:

\[P(\text{Cancer}|\text{Smoke})-P(\text{Cancer}|\text{Not Smoke})=0.\]

In comparison, you observed the following in your sample:

\[P(\text{Cancer}|\text{Smoke})-P(\text{Cancer}|\text{Not Smoke})=0.184-0.124=0.06.\]

So, you could pose the following question: what is the probability that, among the whole population, smokers do not have higher risk of cancer; but, among the random sample of 1000 people that you observed, there is a 6\% (or greater) increased incidence of cancer among smokers as compared to non-smokers? This type of question is the basis for \textbf{hypothesis testing}. Often, in hypothesis testing, we form a \textbf{null hypothesis} (in this case, the null hypothesis might be that smokers and non-smokers have equal cancer incidence among the full population) and \textbf{alternative hypothesis} (in this case, the alternative hypothesis might be that smokers have at least 6\% higher risk of cancer than non-smokers).

Based on the sample you observed, you could estimate that approximately \(\frac{46+204}{1000}*100=25\) percent of the population smokes and approximately \(\frac{46+93}{1000}*100=13.9\) percent of the population has been diagnosed with cancer. If there is no real difference in cancer incidence among smokers and non-smokers, then these two variables are independent: as though 25\% of your sample randomly decided to smoke, and 13.9\% were randomly diagnosed with cancer.

It is easy to simulate datasets under this assumption. In two completely separate (independent) steps, we will randomly assign to each of our 1000 people, a 25\% chance of being a heavy smoker and a 13.9\% chance of getting cancer. In Chapter \ref{test-indep}, we imagined starting with a fixed number of responses (e.g., smoker/non-smoker), shuffling those responses, and then redistributing the values among our virtual subjects. The code below actually does something different. It will draw values with replacement for each of the two variables. This means that in each replication, we will not actually have exactly 250 smokers and 139 cancers. The numbers will fluctuate somewhat in each simulated dataset.

After we generate our simulated datasets, we can calculate \(P(\text{Cancer}|\text{Smoke})-P(\text{Cancer}|\text{Not Smoke})\) and observe what range of values occurs. We will be particularly interested in the proportion of the time that this difference is greater than or equal to \(0.06\). That was our observed value. And in this simulation, we know that the two variables are independent. Thus, we are quantifying the chance of observing such a difference in outcomes due to random chance.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{5123}\NormalTok{)}

\CommentTok{#set some number of iterations/replications}
\NormalTok{nIter =}\StringTok{ }\DecValTok{100}

\CommentTok{#create vector to save differences in proportions}
\NormalTok{differences =}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\DataTypeTok{length =}\NormalTok{ nIter) }
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{nIter)\{ }\CommentTok{#repeat the following process nIter times}

  \CommentTok{#create some fake data and save it as "fakedata"}
\NormalTok{  fakedata =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Smoke =} \KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"Y"}\NormalTok{, }\StringTok{"N"}\NormalTok{), }\DataTypeTok{size=}\DecValTok{1000}\NormalTok{, }
                                       \DataTypeTok{prob=}\KeywordTok{c}\NormalTok{(.}\DecValTok{25}\NormalTok{, }\FloatTok{.75}\NormalTok{), }\DataTypeTok{replace =}\NormalTok{ T),}
                        \DataTypeTok{Cancer =} \KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"Y"}\NormalTok{, }\StringTok{"N"}\NormalTok{), }\DataTypeTok{size=}\DecValTok{1000}\NormalTok{, }
                                        \DataTypeTok{prob=}\KeywordTok{c}\NormalTok{(.}\DecValTok{139}\NormalTok{, }\FloatTok{.861}\NormalTok{), }\DataTypeTok{replace =}\NormalTok{ T))}

  \CommentTok{#use the fake data to calculate P(cancer|smoke)}
\NormalTok{  CgivenS =}\StringTok{ }\KeywordTok{table}\NormalTok{(fakedata)[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(}\KeywordTok{table}\NormalTok{(fakedata)[}\DecValTok{2}\NormalTok{,])}

  \CommentTok{#use the fake data to calculate P(cancer|not smoke)}
\NormalTok{  CgivenNS =}\StringTok{ }\KeywordTok{table}\NormalTok{(fakedata)[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(}\KeywordTok{table}\NormalTok{(fakedata)[}\DecValTok{1}\NormalTok{,])}

  \CommentTok{#save P(cancer|smoke) - P(cancer|not smoke) in the ith location of differences}
\NormalTok{  differences[i] <-}\StringTok{ }\NormalTok{CgivenS }\OperatorTok{-}\StringTok{ }\NormalTok{CgivenNS}
\NormalTok{\}}

\CommentTok{#calculate proportion of differences greater than or equal to .06}
\NormalTok{propGreater <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(differences }\OperatorTok{>=}\StringTok{ }\FloatTok{.06}\NormalTok{)}\OperatorTok{/}\NormalTok{nIter}

\CommentTok{#plot a histogram of the differences with a red vertical line at .06}
\KeywordTok{hist}\NormalTok{(differences, }\DataTypeTok{main=}\StringTok{"Histogram of P(cancer|smoke) - P(cancer|not smoke)"}\NormalTok{,}
     \DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.1}\NormalTok{), }\DataTypeTok{breaks=}\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.01}\NormalTok{))}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{.}\DecValTok{06}\NormalTok{, }\DataTypeTok{lwd=}\DecValTok{2}\NormalTok{, }\DataTypeTok{col=}\DecValTok{2}\NormalTok{)}
\KeywordTok{text}\NormalTok{(}\FloatTok{0.09}\NormalTok{, nIter}\OperatorTok{/}\DecValTok{10}\NormalTok{, }\KeywordTok{paste0}\NormalTok{(propGreater}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\StringTok{"% of cases"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.9\linewidth]{carpedatum_files/figure-latex/unnamed-chunk-20-1} \end{center}

As you might expect, the histogram of simulated differences (\(P(\text{Cancer}|\text{Smoke}) - P(\text{Cancer}|\text{Not Smoke})\)) is centered around zero. If there's no real difference, then you should expect to observe (close to) zero differences among any random sample of 1000 people. That said, you'll see from the histogram that it is still possible, by random chance, to observe a difference as large or greater than 6\%.

In fact, in our simulation, this happened 3 times (3\% of the time). Remember that our simulation was carried out assuming independence, in which case there should be no difference. So we might interpret our findings as follows: there is a relatively low probability of observing \(P(\text{Cancer}|\text{Smoke}) - P(\text{Cancer}|\text{Not Smoke})\ge .06\) in a sample of 1000 people if it were truly the case that this difference was zero.

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{sec:randomness}{%
\chapter{Randomness}\label{sec:randomness}}

Whereas in common language, we may use the word ``random'' to mean surprising or unfamiliar, the concept has a more precise meaning in data science and statistics. Interestingly, there is no (formal) idea of randomness without probability. Just like there is no idea of straightness without the idea of space. What are the odds, right? The concept of randomness needs probability to define it, as we shall see. Moreover, randomness is an idea, like straightness, that has a pure ideal form. But what we observe in practice may be less than perfectly random (or straight).

Consider straightness first. A straight line made using a ruler and a pencil is easily distinguished by eye from a wiggly hand-drawn line or a swooping curvy one. However, if we examine the pencil markings of a ``straight line'' under a magnifying glass, we can observe tiny wiggles at the edge. These wiggles may be due to the texture of the paper or the imperfections in the graphite (the pencil's ``lead'') or both. So the straight line is not actually perfectly straight. Nevertheless, we are able to hold in our minds a mathematical idea of a straight line. For example, we can define a line using math, specifically coordinate algebra and the \((x,y)\) Euclidean plane. You likely can recognize an equation of this form:

\[ y = 2 x + 3.\]

This formula, which expresses a linear functional relationship, assumes a fair amount of prior knowledge, which we typically learn in school. For example, that \(x\) and \(y\) can take on continuous, real-number values, that the \(y\) axis is perpendicular to the x-axis, etc. The line expressed in the formula is perfectly straight. Even if we draw it using only an approximately straight line with a pencil or chalk.

YouTuber \href{https://www.youtube.com/watch?v=9rIy0xY99a0}{Vsauce's video on ``What is Random?''} explores whether things that we take for granted as being random (e.g., coin flips) really are. If you watch this video, you may come away convinced the outcome of tossing a coin or rolling a die is not truly random, and that quantum mechanics is the only truly random mechanism in nature. Alternately you may be satisfied that the fact of having limited information is sufficient to justify the treatment of a coin toss as a random event. On this argument, as long as you can't tell the difference between a coin and a truly random coin, you may, for all intents and purposes, treat is as perfectly random.

\hypertarget{defining-random-processes}{%
\section{Defining random processes}\label{defining-random-processes}}

As with straightness, we can at least define an ideal random process. For example, we can define an ideal coin toss using mathematical notation, although this notation is less commonly learned in school. Here's one way to do it:

\[ x \in \{H, T\} \qquad P(x=H) = p\]

In this definition, \(x\) is not a real number but rather a discrete \textbf{outcome} of a dichotomous \textbf{random process}, i.e., the coin toss. The first part of the formula specifies that \(x\) can take on two possible values, heads (H) or tails (T). This is abbreviated here in set notation. We can read it as ``\(x\) is an element of the set containing H and T.'' The second part defines the probability that \(x\) is observed to have a value of H. The probability is \(p\), which is a \textbf{parameter} that stands in for some number between 0 and 1. For a fair coin, we would put \(p=0.5\). Although it is not written out explicitly, the probability of a coin coming up tails must be \(1-p\) due to the axioms of probability and the fact that there are only two possible outcomes. The axioms of probability say that the the probability of all possible disjoint outcomes must sum up to one.

We could have used any discrete process besides a coin toss. For another example, we might think that whether we will pass the course we are taking or not is a random process. Suppose we give ourselves a 60\% chance of passing. Then we can write:

\[ x \in \{Pass, Fail\} \qquad P(x=Pass) = 0.6\]

That's all there is to it. We defined a random process, so it's random! But that doesn't mean all of this is intuitive. When we observe the outcome of a coin flip, we observe heads or tails, not both. We either pass the class or not. So what's the \emph{evidence} that the process was random? The only way to collect such evidence is to be able to observe the process occur many many times. Or at least imagine observing in many times. Randomness requires probability.

By the way, we don't have to stop at binary or dichotomous outcomes. For a fair six-sided die, we can write

\[ x \in \{1, 2, \dots, 6\} \qquad P(x=n) = 1/6 \quad \forall n \in \{1, 2, \dots, 6\}. \]

The ``forall'' symbol (an upside down A) is a shorthand we use to indicate that all outcomes are equally probable, with probability of 1/6. We can read this part as, ``the probability that \(x\) is observed to take the value \(n\) is equal to 1/6 for all \(n\) in the set containing the values 1, 2, 3, 4, 5, and 6.''

What will the color be of the next car (excluding taxicabs) that crosses your path on the road? An idealized random description of that observation might look like this. Notice that I do not have a compact shortcut ``forall'' because the probabilities are not the same.

\[ x \in \{\mbox{white, black, gray, silver, red, blue, other}\} \]
\[ P(x=\mbox{white}) = 0.24 \]

\[ P(x=\mbox{black}) = 0.23 \]
\[P(x=\mbox{gray}) = 0.16 \]

\[P(x=\mbox{silver}) = 0.15 \]
\[P(x=\mbox{red}) = 0.10 \]
\[P(x=\mbox{blue}) = 0.09 \]
\[P(x=\mbox{other}) = 0.03 \]

\hypertarget{predictions-about-random-events}{%
\section{Predictions about random events}\label{predictions-about-random-events}}

Perfectly random processes may not be very predictable on a case-by-case basis (unless the probability is close to 0 or 1), but good predictions can be made about the outcomes of many observations taken together.

Suppose I predict that you will not be struck by lightning while reading this sentence. Was I right? Probably, because even though getting struck by lightning may be random, the probability is very close to 0. But if I predict that the next car that crosses your path will be black, I may be wrong \emph{most of the time} even if my description above is an accurate one! If instead I predict that of the first 1000 people reading this sentence (and residing in the United States, on which I based my model) 23\% will see a black car cross their path first, then my prediction should be close. How close? Well, quantifying error is part of what statistics is all about\ldots{}

The examples above, using two-sided coins, pass/fail, six-sided dice, and car colors might suggest to you how a general random process can be defined whenever there are a finite number of possible outcomes. We first define the set of all possible outcomes. Then, for each one, we define the probability that it occurs. The probabilities must still add up to one. This is a formal way of declaring that in our ideal random process, \emph{something}, i.e., one of the possible outcomes, must occur.

\hypertarget{modification-for-continuous-observations}{%
\section{Modification for continuous observations}\label{modification-for-continuous-observations}}

When outcomes are continuous and not discrete, the above definitions need to be modified slightly. For example, suppose you were interested in \emph{exactly} how long it will take before a black car crosses your path. (And suppose you have a clock so precise that there is no practical limit on how fine a time difference you can observe. Any fraction of a second is possible.) By the very nature of continuous measures, you cannot enumerate them, so you can't go one by one and declare what the probability of each possible outcome is. Think about it: you can count seconds (one, two, three,\ldots{}) but you can't count infinitessimal fractions of seconds. Instead, you have to limit what you can say to things like, ``the probability that the outcome (time to crossing by black car) is in some range is\ldots{}'' The choice of range can be totally arbitrary, say, the probability that it will take between 37 and 49 seconds. Or, you can turn the continous outcome into a discrete set of ranges (e.g., less than a minute, between one and five minutes, five to ten minutes, or longer than ten minutes) and then proceed as before. You will see this kind of thing done a lot. The formal definition of random processes for continuous variables uses calculus.

\bibliography{book.bib,packages.bib}

\end{document}
