[["index.html", "Carpe Datum Data Science for Life’s Big Questions Preface Caveat Guiding principles in this book", " Carpe Datum Data Science for Life’s Big Questions Yoav Bergner Preface “[T]he most important questions of life are for the most part only problems of probability. It may even be said, strictly speaking, that almost all our knowledge is only probabilistic.” — Pierre-Simon Laplace Caveat This book is an incomplete draft of a work in progress being developed as lecture notes for an online course. Content is provisional, contingent, and possibly wrong, but always well intended. Guiding principles in this book Question-driven Because the presentation of topics in this book is question-driven rather than method-driven, this coursebook has some idiosyncracies. Some topics that might be considered rather basic may be omitted, while some topics that are typically considered as advanced will get (a simplified) treatment. No proofs As a mathematical subject, statistics is often taught with derivation and proof using definitions, simple assumptions, and the logic of algebra and calculus. Mathematical formulas are the standard language of statistics. This approach to learning is powerful if the math supports rather than gets in the way of understanding. However, for many learners, the math obscures rather than clarifies, and another way–using demonstrations and simulations–might enable understanding, as Johnson &amp; Johnson once said, without tears. Now, a demonstration is not a proof. That said, repeated experiments can be convincing even in the absence of proof. For example, I can prove to you that if you take any whole number (e.g., 1, 2, 3, 7, 21, 118, 8675309), multiply it by 9, and then sum the individual digits of that resulting product, that the sum itself will be a multiple of 9 Example: 7 * 9 = 63; 6 + 3 = 9. 21 * 9 = 189; 1 + 8 + 9 = 18. An elegant and simple proof can be constructed (hint: by induction), but if you try it out yourself enough times, you won’t need the proof to be convinced. Twice, by the way, might not be enough. You could, however, write a computer program to test this equality a thousand times using a thousand random whole numbers. This is sometimes called a computer simulation. Now problems like this one are often used to teach proof technique rather than to encode cute number-facts in memory. And indeed, for training statisticians, a rigorous mathematical presentation is important. So, for that matter, is computer simulation. For most users of this book, intuition and understanding are the priority, and the ability to derive formulas is not necessary. We will, in due course, bring out some computer simulations. Figure 0.1: Hopefully you are in the right place. Credit xkcd.com "],["how-many-kinds-of-people-are-there.html", "How Many Kinds of People Are There? Things are about to get meta right from the start", " How Many Kinds of People Are There? There are 10 kinds of people in this world. Those who understand binary code and those who don’t. — seen on a T-shirt Things are about to get meta right from the start I’m going to start off this first chapter in a book about data science with an unsubstantiated claim. My claim is this: People love to categorize themselves and others. They love to take quizzes online that tell you “what kind of person you are” in some way or another. They love to make statements that begin with, “there are two kinds of people in this world…” and so on. Ok? That’s my claim. It’s a bit of a mouthful. Now, I just made a claim in support of which data can absolutely be brought to bear. But I won’t use data to support it. What? Why not, for crying out loud?! This is a book about data science!!! The reason is this: this book encourages you to think critically and skeptically about all kinds of ideas, claims, and questions. It tries to show you how to talk about these ideas precisely and not succumb to fallacies and bad intuition. But while trying to develop these skills, it is important to know when we are in turbo critical thinking mode (that’s a technical term1) and when we’re not. Sometimes, we need to be able to say common-sense things and not have to support them. What exactly am I even saying in my claim, you might be thinking? What do you mean by, “people love to” do X, where X, like ______ [“blank”], is a stand-in for some of the specific things I mentioned. That everybody does X? Most people? That people who do X derive pleasure above some pleasure threshold, thus designating “love” as opposed to “like?” You see, I could have tried to make my claim more precise. And I could have found polls and published reports that estimate just how many people have, by choice, taken some kind of person-category-test-thing, or posted funny jokes about “two kinds of people.” But I’m just letting my claim stand as a common-sense claim. Just like if I said, people love going to the movies. I wouldn’t feel the need to cite a scientific study to support that claim.2 Now, if someone is making what to them appears to be a common-sense claim but to you appears false or at least non-obvious, you have a few options. You can challenge the assumption and ask for evidence. Or you can accept the assumption, for argument’s sake, to see where this is going. Hopefully, my claim feels common-sense enough to you too (i.e., we have that in common). If not, I’ll just ask you to follow along to see where this is all going… Just kidding; it’s not really a technical term.↩︎ https://digg.com/2019/movie-genre-popularity-1910-to-2018-data↩︎ "],["categories.html", "Chapter 1 Categories, counts, and kinds", " Chapter 1 Categories, counts, and kinds Two Kinds of People “There are two kinds of people… which one are you?” questions have become something of an internet meme, particulary with the categorizations represented graphically or pictorially. There is a whole blog devoted to them by João Rocha. The images in Figure 1.1 probably need no explanation, as they concern the great toilet paper orientation debate. Figure 1.1: The great debate. Source Wikimedia Commons User:Elya Toilet paper orientation is a distinguishing test question that separates people into one of two “kinds” (or “types” or “categories”; sometimes English has several words that are used interchangeably). A fancy word for this “splitting into two” is dichotomy (die-COT-uh-mee), from the Greek. A dichotomous question has two possible answers. Here, you choose one way to orient the roll or the other. Let’s call this roll choice “over” (shown on left) or “under” (shown on right). Perhaps you have debated which is better with a friend or family member. In any case, armed with this particular test question, we can go out and collect some data. Table 1.1: How people roll count over 23 under 17 I went ahead and asked 40 people in Washington Square Park in New York City which kind of person they were, and the results are shown in Table 1.1. This being a book about data science, you might think I’m going to start calculating proportions right away, for example by saying that 57.5% of New Yorkers are over-hangers. Nope. Although you should be able to figure out that proportion conversion, it is not the point I want to focus on right now. That point I want to focus on is that, based on our data, there are indeed two kinds of people here. If, for example, everyone in the world were an under-hanger (heaven forbid), then I couldn’t very well say that there were two kinds of people in this world. At least not with regard to toilet paper orientation. It would be like if I presented you with the data in Table 1.2. Looking at that, I can’t very well convince you that there are two kinds of people. Table 1.2: Kinds of people in Washington Square count human 40 not human 0 That all seems pretty obvious, in part because I made up a tautology in the second example there. Being a human being is automatically associated with everyone who can be a kind of person. But what if I had gotten exactly the same results for the toilet paper question? What if the data looked like Table 1.3. In this alternate universe, everyone I ask in Washington Square is an under-hanger. Yes, it’s one of those scary alternate universes, like the Twilight Zone. Anyway, does that mean that there is only one kind of person when it comes to toilet paper orientation? Well…not necessarily. After all, this was just a sample of people in Washington Square. It was not the whole population of Washington Square, even, let alone New York City, let alone the world. Table 1.3: How people roll (alternate universe) count under 40 over 0 Samples and Populations: A Statistical Caveat Samples and populations are sort of a big deal in statistics and data science, where these words have somewhat specialized meanings. Consider the following utterances: The population of New York City is 8.6 million The population of New York City is ethnically diverse How can both of these statements make sense? In common usage, population often refers to the number or count of people, in a town, area, or country. Among statisticians and data scientists, population refers to a set or collection under consideration. It doesn’t have to be a set of people. It could be a set of rats, non-governmental organizations, or domestic flights originating in Chicago. But let’s suppose the population does refer to a set of people. The number of those people is just one summary about the population, also known as the total count. The proportion of over-hangers is another summary of the population, as is the most-common birth-month. The two statements above would be more consistent if the first were rephrased to be: the population (set of people) of New York City is numbers 8.6 million. If we always had access to all of the members in a population (the set or collection under consideration), the field of statistics wouldn’t exist. We would just know a bunch of facts about, say, everyone in the whole world. And that would be that. While it is true that data are becoming more and ubiquitous, don’t start betting on the demise of statistics. Even if we did have complete data for everyone in the world today, our population of interest might extend to the world as it will be next month, next year, or ten years from now. That is, we might want to make predictions about the future. In which case, we would want to draw inferences and to generalize from the data we have on hand—our sample—to data we don’t have—the rest of the (i.e., the future) population. Making inferences from samples to populations will always be a compelling and challenging problem. Since Washington Square is the center of my universe, that’s where I sample. Even if we agreed that our population of interest were confined to Washington Square, we would still find it difficult to collect data on everyone there. There are a lot of them, many of them are on skateboards, and new people keep leaving and entering the park. It turns out, that’s okay. We don’t actually have to reach everybody to be able to do data science. However, we need to understand that when we sample 40 particular people in Washington Square, we might not get the same exact answers as if we had sampled 40 different people. The sampling process introduces an element of uncertainty into our process. Coming back to our toilet paper debate, if we did find zero over-hangers in one sample, it doesn’t guarantee that the number of over-hangers will also be zero in the next. The number may vary from sample to sample. Uncertainty does not, however, mean that the information derived from one sample is useless. In fact, soon we’ll see that we can actually learn a lot from a sample simply by recognizing that sample values will vary. We can simulate samples on a computer to see how much they will vary. And then, using our simulations, we will be able to give probabilistic answers to questions like, “what are the chances that there really are no over-hangers in Washington Square?” A Psychological Caveat The section above explored a statistical caveat about drawing conclusions from samples. But another caveat that applies here is more psychological. I have assumed, for this argument, that a person’s answer to the toilet paper question is a fairly stable thing and not just a transient state-of-mind. That is, if I asked you tomorrow or next week, your answer would be the same as if I asked you today. For the most part. I’m not saying you can’t ever decide to change your mind. But it wouldn’t make sense to describe a person as an over-hanger if there were no stability at all to their answer. A transient state-of-mind, by contrast, could be asking someone if they are hungry. Everyone is hungry some of the time, but not all of the time. And it wouldn’t make any sense, based on that line of questioning, to imply that there are two kinds of people. Summarizing data When I presented my survey results to you in Table 1.1, notice that I did not present you with the raw data, but rather with a summary of the data. The particular summary I used was called “counts,” that is, a total count of how many people responded “over” or “under.” The raw data, in contrast, would have contained each individual response I collected, labled either with a name of the individual, or perhaps with some other unique identifier (such as a random number), or—if I don’t need to keep track of particular individuals—with just a row number. Something like this, if we examine at the first six responses rather than all 40 of them. Raw data: row randomID response 1 9246 under 2 1478 over 3 8831 under 4 8194 over 5 4178 under 6 4243 under Counts is an example of a summary statistic, which is a fancy term for a number that is derived from the raw data. The count summary is as simple as it gets. It is literally the number of times that each response appears. We might note as well that, count(under) + count(over) = total number of responses. This mathematical statement is true because there are only two possible responses. If there were more than two responses, then I would need to add the counts for each possible response. Note that the proportion of “over” responses is also a summary statistic (which is just the counts of “over” divided by the total number of responses). Another summary statistic could be the ratio of “over” responses (counts) to “under” responses. For example, one way people use summary statistics in reporting data is through statements like, “twice as many people prefer chunky peanut butter to smooth.” No mean feat Whenever someone reports a mean (another word for average) value of some set of data, that is also a summary statistic. Does it make sense to construct an average from responses that are either “over” or “under?” No, it doesn’t. That’s because {over, under} is a categorical response, and you can’t average over categories. Unless you’re trying to make fun of statistics with a puerile joke. Different versions of this joke appear: “the average American has one tit and one testicle.” At the risk of explaining the joke too much, here goes: Tits and testicles can certainly be treated as numerical data, and hence can be averaged. This joke hinges on the fact that the existence of testicles (or tits) is associated with a person’s sex, which is categorical and not numerical. Assuming that half of all Americans are female (roughly true), we can’t say that the average American is half male and half female. The real “punch” of this joke is to suggest that summary statistics about averages are just a bunch of nonsense. What do you think? This is about as much as we need to say about summary statistics for the time being. But they’ll be back. Checkpoint While focusing on the great toilet paper debate, we’ve managed to establish some important fundamental ideas. Dichotomous questions split people into two kinds, but only as long as it is actually possible for both answers to occur. In an alternate universe, people might give different answers than they do in this one. (Seriously, this is an important idea). Even when we casually refer to people, we may have a particular set of people, a population, in mind. Data about this population are likely to come from a sample, rather than from the whole population, and this fact introduces some uncertainty into claims about the whole population. Data science to the rescue! If we have types of people in mind, our questions ought to elicit fairly stable answers Clearly, we can ask people questions that prompt them to choose between more than two categories. But “two types of people” questions are more fun.3 I mean there are so many of them! So… does that mean that there really are two types of people? To answer this, we will need to get into another great debate. That was another unsubstantiated claim.↩︎ "],["dimensions.html", "Chapter 2 Dimensions Independence, Association, and Contingency Latent Factors and Measurement", " Chapter 2 Dimensions “I always said if I had one breakfast to eat before I die, it would be Wonder Bread toasted, with Skippy Super Chunky melted on it, slices of overripe banana and fresh crisp bacon.” — Michael Bloomberg Former NYC mayor Michael Bloomberg is a chunky peanut butter kind of person. Are you? As peanut butter comes in “smooth” and “chunky” varieties (also known as creamy and crunchy, respectively), this question is also a dichotomous one. However, if we add this test question to our question pool, in addition to the one about toilet paper orientation, we will soon find that having two two-kinds-of-people questions begins to imply more than two kinds of people. Wait, what? See, back when I went to talk to the people in Washington Square, I also asked them about the great peanut butter debate. As you can see from Table 2.1, smooth came out slightly ahead. Table 2.1: How people spread counts chunky 17 smooth 23 But this second question did not erase the first question about toilet paper. In fact the first few rows of our data from Washington Square are displayed below. Each row, representing one person, now has two columns, labeled “roll” (for toilet paper) and “spread” (for peanut butter): ## roll spread ## 1 under chunky ## 2 over chunky ## 3 under smooth ## 4 over chunky ## 5 under smooth ## 6 under chunky You may have noticed that among the first six people for whom I have shown data, none of them answered both over and smooth. But such response pairs exist. In fact, if we count each combination as it occurs–that is, under-chunky, over-chunky, under-smooth, and over-smooth–we get the results shown in Table 2.2. There are four combinations, because we have two questions with two possibilities (dichotomies) for each. Before you read on, it’s a good time to ask yourself if you can answer the following questions (answers in the footnote): (a) if there were two questions with three categories each, how many combinations could be observed? (b) if there were three dichotmous questions, how many combinations could be observed?4 Table 2.2: Two questions chunky smooth over 10 13 under 7 10 Table 2.2 is an example of a kind of table that is so common in data science, it has its own name. Three of them, in fact. It is sometimes called a cross table (or crosstab), or a two-way table (makes sense), but most commonly it is known as a contingency table (wha? I’ll explain later) I’m sorry that there are three names for the same thing. Really I am. If you’re like me, you can’t resist paying some attention to the values in the Table 2.2. For example, you might notice that one of the cells of the table (over AND smooth) has the highest number of people in it. We can say that this is the modal category, referring to the mode, which is the most common value in a distribution of values. We have four possible values in this example. Ok, now things are about to get deep. This first module is “How Many Kinds of People are There?” And we’ve now explored how using two two-kinds questions leads to four types. You’ve probably figured out yourself that you have to multiply the number of categories in each of the questions, and that tells you how many “buckets” you can have overall. But still, there are different ways to arrive at a certain number of buckets. Table 2.3: PB preference counts chunky 13 don’t care 3 hate all 4 smooth 20 Consider Table 2.3 in contrast to 2.2. We’ve now given people four choices to express their peanut butter preference. In addition to chunky and smooth, they can also choose to say that they hate all peanut butter or don’t care. We now have four kinds of people. But since we make the determination of what kind of person you are using just one question, we say that there is one dimension (in this case, peanut butter preference) along which people can be divided into four groups. In Table 2.2, there were two dimensions, a dimension of peanut butter and a dimension of toilet paper. Notice that this word, dimension, is used in much the same way as when we refer to geometric space as being two-dimensional (e.g., a drawing on flat sheet) or three-dimensional (e.g., a solid object, or sometimes a drawing that creates the illusion of looking at a solid object.) The three dimensions of space are often labeled something like (x, y, z). Here, our two dimensions could be labeled (pb, tp). The order doesn’t matter. We are merely indicating that there are two different variables used in categorizing our data (people, in this case). To summarize, in Table 2.2, we have two dimensions and four kinds. In Table 2.3, we have one dimension and four kinds. So far so good: two questions, two dimensions, right? Well… maybe. We already saw that if a question does not actually divide people into kinds, because only one answer appears, then it doesn’t really count. It is not a dimension, because it is not really a variable. It does not vary; it is constant. In our contingency table representation, this might look like the left side of Table 2.4. In an alternate universe, no one prefers smooth to chunky. Another way to say it is that the peanut butter question is not informative because it has no variance. Everyone in our sample is the same. Table 2.4: Two questions (alternate universes) chunky smooth over 23 0 under 17 0 chunky smooth over 0 23 under 17 0 But now consider the alternate universe on the right of Table 2.4. In that case, everyone who is an over-hanger of toilet paper prefers smooth peanut butter, and everyone who is an under-hanger prefers chunky. If this is the case, there are only two kinds of people, at least in our sample. Those who over-hang and prefer smooth and those who under-hang and prefer chunky. But does it make sense to say there are two dimensions? We did ask two different questions! You might reason about it the following way: in our sample, if I ask anyone just one of the two questions–about either toilet paper or peanut butter–then I immediately know the answer they would give to the other one. Another way to say this is that the answer to one question completely determines the answer to the other, and thus the relationship between these questions (really, the answers) is deterministic. I don’t actually have to ask two questions, other than to establish in the first place that I didn’t have to. And since I only get information from one question, there is only one dimension. Independence, Association, and Contingency This section title sounds like a philosophy book by the late Richard Rorty. — inner voice We just spent a little bit of time in an alternate universe, a bizarro world in which knowing how someone prefers to orient their toilet paper tells you what style of peanut butter they like, and vice versa. Notice that this knowing-about relationship is symmetric, and that in fact, the two representations as shown in Table 2.5 are informationally equivalent. Table 2.5: Alternate universe (two equivalent ways) chunky smooth over 0 23 under 17 0 over under chunky 0 17 smooth 23 0 In our regular universe, however, this relationship was not observed. In Table 2.2, all four possible combinations occur. When knowledge about a person’s answer to one question provides information about their answer to another question, we say that the two answers are contingent upon one another. This is the reason we called the two-way table a contingency table in the first place, although it is still called that even when two answers are not contingent. Go figure. Contingent is another word for dependent. To make matters worse, we also often say that the two responses are associated. In our bizarro world scenario, one answer completely determines the other. This deterministic relationship is one extreme in the spectrum of association/dependence/contingency. It expresses a certainty in knowing the answer to one question if we know the answer to another. At the other extreme, if the two responses are not at all associated/dependent/contingent, then we say that they are independent. To say that two responses are independent is to assert that knowing one of them does not give you any information about what the other one might be. This would have been my intuition, at least, about toilet paper and peanut butter. Somewhere in the middle, we might say that one answer gives you some information, but not certainty about another answer. Whether two answers are independent or mildly associated with one another is an empirical question, which means we should try to answer it with data. In bizarro world, where they were deterministically related, we might reasonably want to know why. Could there be a gene that controls both toilet paper orientation and peanut butter preference at the same time? Latent Factors and Measurement Figure 2.1: Two more two-kinds questions Figure 2.1 (source) shows two more two-kinds of people graphics from João Rocha’s blog. I bet that you can identify yourself with one of the two images in each pair. I certainly can. But ask yourself, given our discussion above, do you think the choices a person would identify in each case above are independent or not independent (e.g., contingent, associated, dependent)? In contrast to the toilet paper and peanut butter questions, which at least appear to be about totally different things, these two dichotomies have something similar going on in each of them. The choice on the left is about organizing your desktop browser, either in tabs or as separate windows. The choice on the right is about organizing apps in your smartphone, either loose or in folders. We might say that both of them get at a tendency to organize your digital environment. Call it digitidness (short for digital tidiness). This tendency, we may imagine, might even carry over into non-digital environments, like your actual desk, bookshelf, or filing cabinet. What we’ve done here is to try to explain the association between responses to the two questions (assuming that there is, i.e. that they are not independent) by appeal to some underlying latent factor. We say a factor is latent (meaning hidden) because we don’t observe digitidiness itself directly, but we only observe tidy browsers or smartphone app folders. Perhaps you can think of another candidate factor besides digitidiness. In any case, we might propose that both of the two two-kinds questions in Figure 2.1 are in effect indirect measurements of the same factor. If so, this could explain why the two answers would be associated. Notice that a factor is also a dimension, in the sense we used before. We could have said “latent dimension,” but we tend to use the word factor when we are drawing attention to the specific nature of the dimension rather than just counting. We also sometimes use the word trait. At least in psychology, trait tends to be reserved for stable psychological factors. Thus “stress” can be a factor but not a trait, whereas “social anxiety” may be a trait, if it is persistent. In this case, digitidiness might be considered a trait (and thus also a factor and a dimension). Contrast this with toilet roll orientation, which we can observe directly just by looking in someone’s bathroom. (We assume that they are telling the truth when they answered our questions, but we could in principle verify it.) It was only in the bizarro world when toilet roll orientation and peanut butter preference were perfectly related that we started to wonder if there maybe was an underlying genetic factor. Genetic factors were once not directly observable either, but we assumed them for explanatory value. Today we can of course observe specific genetic variation, although there are still many gaps in our understanding of the relationship between genes and observed behaviors. Consider some data again, in two possible worlds, shown in Table 2.6. On the left, we have the deterministic scenario we saw before. As before, we identified this situation as having two kinds of people and really just one dimension. In contrast to before, where we had no real explanation for this coincidence, we attribute it now to some factor, like digital tidiness. Table 2.6: Possible data for digital tidiness app-folders apps-loose browser-tabs 21 0 browser-windows 0 19 app-folders apps-loose browser-tabs 16 6 browser-windows 5 13 But now consider the possible results in the table on the right. Since all four possible quadrants have non-zero counts, we see that knowing whether someone organizes their browser using tabs does not completely (i.e., deterministically) specify whether or not they put their apps into folders. On the other hand, one answer does seem to be associated with the other. Notice that the values are still much higher in the diagonal “buckets” that we think of as indicating the presence or absence of digitial tidiness. These are the tabs-folders bucket (tidy) or the windows-loose bucket (not tidy). We say that the tidiness factor appears to explain much of the observed range, or variance, in responses to the two questions. But it doesn’t explain all of it, since there are people (11 out of 40, in this case) who don’t fall into one of these buckets. This situation on the right is probably more realistic. After all, very few things in this world are absolute (unlike in bizarro world). So now the big question re-emerges: are there two kinds of people or four? One dimension, or two? It’s sort of…like…in between…? Golda says: Although digitidiness explains a lot of what we see in our data, it doesn’t explain it all. I believe that desktop tidiness and mobile tidiness are different, if related, tendencies. For example, when we use mobile phones, we’re typically on-the-go and have less time. If we knew more about the people in our sample, we might see that these discrepancies in the organization of apps and tabs actually relate to other aspects of their lives. So, I say there are two dimensions. Sidney says: Digitidiness is the only real factor here, but people may not always be consistent in these particular behaviors. Also some people are only sort-of-tidy, and apply this tidiness unevenly and randomly. These two-kinds of people choices don’t leave room for shades of gray, so that’s what we’re seeing in the mixed categories where people are tidy in one environment and not in another. But ultimately there is really just one dimension here. What do you think? (a) If the categories for each question are A, B, and C, we can get AA, AB, AC, BA, BB, … etc. We multiply the number of categories as many times as we have questions. So 3*3 = 9. (b) This time we have three questions, and for each one we have two options, so there are 2*2*2=8 possible combinations.↩︎ "],["test-indep.html", "Chapter 3 Tests of independence: A first look starring alternate universes Rise of the machines Hypothesis testing just happened Caveats Sampling, simulations, and randomness Recap", " Chapter 3 Tests of independence: A first look starring alternate universes So far, we have looked at some data in two-way tables and judged them by inspection. That is, we looked at them and said, these two responses appear to be deterministically related, or they appear to be associated but not deterministically, or they appear to be independent. But we acknowledged that our data come from a sample, and that on another day (on in another universe with rules the same as this one), we might have observed slightly different data. It is worth noting that a single dataset often can’t tell us for sure whether two variables are independent or associated (aka dependent/contingent), and whether or not an association is a deterministic one. (Two variables cannot be deterministically independent; that would be a self-contradiction.) In this chapter, we’ll take a first look at how a particular two-way table might have come about, and thus how it might have come out differently. Let’s suppose that we observe the data summarized here: Table 3.1: Another alternate universe chunky smooth over 17 6 under 5 12 It looks like certain pairs of outcomes are coming up more than we would expect “by chance” if the answers were not associated. Equivalently, it looks like there is an association between these variables, but let’s investigate further. Let’s take the cell with the largest number of people in it, or the modal value. In this case, the mode occurs for people who answer over and chunky. Which was 17 out of 40. Now consider what contributes to making this particular value what it is. For example, what would make it bigger? Well, if there were more over-hangers than under-hangers, that would tend to increase the number of people who could be both over-hangers and chunky-spreaders. Similarly, if there were more chunky-spreaders. Also, if there were just more people in our sample, then of course we could have more people in this cell of the table. And all of the above arguments could work in reverse, if we lowered any of those particular numbers. What if, for argument’s sake, we fixed the total number of people, the number over-hangers among them, and the number of chunky-spreaders among them. We’re going to keep these values constant while letting the individual table elements vary. The technical term for this, by the way, is fixing the marginal values. This is because the sums of the columns (22 and 18) and the sums of the rows (23 and 17), when written at the bottom and side of the table, are called margins (just like the margins of a page). We could still have fixed the marginal values and have different values in the table. For example, Table 3.2: Yet more alternate universes! chunky smooth over 19 4 under 3 14 chunky smooth over 13 10 under 9 8 Notice that the row and column sums are the same. But on the left we would say an association between the variables appears even more clear, whereas on the right, it appears less obvious. Can you see why? Recall that the actual data summarized in a two-way table originally came from some observation sample. There were 40 people sampled, and they were each asked two questions. Before we tabulated them, the data might have looked something like this: Peanutbutter Toiletpaper Smooth Under Chunky Over Chunky Under Chunky Over Smooth Under Now suppose we wrote every person’s response to the peanut butter question on an index card, shuffled them randomly, and then re-distributed them. We then did the same for the toilet paper question. We did this one question at a time, separately. Finally, we ask our sample to show us their cards, and we treat these paired observations as a new data set. We generate a new two-way table from the results. If we do all this, then we will have kept the total number of people the same, as well as the total number of “over” answers and “chunky” answers. Moreover, since we shuffled responses to each question separately, there is absolutely no way for someone’s randomly assigned answer to one question (the index card they got for peanut butter) to influence their randomly assigned answer to the other (the index card for toilet paper). The answers should, by definition, be independent. Rise of the machines Well, we can do this shuffling experiment very easily on a computer. And we can very easily repeat it 1000 times. So, the question we might then ask is, how many times out of 1000 such simulations do we get a value for over and chunky that is as large as (or bigger even) than 17? The code to do this, and to visualize the outcome, is below. (You do not need to understand this code yet, but some of it might make sense to you). simulated_overchunky = c() #initialize empty vector for(i in 1:1000){ # the sample() function does the shuffling shuffle_data$Peanutbutter = sample(shuffle_data$Peanutbutter) shuffle_data$Toiletpaper = sample(shuffle_data$Toiletpaper) # the table() function does the tabulating simulated_overchunky[i] = table(shuffle_data)[&quot;Chunky&quot;,&quot;Over&quot;] } # plot the results using a histogram hist(simulated_overchunky, main = &quot;&quot;, xlab = &quot;Count&quot;, breaks = seq(0,20,1)) abline(v=17, lwd=2, col=3, lty=2) Figure 3.1: Counts of ``over and chunky’’ after 1000 shuffling simulations Figure 3.1 is histogram, or a bar plot of count data. We will cover histograms in detail the next chapter, so don’t worry too much if some of this is still confusing. The main thing to notice is that green dotted line at 17, and this: in our 1000 shuffling experiments, we got values greater than or equal to 17 only 5 times. (You probably can’t see this in the histogram, but if you run the code, you can ask R to report this: length(which(simulated_overchunky &gt;= 17))) The rest of the time, the values were smaller, and typical values were around 13. This simulation we just carried out is a way of validating our intuition, from inspection of Table 3.1, that 17 was a high value if toilet paper orientation and peanut butter preference were indeed independent. What we showed is that, if indeed the two variables were independent, that such values would be observed only rarely. Since this value was observed on our first day out in the park (in this scenario), we begin to doubt independence. If on the other hand, we had observed 13 over-chunky people (out of 40, and with the same marginal values as before), then we would say that this observation seems quite consistent with independence. Because values near 13 occur very often in our shuffling simulation. Hypothesis testing just happened What we just did is in fact an example of a hypothesis test. In the statistical framework of hypothesis testing, we have a null hypothesis (usually, a skeptical position) and an alternative hypothesis. Here our null hypothesis was that the two variables represented in our response data are independent. Our alternative hypothesis was that people who are over-hangers are truly more likely to be chunky-spreaders. We didn’t state this hypothesis at the outset. But it was implied by the fact that we were investigating this “high” value in the contingency table. If you take a regular statistics class, however, you will see that, when it comes to inference, it is quite important to take care in constructing your null and alternative hypotheses. For now, we just want to get the main idea. The main idea was that we simulated what data might look like under the null hypothesis (independence) and checked if our observations were consistent with that or not. Consistent (with the null) results would mean that we cannot “reject” the null hypothesis, while highly inconsisent results give us justification to “reject the null” in favor of the alternative. In this example, we would probably reject the independence assumption (null) in favor of the alternative. The independence assumption was consistent with values around 13 plus or minus 3 (i.e., the range of 10-16). But 17 is unusually high. Caveats Rare events still do happen, rarely. What our simulation showed us is that high values do still happen by chance. Even if the variables were independent, if we sent 1000 people out in the world to collect data from samples of 40 people each, some of those people would observe large values that look associated. This is to be expected. This is why it is important, if we are seeking the truth, that we not game the system by checking over and over again until we get an answer that we like. This would be like rolling double-sixes on the fourteenth try and then saying “ha! see, I told you I was lucky.” What about the other cells in the table? You might be wondering how we can do this whole simulation just for the one cell in the table that corresponded to over-chunky. You’re right to wonder this (if you are). In time, we will see that we can do independence-tests (or other hypothesis tests) more democratically by examining all of the cells in the table and how they differ from what we would expect under the independence (null) hypothesis. For now, though, note the following. Each of our variables is dichotomous, and we fixed the row and column marginals (totals). So if over-chunky is high, and total count of chunky is held fixed, that means that under-chunky must be low. Our test would have gone the same way if we sought to explain that cell. So, too, must over-smooth be low (lower than it would be under independence assumptions). Sampling, simulations, and randomness In the last section, we used the sample() function in R to shuffle some values in our simulation. That is, starting with a data set of 40 responses (for example, 23 “over” responses and 17 “under” responses), we wanted to reassign these responses randomly among the people in our observation sample. What if we wanted to simulate the original set of answers in the first place? It turns out we can also use the computer without collecting any data at all. To reproduce the exact example above, it would suffice to write down a series of values like this (over, over, over, over, …, over, under, under, … , under), 23 overs and 17 unders, and then shuffle them. Boom. The first shuffle is our simulated data. But that procedure wouldn’t work if we wanted a sample of 3, would it? If we want to be able to simulate data with different sample sizes, we need another approach. Tossing (virtual) coins Let’s simplify things for a minute and assume that in actuality, people are equally likely to be over-hangers or under-hangers. Or that for whatever dichotomous question we want to ask, the likelihood of either responses is 50%. If you want to simulate an event that has a 50% chance (or probability of 0.5), you can toss a coin. If we want to randomly assign 10 people to one of two groups, say the Sharks and the Jets, we can do the following. First, decide that Heads we choose Sharks (this is, of course, arbitrary). Then, for each person, flip a coin. If heads, then assign them to Sharks. If tails, assign them to Jets. Another way to do this is to put two poker chips into a hat, say a red chip (for Sharks) and a blue chip (for Jets). Then blindly reach into the hat and pick out one chip, record the value, and then put it back and shake the hat before the next draw. Pulling a chip out of a hat or tossing a coin are equivalent manifestations of the ideal process of sampling a random event with a 50% chance. This is actually a profound idea. The coin toss or hat draw are concrete, mechanical processes. Each has two possible outcomes (heads/tails or red/blue), and, for all intents and purposes, the outcomes in both cases are equally likely. We human beings have learned to abstract from both of these mechanical processes to an idea of a stochastic process which is the same thing as a random process and the exact opposite of a deterministic process. This abstract process requires that we have a state space, which is just a set of possible outcomes (mathematicians refer to some sets as spaces). As long as the set has two possibilities, it doesn’t really matter whether we label them heads/tails, over/under, red/blue, Sharks/Jets, or 0/1, because the label can always be assigned to mean whatever we want it to mean (just as we decided heads corresponds to Sharks). In addition to the state space, we abstract the idea of the sampling process. This is what “picks” out one of the possible outcomes. Like the coin-flip or the hat-draw. And it is this sampling process where the probability of the outcome appears. We can in fact simulate this sampling process on the computer in R. In fact we can do it several different ways. Here, first, is a more or less direct translation of what our mechanical process would do. The key enabling function here is called sample(), which works just like pulling chips out of a hat. We will tell sample() what’s in the hat by defining a set (a vector, in R) called coinstates. Then we will sample once from the hat for each person. set.seed(8675309) # I&#39;ll explain this later coinstates &lt;- c(&quot;H&quot;,&quot;T&quot;) numPeople &lt;- 10 # start a blank list to hold my team assignments teamassignments &lt;- c() # now, go one by one; this is called a programming &quot;loop&quot; for (i in 1:numPeople) { ## toss coin and observe coinresult &lt;- sample(coinstates, 1) # the 1 means sample once # associate outcome with team assignemnt if (coinresult == &quot;H&quot;) { team &lt;- &quot;Sharks&quot; } else if (coinresult == &quot;T&quot;) { team &lt;- &quot;Jets&quot; } # record team assignment teamassignments &lt;- c(teamassignments, team) } teamassignments ## [1] &quot;Jets&quot; &quot;Sharks&quot; &quot;Jets&quot; &quot;Sharks&quot; &quot;Jets&quot; &quot;Sharks&quot; &quot;Sharks&quot; &quot;Sharks&quot; ## [9] &quot;Jets&quot; &quot;Sharks&quot; Voila. The sample function did the apparent work of the coin flip or, equivalently, picking a “Heads” or “Tails” out of a hat containing both. Just as good, right? Notice that (a) we only got four Jets, not five, and (b) sometimes we got three Sharks in a row. That’s randomness for you. Technically, it is pseudo-randomness. Notice the first line of the code where I set the “seed” for the random number generator. What this does is make sure that every time I run this code I get the same results. If that seems not random to you, I don’t blame you. But here are some important things to keep in mind. First, even though using this random seed makes my experiment repeatable, I still have no idea what the results are going to be until I run it the first time. Second, if I change the seed, even by a tiny amount, then I will once again have no idea. Try it yourself. Note that if you do not declare a random seed, then the computer will effectively choose one for you, perhaps by using the exact computer time in milliseconds. This way the results will still be different every time you run it. If you want to read (a little bit) more about randomness, see the Appendix. If you’re thinking we could have removed the coin from this process, you’re right. Coins are not essential to this process. We could imagine a mechanical device that drops marbles into jars directly, provided that we believed the marble dropping process was equivalent to the coin flip in terms of 50/50 chances. In R, here is a faster way. We just sample out of a “hat” containing each of the team names. set.seed(8675309) # same seed, on purpose teamnames &lt;- c(&quot;Sharks&quot;,&quot;Jets&quot;) numPeople &lt;- 10 teamassignments &lt;- sample(teamnames, numPeople, replace=TRUE) teamassignments ## [1] &quot;Jets&quot; &quot;Sharks&quot; &quot;Jets&quot; &quot;Sharks&quot; &quot;Jets&quot; &quot;Sharks&quot; &quot;Sharks&quot; &quot;Sharks&quot; ## [9] &quot;Jets&quot; &quot;Sharks&quot; Well, that was simpler. Notice also that by using the same random seed, I got the same team assignments, even though the code was completely different. (By the way, the [1] and [9] here are just index–count–values that R writes out when the data flow across multiple rows. So you know that the first value on the second row is in fact the 9th element of the vector.) There’s another difference here, which is that instead of going one by one for each person, I made the selection all at once by passing the number of people to the sample function instead of asking for one sample. Because I’m sampling numPeople times and there are only two outcomes (“in the hat”), I have to add the argument replace = TRUE when I call the sample() function. This means that after I look at the result, I put it back in the hat (I replace it), and then draw again. Otherwise, I could sample only as many times as I have choices. Sampling without replacement is suitable if, as in the independence test from earlier, all I wanted was to put a set of choices in random order. As we said, this is like shuffling a deck of cards by dropping the deck into a big bag, shaking it, and then pulling the cards out one by one. No replacement. But some observations do occur more than others To complete the functionality of our data-generating machine, we need a way to extend this sampling process to outcomes that are not equally likely. What if we want to base a simulation on the observed proportion that 23 out of 40 people chose “over” and only 17 chose “under,” but we only want to sample 10 people? Once more, I will show you two ways to do this. The first way is to put all 40 virtual cards into our virtual hat, and sample 10 times (with or without replacement? what do you think?). The second way involves abandoning our connection to these virtual cards and replacing them with the mathematical abstraction of a probability. First, the virtual cards: set.seed(2125551212) # I can choose any see I want tp_cards &lt;- c(rep(&quot;over&quot;, 23), rep(&quot;under&quot;, 17)) numPeople &lt;- 10 simulated_tp &lt;- sample(tp_cards, numPeople, replace=TRUE) simulated_tp ## [1] &quot;over&quot; &quot;over&quot; &quot;under&quot; &quot;under&quot; &quot;under&quot; &quot;under&quot; &quot;under&quot; &quot;over&quot; &quot;under&quot; ## [10] &quot;over&quot; Hopefully this code seems pretty straightforward. Why did I use replace=TRUE? I didn’t need to do that, since I was only drawing 10 samples from a hat with 40 cards. However, by replacing the card I drew each time, I made sure that the chances were the same for each virtual person in my sample. Suppose that I did not replace the samples each time. And suppose that, by dumb luck, I drew five “unders” in a row on my first five draws (this sequence did actually happen on draws 3-7 above). The sixth draw is now coming from a hat with 23 overs and 12 unders. That draw definitely has different chances of coming up under, and, importantly, it is not independent of what happened before. In real-world sampling, I do not expect the answer from the sixth person I ask to depend on the answers given by the previous five. So it is essential that my simulation have this same property. (Of course, it is true that the proportions in the hat changed even after the very first draw, assuming I don’t replace the cards, but I used the sixth draw to make it more obvious.) Think about this: Now that we have added replacement to the sampling process, if we change the sample size to 40 (numPeople &lt;- 40), are we guaranteed to get 23 overs and 17 unders? Now for the second way. I don’t really need to produce 40 cards at all. I just need to recognize that 23/40 or 0.575 is my target probability of “over” on every single sample. (17/40 or 0.425 is the probability of “under” automatically). I can still do everything with the good old sample() function. set.seed(2129981212) # I can choose any seed; I like using phone numbers. numPeople &lt;- 10 simulated_tp &lt;- sample(c(&quot;over&quot;,&quot;under&quot;), numPeople, replace=TRUE, prob=c(0.575, 0.425)) simulated_tp ## [1] &quot;under&quot; &quot;over&quot; &quot;over&quot; &quot;over&quot; &quot;under&quot; &quot;under&quot; &quot;under&quot; &quot;under&quot; &quot;under&quot; ## [10] &quot;over&quot; set.seed(2129981212) # if I want to same results simulated_tp &lt;- sample(c(&quot;over&quot;,&quot;under&quot;), numPeople, replace=TRUE, prob=c(23/40, 17/40)) simulated_tp ## [1] &quot;under&quot; &quot;over&quot; &quot;over&quot; &quot;over&quot; &quot;under&quot; &quot;under&quot; &quot;under&quot; &quot;under&quot; &quot;under&quot; ## [10] &quot;over&quot; There it is. For good measure, I included the probability expression using decimals and fractions, to prove that it doesn’t matter to R. I did, however, reset the random seed, otherwise I would not have gotten the same results. Recap In this chapter, we used computer simulation to overcome one of the limitations of a single data collection with a small sample. We introduced the R function sample() which can be used to either shuffle a bunch of data in random order or to select (i.e. sample) some data from a larger set. We saw a couple of different ways of doing this, some in which we literally create the large set and some in which we use probabilities instead that “act like” we have a larger set. More importantly to the original goal, using the R function sample() we saw that we can create many replications of simulated data collections. We can use these replications to see what answer-pair observations are consistent with independent answering processes and what observations seem highly unlikely if the answers are truly independent. Notice that we keep using this language of likely/unlikely, consistent/inconsistent, etc. None of our results “prove” independence or association between question pairs. That’s just not possible with stochastic/random processes and finite samples. We need to be able accept this condition of uncertainty. Note, though, that if we get larger and larger samples, there are some statements that we can make with more and more confidence. Statistics does not eliminate uncertainty, but it can be used to put precise bounds on it. "],["shades.html", "Chapter 4 (An Infinite Number of) Shades of Gray (or Brown) Poopiness Crappiness", " Chapter 4 (An Infinite Number of) Shades of Gray (or Brown) We’ve taken the two-kinds-of-people idea pretty far already. But it’s time to acknowledge the elephant in the room. Not every question about attributes, preferences, or behaviors can be answered in such an either/or manner. Digitidiness might be one of those things. Consider the following dialogue: Stacy: “There are liberal and conservative kinds of people, Trang. Which one are you?” Trang: “Well, you know I’m not sure I’m exactly one or the other. I think I’m somewhere in the middle.” Although we often use them as discrete categories, the words liberal and conservative might be better thought of as endpoints of continuous scale. In fact, they might even apply to different dimensions of political thought with respect to social issues or economic issues. If you think about it, it’s not hard to come up with other examples of “categories” that really just describe one end or another of a continuous scale. Yes, there are short people and tall people, but everyone has a height, and a lot of people are “about average.” Height is just a number on some scale. So it wouldn’t necessarily make sense to put people into the categories of tall or short. This may be an old-timey analogy in the age of digital streaming, but think of the knobs on a stereo receiver. Some of them click between categories, like the input-selector (phono, radio, aux). And some of them turn smoothly through continous values, like volume and tone. Categorical, or discrete, variables are the clicky knobs. In the great toilet paper debate, we were able to identify two kinds of people based on two possible responses to the question of roll orientation. Two answers; two kinds. If instead of discrete categories, we have a number on a continuous scale, does that mean that there can’t be “kinds” of people anymore? To answer this question, we’ll need to understand what exactly we’re talking about when we characterize people using a continuous scale. Poopiness Consider poopiness. And consider a scale where some people are really poopy (close to poopiness = 1), some aren’t poopy at all (close to 0), and many are somewhere near the middle. That’s not a very quantitative description. I used the words “some” and “most,” but I didn’t give you counts like I did in Table 1.1 about toilet roll orientation. I will try to do that in just a minute. Meanwhile, notice that my scale here runs from 0 to 1, which I will also sometimes write as [0,1]. When it comes to height, we have well-established scales, like inches or centimeters, and we can use measuring sticks. But when it comes to liberalism or poopiness, the scale does not necessarily refer to something we can see directly. Nevertheless we can use the scale to compare people and to see how a whole bunch of people “measure up.” I’ve set the scale to [0,1], because it is a common scale, but it could have run from 1 to 10, for example, without significantly changing anything in what I’m about to say. If I showed you the poopiness data for a sample of people, the list would look something like Table 4.1. As before, in this table each row stands for one person. To protect their identities, everyone is identified only by a number (e.g., 0083), which is shown in the first column. In the second column is each person’s poopiness value. Table 4.1: Don’t ask me how I got these numbers. poopiness 0040 0.319 0140 0.703 0033 0.401 0107 0.544 0031 0.538 0100 0.657 Poopiness is shown as a decimal number. Part of the reason I’ve used this scale, instead of 1-100, is to emphasize that the data values can be arbitrarily close to one another. Two values may be different by 0.1 or 0.03, or even 0.000027, if we have enough precision in our data to say such a thing. These data are called numerical or quantitative as opposed to categorical. There are actually 148 values in the data set, but I’ve only shown the first six in Table 4.1. It’s not as easy to make sense of a bunch of decimal values like this as it is to look at simple counts of categories (like 17 for chunky, 23 for smooth). However, this sense-making problem has been solved by representing the same data using dot plots, stacked dot plots, frequency tables, and histograms, which you can read all about in any standard textbook (for example OpenIntro Statistics, Chapter 2). I’m going to go straight into the frequency table and histogram, which you may indeed have seen before. These are the most commonly used representation for data of this kind. Again, it is a bit awkward to count how many people have poopiness value of exactly 0.473. Maybe there is one, maybe none. How would we interpret that answer, anyway? Instead, we can group values into ranges, or “bins,” e.g. 0-0.05, 0.05-0.1, 0.1-0.15, etc. so that the ranges together span the entire possible range, which in this case is [0,1]. We can then count how many of our data fall into each bin.5 This table of counts is typically called a frequency table. Frequency is just another word for counts. Table 4.2: Frequency Table for Poopiness Range Frequency 0 - 0.05 0 0.05 - 0.1 0 0.1 - 0.15 0 0.15 - 0.2 0 0.2 - 0.25 2 0.25 - 0.3 9 0.3 - 0.35 12 0.35 - 0.4 9 0.4 - 0.45 9 0.45 - 0.5 14 0.5 - 0.55 14 0.55 - 0.6 9 0.6 - 0.65 19 0.65 - 0.7 9 0.7 - 0.75 13 0.75 - 0.8 16 0.8 - 0.85 7 0.85 - 0.9 5 0.9 - 0.95 1 0.95 - 1 0 Figure 4.1: Histogram of Poopiness A histogram is a bar plot of counts for values that fall into certain numerical ranges. So it’s a bar plot of the data in Table 4.2. But oftentimes you’ll just see the histogram without the frequency table. It is worth noting that the information contained in the frequency table is equivalent to the information contained in the histogram, but it is sometimes easier to get a general idea of what is going on in the data by looking at the histogram. Consider the range of poopiness values from 0.40-0.45. Our data set has 9 values in this range, as you can see in Table 4.2, so the height of the bar above this range of values on the x-axis (horizontal axis) is 9. I’ve colored it in pink only to help you see what I’m referring to. The y-axis in Figure 4.1 is labeled “Frequency,” as in the table. Some more jargon: the numerical values that separate the bins are called “breaks.” In Figure 4.1, the breaks occur are at increments of 0.05. Question: Given that there are 20 possible bins in the histogram in Figure 4.1, but only some of them have non-zero counts, are there 20 kinds of people (in terms of poopiness) or 15 kinds of people? Trick question? You bet. The breaks (and thus bins) in a histogram are arbitrary. I can choose any breaks I want, as long as all of the data points fall into exactly one bin. (I can’t just exclude some bins, though. That would be cheating.) The histograms in Figure 4.2 are both perfectly valid histograms. One of them has four bins, and one of them has only two bins. Figure 4.2: Other Histograms of Poopiness It’s tempting to take the counts on the right of Figure 4.2 and declare that there are two kinds of people. After all, this gets us back to familiar territory. Ta-dah! Figure 4.3: This is a terrible, horrible, no-good, very-bad thing to do. As you can tell, because it says so right in the figure caption, this is terrible, horrible, no-good, very-bad thing to do. Why do you think it is a bad thing to do? Choose one: The split was made at 0.5 on the poopiness scale, but that is not the average value of poopiness in the data set, which is closer to 0.57, as can be seen in Figure 4.1 (or from the “raw” data themselves). You should always use at least 5 bins when you have numerical data Representations of data should communicate honestly about the nature of the data themselves. In this case, poopiness is not a category. What I did here was take a numerical/quantitative value (poopiness) and mis-represent it as a categorical value. I did it by dichotomizing it, i.e., by splitting off everyone above 0.5 and labeling them as “poopy.” I could have alternately split at the mean or median value and labeled the resulting two groups as “low poopiness” and “high poopiness.” But this would still have been a mis-representation. It would hide the fact that poopiness comes in a continuous range of values. ASIDE (delivered in a hushed voice): I won’t be able to convince you of this now, but it turns out that if you do this—if you dichotomize numerical data—you will BREAK STATISTICS! Ok, that sounds a bit dramatic. But in all seriousness, one of the jobs of statistics is to understand associations between different variables, such as poopiness and, say, earning potential. If you treat poopiness (or other variables) as discrete when they are really continuous, you may very well get the wrong answers. As the man down the street from where I used to live often muttered to himself while waving his arms in the air, THAT IS AN ABSOLUTE IRONCLAD MATHEMATICAL FACT. No, but in all seriousness, there is a very good paper on exactly this subject (MacCallum et al. 2002). Dang it! you say. You’ve taken me down this rabbit hole of poopiness for too long. How many kinds of people are there? Are you saying that if one looks at properties that are described by numbers instead of categories, that are no kinds of people at all? Is it all just shades of gray (or brown)? Mixtures Remember Figure 4.1? (Don’t click it!) Here it is again so you don’t have to scroll back. Data scientists like to say this picture shows you the distribution of poopiness in our sample. Statisticians use the word distribution in a more formal way that is best put off until we actually need it. We don’t need it yet. What if I told you that there ARE two kinds of people; you just can’t see them unless I give you special glasses (or more information). If I gave you special glasses (or information), you would see this: Figure 4.4: A mixture of poopiness By what dark magic have you colorized the data! you say. Or, perhaps you just said, hm, interesting. In Figure 4.4, I’ve made a histogram with bars in two different colors, light green and pink. The colors are slightly transparent so that you can see both the green and pink distributions in their entirety even though they overlap. That’s what the brownish bars mean. You’re looking at the overlap of the green and pink bars, not another set of bars. Now, if you compare this histogram closely with the original, colorless histogram above, you’ll see that the bin ranges are the same (width=0.05), and the the counts of green and pink bars add up to the total values that we had before. If there are green people and pink people, or in any case two different kinds of people, and if their poopiness is distributed as shown in Figure 4.4, then the poopiness of the mixture of these two groups of people will look just like Figure 4.1. Ok, but that doesn’t explain how you would know that there are two groups. If I didn’t tell you. That’s because you wouldn’t necessarily know. You would need to have more information. Now you might suspect something if you saw a distribution that looked like this: Figure 4.5: A suspicious mixture of poopiness In Figure 4.5, the distribution has a double-hump like a Bactrian camel. In spite of that, it is not called a Bactrian distribution–which would make me happy–but a bimodal distribution. The point that I’m trying to make here is that a bimodal distribution makes you suspect that there could actually be (at least) two groups mixed together in our data. But the original data for poopiness did not look bimodal. I suggested to you that you would need more information to determine if there are two groups. And so, I present you with… Crappiness! Crappiness For each of the subjects in our poopiness data set, we have also collected data on their crappiness. Crappiness is also reported as a numerical value ranging from [0,1]. It’s sort of like poopiness, but different. Here are some values: ## poopiness crappiness ## 0040 0.319 0.564 ## 0140 0.703 0.415 ## 0033 0.401 0.729 ## 0107 0.544 0.374 ## 0031 0.538 0.853 ## 0100 0.657 0.316 And here…(drum roll please)… is a histogram of crappiness! Figure 4.6: Histogram of Crappiness Hmm. I bet you were hoping that the crappiness data would look obviously bimodal, but it’s not obvious. Nevertheless, hopefully you trust that I wouldn’t lead you on a wild goose chase for no reason. Perhaps you can even see it coming. If we look at poopiness and crappiness separately, there is no clue that there might be distinct groups of people in our data set. But if we look at them together… there is. When we looked at categorical data for two two-kinds-of-people questions, we made 2x2 contingency tables. We also used the word “dimension,” for example to say that we were describing people along two dimensions (recall: toilet paper and peanut butter). Now that we are looking at numerical data (poopiness and crappiness), we can also use two dimensions, as in a two-dimensional scatterplot, to examine both variables at once. A scatterplot is just a name for a data plot, in which the position of each data point corresponds to its coordinates along more than one dimension. We often refer to two-dimensional coordinate systems as (x,y), where x is the horizontal axis and y is the vertical axis. Technically, in this case our coordinate system is (poopiness, crappiness). These are the names of our dimensions. But we will still often refer back to the idea of an x- or y-axis. This scatterplot is shown in Figure 4.7. Each point represents data from one person, with their poopiness value on the x-axis and crappiness on the y-axis. Figure 4.7: Scatterplot of Crappiness vs Poopiness Alas, oh data! Your bimodal nature has revealed itself in the higher-dimensional plane! How many kinds of people are there? When it comes to poopiness and crappiness, people exhibit a continuous range of values, so we can’t neatly put them into buckets. Neither poopiness nor crappiness appear to be bimodally distributed on their own. However, when examined together, as in the scatterplot in Figure 4.7, a pretty suggestive pattern emerges in the data. There are two clusters of points, one group of which is lower in poopiness but higher in crappiness than the other. Interestingly, though, in both groups poopiness and crappiness tend to increase together. That is, they appear to be associated, not independent. I do not mean to imply that clusters of points can always be found if we have data along many dimensions. That is certainly not always the case. The present example was concocted (I admit it!) to show that groups can emerge, even in numerical data. Cluster analysis (Kaufman and Rousseeuw 2009) refers to set of data-science methods all about looking for the existence of groups in multidimensional data. Check your understanding Based on the scatterplot in Figure 4.7 and the grouped-by-color histogram for poopiness in Figure 4.4, describe what the equivalent grouped-by-color histogram for crappiness would look like. Would it look the same or different? Explain. Technically, each range is a semi-open interval, e.g. (0.1,0.15], so that any values exactly equal to 0.1 can only be included in one bin and not the ones on either side.↩︎ "],["cut-scores-and-abnormality.html", "Chapter 5 Cut Scores and Abnormality", " Chapter 5 Cut Scores and Abnormality Because that’s not what normal people do. — things my spouse says You’ll recall that I previously warned against possible negative consequences of setting arbitrary cut points to dichotomize a data set—that is, turning numerical data on a continuous scale into two categories by using a cutoff value. But now consider the following scenarios: To pass the written test for your a driving learner’s permit in California, you must answer at least 38 questions correctly out of 46. That’s 82.6% correct. At 80.4% (37/46) or below, you fail and have to retake the test on another day. A patient’s blood test shows levels of ALT (alanine aminotransferase) at 77 units per liter. The lab report labels this as “abnormally” high, and the physician is concerned about possible liver damage or disease. These two examples involve just the kind of dichotomization that I cautioned against, and yet they occur very commonly in practice. So what gives? Is it wrong to use cutoffs this way? Why do people do it? The short answer is that we often find ourselves in need of a classification (pass or fail; diagnose liver disease or not) but without a perfect classification device. Rather we have only indirect measurements (of knowledge or liver function) in some quantitative measure. Perhaps you once found yourself on the “border” between letter grades for a course and were particularly perturbed (or relieved) by the imperfections of such a system. Or you may have found yourself with “slightly” abnormal levels in a blood test and wondered whether you should seek further tests. Both the California department of motor vehicles and the physician in our scenarios need to make a decision based on imperfect evidence. They want to be able to say that the person’s test results show that they are ready to get behind the wheel of a car, in one scenario, or suffering from liver problems in the other. But all they can really do is express this belief using a probability. This probabilistic judgement is based on a mathematical model that relates traits like readiness-to-drive or liver-disease to certain test results. Understanding how these models come into existence is one of the learning objectives of this course. The term normal distribution arose in statistics because the particular bell-shaped distribution occurs so frequently. If poopiness were normally distributed in our sample from before it might look like this. Figure 5.1: Normal poopiness Technically speaking, all of the values, including the maximal value of 0.962 that we observe in Figure 5.1 are normal. Poopiness varies in the population. It is impossible to be abnormally poopy, under the circumstances. By definition, some values at the extreme ends of a normal distibution are less likely to occur than values in the middle. But still they may occur rarely. It is only when extreme values (large or small) are associated with other conditions of interest, such as the relationship between elevated ALT and liver disease, that it makes sense to “flag” these extreme values. We say that we discretize continuous variables (i.e., turn them into discrete categorical variables) by using thresholds or cut scores. Passing a test or being flagged for liver disease is usually based on a single cut score. The cut score is a numerical value, and data that fall above or below that value are categorized differently. It is possible to use more than one threshold. For example, in the next chapter we will see that people can be classified as belonging to different generations based on their age, and neighborhoods can be categorized based on their population density. We started out this course on a quest to answer our first big question: How many kinds of people are there? En route, we have examined both categorical data, such as from two-kinds-of-people questions, and numerical values like poopiness. The toilet paper and peanut butter orientation questions may seem silly and inconsequential to you. I can only imagine what you might think of the poopiness and crappiness dimensions that I completely made up (I admitted it!). However, in the next chapter we will see how some more standard variables are used to profile American people. Next week, we will see more issues of discrete/categorical and continous/numerical multi-dimensional descriptions of people that arise when it comes to personality psychology. Remember these terms? kind, type, category discrete, continuous, numerical, dichotomous crosstab, two-way table, contingency table association, contingency, dependence latent factor, dimension, trait measurement, model, histogram, bimodal, cluster "],["the-modal-american.html", "Chapter 6 The Modal American", " Chapter 6 The Modal American The NPR podcast Planet Money aired an episode in the summer of 2019 called The Modal American. If you haven’t listened to it, you should stop what you’re doing and listen to it right now. (Seriously, this will make a lot more sense). The podcast hosts, Kenny Malone and Jacob Goldstein, set the stage as follows. People sometimes talk about the “average” American, but that doesn’t really make sense, even as an idea. If you average the traits of all Americans, you don’t end up with a real human being.6 Malone suggests that what people are really thinking about is the modal American, i.e., the most common type of American. The kind person you’re more likely to bump into on the street than any other. The hosts then consult with economics and data reporter Ben Casselman to help identify who is the modal American. For the purposes of their investigation, the kinds of American the Planet Money hosts are interested in are determined by groupings of demographic variables like those collected by the US Census. The census does not, to my dismay, include questions about peanut butter preference, toilet paper orientation, digitidiness, or poopiness. Rather, they stick with things like sex, race, age, income, marital-status, etc. It turns out that different people mean different things when the ask, “how many kinds of Americans are there?” In any case, this journey to put Americans into buckets leads down some paths that will be very familiar to you by now. Step one: categories First of all, some of the variables, like sex, race, and marital status, are categorical to begin with. We can ask people to answer these survey questions, and then we can combine their answers into kinds like married-Hispanic-male or single-White-female types of people. Some of the variables included, like neighborhood-type, are categorical in the way we think about them (read: rural, suburban, or urban), but are actually derived from numerical variables. In this case, neighborhood type is derived from the population density in the census block. Other variables, like income and, arguably, age, are just plain numerical. But to identify the most common kind of American, our podcast hosts need to put people into categories (like rich and poor–to make things dichotomous–or low-, middle-, and high-income). So they discretize all of the numerical variables into a small number of categories. They “bin” the incomes as well as the ages, which they combine into generation labels like “Generation X” and “Baby Boomers.” Step two: contingency table Having done all this, the next step is to combine the data about all Americans along each of these variables into one big contingency table. It’s not a two-way table, because there are more than two variables, but it’s still a contingency table. (It is not very common to use terms like four-way or five-way table.) The choice of variables, and specifically the choice of the number of categories to keep for each one, determines how many kinds of Americans there can be. Having made the choices, it is a simple matter of looking to see which is the most common kind. Along the way, the idea of dependence, or association, also comes up, but it is not necessarily mentioned by name. Supppose you know the most common age category is Children. And suppose the most common category of marital-status is Married. It does not necessarily follow that married-children is the most common category in a two-way table made from those two variables. Of course, the opposite is true. And this is because age and marital status are associated variables. The older you are, the more likely you are to be (now or previously) married, as opposed to never married. In general, you cannot assume that the most common category in two variables will combine to produce the most common two-variable-category. But this would be true if the variables were independent. For independent variables, it must be the case that the most common joint category will emerge from the simple combination of the most common individual categories. You could actually use that as a definition of independence. Take sex and race, for example. In the Planet Money podcast, both of these are actually treated as dichotomous variables (all race categories other than white are combined into one, so it is basically white or not; perhaps you are hearing “there are two kinds of people in this world…”). Since female is the most common sex and white is the most common race category, it has to be the case that white-female comes out as the most common combination. Sex and race are independent. Perhaps it is reassuring to you that the ideas we have been developing are not just limited to semi-jokey two-kinds of people questions, or Buzzfeed personality tests. But that they apply exactly as before to serious data. Exercises At around minute 5:15, the hosts describe the what they see looking at the (histogram of) age distribution. What kind of distribution are the hosts describing? Why such (over)simplification? Hint: there is a discussion around 9 minutes in. The methodology of the Planet Money analysis is described on their website. From this, determine how many variables there are, how many categories there are for each one, and thus how many kinds of Americans (i.e., how many buckets) there are in total. We encountered this fact in the form of a joke in Chapter 1, specifically here↩︎ "],["sixteen-personalities-or-five-factors.html", "Chapter 7 Sixteen Personalities or Five Factors? Recap", " Chapter 7 Sixteen Personalities or Five Factors? Before you read this chapter, you might want to go ahead and take one of the personality tests based on the Meyers-Briggs Type Indicator (MBTI) categories and/or the five-factor model of personality (also called the Big Five). There is only one “official” MBTI, which is a commercial product. However, there are several free alternatives online which use the same typology classification. There are also several variations of the Big Five. Test yourself: MBTI-style at 16personalities.com or https://openpsychometrics.org/tests/OEJTS/ Big Five http://www.personal.psu.edu/~j5j/IPIP/ https://bigfive-test.com/ https://openpsychometrics.org/tests/IPIP-BFFM/ General information about these test items: https://ipip.ori.org/ I will only minimially describe the MBTI and the Five Factor Model (FFM, or Big Five) here, in terms of the topics we have been discussing. There are many resources for learning more about these personality tests. Some are referenced under further reading. MBTI The MBTI will categorize people, based on their responses, dichotomously along each of four dimensions, also called “scales.” These are: Extraversion-Introversion (E-I) Sensation-Intuition (S-N) Thinking-Feeling (T-F) Judging-Perceiving (J-P) Thus there are sixteen possible combinations, for example “INTP.” Each person is assigned to one of these sixteen personalities. Many online tests will provide you with a report to help interpret your classification. That is, the four dimensions are understood to come together in some holistic picture of your “type.” Big Five The term “Big Five” is a commonly used term for the five-factor model of personality. Based on responses to questionnaires, people are assigned a numerical score along five dimensions (also called scales or factors!) Neuroticism refers to the tendency to experience negative feelings. Extraversion is marked by pronounced engagement with the external world. Openness to Experience describes a dimension of cognitive style that distinguishes imaginative, creative people from down-to-earth, conventional people. Agreeableness reflects individual differences in concern with cooperation and social harmony. Agreeable individuals value getting along with others. Conscientiousness concerns the way in which we control, regulate, and direct our impulses. Fun fact: both OCEAN and CANOE are mnemonic devices that can help you recall the names of the Big Five dimensions. Since the results of a Big Five test, such as the IPIP-NEO, are five numbers, you don’t get assigned a personality “type” by these tests. Rather, you may be provided with an explanation of what it means to score high (or low) on, say, Extraversion. You may have noticed that extraversion (occasionally spelled “extroversion”) appears on both the MBTI and the Big Five. Twenty Questions (about Extraversion) Suppose, for whatever reason, we want to identify a person’s extraversion. We may want either (a) to classify them as extraverted or not (i.e., introverted), or (b) to quantify a degree of extraversion, say on a scale of 0-100. Why not then just pose the question in the following way? In the first case: Choose the one that describes you: Extraverted | Introverted or, in the second case, Locate yourself on the scale: Extraversion 0 . . . . 50 . . . . 100 Personality tests, such as those we’ve discussed above, do not ask questions like these. Rather, they include many different questions, sometimes twenty or even more, about things like going to parties, making friends, and drawing attention to oneself. Why ask twenty questions instead of just one? Recall from the great toilet paper debate that it was not necessary to ask twenty questions to know whether you were an over-hanger or an under-hanger. However, when we discussed digitidiness, we suspected that two different questions may have both been getting at the same latent factor. The situation here, in the real-life domain of personality testing, is similar. Golda asks: So the idea is to design questions that are smarter than the user? Do we not trust people’s abilities to self-categorize? Maybe these tests would be more effective if they allowed people to draw their own conclusions about themselves? Sidney says: Developers of these questionnaires acknowledge that it is not as easy for people to self-categorize (what do you really mean by introverted? is that a bad thing?) as it is to describe how they feel at parties. Furthermore, category labels may seem socially desirable to the respondent, leading them to make a certain choice for “appearances.” So, to some extent yes, psychologists may not always trust people to self-categorize. But I think they would still defend everyone’s right to draw their own conclusions from the results of a personality test. Psychologists believe that extraversion is an underlying factor invented the idea of extraversion to explain patterns of behavior, including patterns of responses to questions about how people feel in various situations. Such as enjoyment or lack thereof in being the center of attention. The use of indirect evidence such as questionnaire responses to make inferences about psychological traits is the main task of psychological measurement (also called psychometrics). The main challenge of psychological measurement, perhaps even the reason for its existence as a method and field of study, is that human beings are noisy. Put another way, you cannot expect a deterministic relationships between a person’s response or behavior (how a person feels or acts) in one situation and how they respond or behave in another. Even someone we may want to call an extraverted person is not always extraverted. And an extraverted person might not always answer questions about their feelings in the same way. It is also hard to directly observe extraversion or to ask people to simply self-report it. Extraversion manifests itself differently at different times and in different contexts. Whether this noise is due to some mysterious internal process, like a coin flip in your brain, or due to many unnaccountable external factors, like whether you slept poorly that day, we can’t really say. What we can say is that human noisiness manifests itself as measurement error when we try to measure things like extraversion using questions and other observations. Variability in human behavior is one of the factors that may contribute to measurement error. If the same person’s behavior varies randomly over time, then our observations at any given time are subject to this variation. But we could also imagine having measurement error sources that come from ambiguously worded questions, which would still lead to errors even if behavior were stable. Say someone is asked if they engage in risky health behaviors. Even if their behavior is very consistent (for argument’s sake), different people make have different perceptions of what counts as risky. So in that case measurement error results from (variation in) the interpretation of the question rather than from variation in risk-behavior over time. The word error makes it sounds like there is a right answer, and that (personality) tests get it wrong. This is, indeed, one view (called true score theory in psychometrics). However, you don’t have to believe that there is a right answer. For example, you can believe that human beings have some amount of inherent unpredictability. But just because human behavior is not perfectly predictable, just because observations are not deterministically related, we can still say that—in spite of measurement error—some patterns do remain. The responses to different questions about extraversion, for example, are associated with one another. People who say that they feel comfortable at parties are also likely to say that they make friends easily. But what’s the point? Trying to describe people in terms of kinds or numerical scales is complicated. Why do we even bother? It’s tempting to say that we just want to understand ourselves better, and that is certainly a reasonable answer. Sometimes, though, we want to predict how someone will act in the future, perhaps in a situation that differs from one that they have faced in the past. In that case, we can’t exactly use the past to predict the future, unless we do so by making inferences about underlying traits from past behaviors and then predicting how someone with those particular traits would act in a new context. This purpose drives some uses of tests based on the MBTI and the Big Five, for example by employers or career counselors. However, although the MBTI is often used for these purposes, one should exercise caution in doing so (Pittenger 1993). You should certainly not assume that all personality tests do an equally good job of providing information for the desired inferences. According to the standards of the American Psychological Association (American Educational Research Association and American Psychological Association and National Council on Measurement in Education and Joint Committee on Standards for Educational and Psychological Testing 1999), whenever psychological tests are used for some specific purpose (e.g., employment, admission to a school or hospital, or even in court) there must be a valid argument for the intended purpose of the test scores. This validation argument will usually involve many facets, including how consistent the results of the test are, whether it is a fair test for all groups of people, whether test scores really are associated with relevant outcomes in the domain of use, and so on. These arguments, and challenges to them, are all part of what we mean by validity. Recap So far, in pursuit of our question “How many kinds of people are there?” we have not been overly concerned with specific proportions or probabilities. Our discussion has been rather metaphysical. We have tried to understand how differences that we observe among people can be expressed in terms of kinds (categories) or quantities (numbers), which are themselves different kinds of data. We have looked at dichotomous questions of both frivolous (toilet paper orientation or peanut butter preference) and less frivolous (attainment of Bachelors degree, employment status) kinds. Similarly, we’ve considered numerical variables from poopiness and crappiness to neuroticism and conscientiousness. We’ve seen that categorical questions can include more categories than two (e.g., peanut butter preference, if we include “hate all” and “don’t care,” but also ethnicity or neighborhood-type). We saw that numerical variables can sometimes be discretized and thus converted into categories, such as when income is classified as high or low, when age is converted to “Generation Z,” or when a score on a personality test assigns you the value “Perceiving” as opposed to “Judging.” We saw that categorical questions can naturally classify people into types, and that when we combine different kinds of questions, the number of types/kinds/buckets can increase. But we also observed that the number of observed types does not necessarily increase to its mathematical maximum because answers to different questions can be associated. In other words, the number of factors or dimensions can be smaller than the number of questions. (But it can also be equal to the number of questions, in simple cases). We also saw that numerical variables can be used to classify people. This can happen when we use cut-scores as thresholds and assign people above a threshold a value like “pass” or “abnormal” or “introverted.” The distribution of a numerical variable can be observed using a histogram, and sometimes a histogram is observed to have two bumps, which suggests a mixture of two different groups. But we also saw by plotting points on a coordinate plane of poopiness and crappiness that numerical data can cluster into groups, even when we don’t see tell-tale signs of those groups along any single variable. The notion of clustering is all about identifying groups of data points that might designate types. According to some researchers, types can actually be recovered in the five-factor (Big Five) model of personality, which until recently was strongly thought of as not having types. You’ve probably figured out by now that this course is less about answering the Big Questions than about understanding how they might in principle be answered. What does one have to understand to even formulate an answer? Different people may indeed come to different answers about how many kinds of people there are. Hopefully you feel better equipped to reason rigorously and to discuss more precisely the evidence used to support any particular claim. "],["when-and-how-will-you-die.html", "Chapter 8 When and How Will You Die? Not Quite Death, but, um… Rain? Ways of thinking about probabilistic statements Death", " Chapter 8 When and How Will You Die? It is difficult to make predictions, especially about the future. — Niels Bohr (probably) In our first Big Question, we began to look at individual differences between people or what statisticians call variation within a population. If there is no variation—like in the bizarro world where everyone orients their toilet paper in the “under” orientation—then there is nothing to talk about, at least not statistically speaking. There is, however, considerable variation in health outcomes and human lifespan. Lots to talk about there. In our next Big Question, we ask “when and how will you die?” and “what, if anything, can you do about it?” What kind of question is, “when and how will you die?” Well, according to some of my colleagues, it is a morbid question. Feelings aside, we might say that it sounds like a prediction question, since it is about the future. So to explore this big question, we will need to understand what it means in general to make a forecast about some future event. We’ll also find it useful to distinguish between predictions that are or are not explanatory. Most efforts in health sciences attempt to explain relationships between behavioral and genetic factors and health outcomes. In particular, they try to understand causal effects. So in the next few chapters, we will also try to understand causal explanations more generally. Not Quite Death, but, um… Rain? Perhaps it is a good idea to warm up, before we face the grim reaper. What does it mean to say there’s a 30% chance of rain tomorrow in New York? Does it mean that it will definitely rain in 30% of the city (say, Brooklyn), but not in the other 70%? Or that it will rain for 30% of the day (say, from 8am-3pm). Here are some possibilities to consider: It will definitely rain in some parts of the city but not in all of them It will definitely rain for some part of the day in all of the city It will definitely rain for some part of the day in some of the city It may or may not rain anywhere in the city at any point in the day. Read here for an explanation of what meteorologists probably mean Stochastic vs Deterministic relationships Sometimes when I say definitely, I mean probably. Like if I say, I’m definitely going to do something about all of this clutter on my desk. But when I really mean business, I say deterministically. It definitely sounds more serious. Meteorologists—scientists who model the weather—cannot tell us deterministically about weather events. Recall that we previously considered deterministic associations between two two-kinds-people questions. We imagined a world where if you knew a person’s answer to one question (e.g., are you right- or left-handed?) then you would know for sure their answer to another. Now some of our examples were hypothetical and unrealistic, because, well, people are not in actuality very deterministic. If we want realistic examples, we end up making use of tautologies like “Are you single? Are you in a relationship?” But some events in nature are, more or less, deterministically related. An example might be something like, if I let go of the umbrella I am holding, then it will fall to the ground. If A then B. No exceptions (and no strings). You can imagine that I asked two questions: (a) did I let go of the umbrella (at a certain time T)? and (b) immediately after time T, did the umbrella fall to the ground? If you know the answer to one question, then you know the answer to the other. Weather events are stochastic. As we know all too well from experience, they have an element of chance or randomness, like tossing a coin or rolling a die. So, just as we can say that a coin has a 50% chance of coming up heads—assuming it is a fair coin—we can make statements like there is a 30% chance that it will rain tomorrow. Stochastic is another word for random, but I prefer it because the word “random” is often used casually to mean weird or unusual (as in, “that’s random!”) Although we can’t speak with certainty about random, or stochastic, events, that doesn’t mean we can’t speak usefully about them. We just need to learn to speak probabilistically. Ways of thinking about probabilistic statements Ensembles One way to think about the 30% chance of rain is to imagine that our experience in the world is one possibility in a multiplicity of possible worlds. See, I told you this idea of multiple alternate universes was going to be important! Imagine that there are 10 possible worlds, indistinguishable from ours in terms of the laws of physics, and that tomorrow it will in fact rain in 3 of them. To the great being-who-knows-all-things, which 3 worlds will see rain may well be known. However, to us mortals who merely live in the world that we know, we don’t know which one of these possible worlds is the one we live in. Nevertheless we are capable of imagining these different potential outcomes. As you just did. It didn’t have to be 10 worlds, of course. That was arbitrary. If we imagined thirty worlds, it could rain in 9 of them, as I’ve represented in Figure 8.1. I did this by making thirty circles and coloring in 9 of them at random. Since I like to pull back the curtain every once in a while, I will even show you the R code I use to generate this simple figure. # start with a 10 x 3 grid of points norain &lt;- cbind(rep(1:10,3), rep(1:3, each=10)) # choose (sample) nine at random, using the sample() function in R rainworlds &lt;- norain[sample(1:nrow(norain), 9),] # plot the points plot(norain, xlab=&quot;&quot;, ylab=&quot;&quot;, ylim = c(1,3), axes = FALSE, asp = 1) # color in the nine points(rainworlds, pch=19, col=&quot;lightblue&quot;) Figure 8.1: Rain (filled, blue dots) in 9 out of 30 possible worlds. It does not rain (hollow circles) in the other worlds. Using this ensemble of possible worlds provides us with a sense-making device for probabilistic statements. Ultimately, it either will or will not rain tomorrow. You can also think of this observation as sampling from the ensemble of possible worlds. As though we put them all of these worlds into a hat and drew one of them. The probability of an event is thus thought of as the frequency with which it occurs. Degree of belief There is another way to think about 30% as a probability. Suppose a meteorologist said to you, I’m 30% sure it is going to rain tomorrow. And you say back, “Oh, you mean that, say there are really 1000 alternate universes out there, that in roughly 300 of them, it will rain tomorrow?” And the meteorologist says, “I have no idea what you’re talking about. There is only one universe, and I’m not totally sure what will happen tomorrow, but I put the chances of rain at 30% [walks away slowly towards the door].” For your meteorologist friend, let’s call them Mel, 30% may represent a degree of belief. Importantly, the degree of belief is subjective. Here it is attributed to a meteorologist, which might make you take it more seriously than if your Uncle Bob said the same thing (unless Uncle Bob is actually a meteorologist). Anyway, degree of belief is subjective. Which doesn’t mean it is arbitrary or just a matter of opinion. When it comes to forecasts, some people or some forecasting models are going to be right more often than others. This idea of “being right more often” helps us connect the degree of belief way of thinking about probability to the ensemble sampling idea of probability. Sure, tomorrow only one universe will be ours to observe. It will either rain or not. If it rains, will you say that Mel the meteorologist did a good job or a bad job? What if it doesn’t rain? It’s going to be hard to say based on a single observation! But a few days or weeks from now, Mel (the meteorologist) will come along again and say there is a 30% chance of rain tomorrow. And again. And again some time later. What we could do is collect all of the times that Mel gave 30% as their chance of rain and compare the actual occurrence of rain the next day. Suppose that we have 88 such cases to examine, and that it rained in 30 of them. Thats 30/88 or 34% of the time. While not exactly 30%, that still seems pretty good for something as complex as the weather! Now maybe, maaaaybe, we could wonder if Mel is in fact under-predicting the chance of rain. Then we could add a bit to their forecasts when deciding what to do about it. I would feel more confident about that if we had a larger sample size. This is because we know that the observed proportions in a stochastic process will only converge to “true” proportions when sample sizes get large. We will come back to this in the module about money. To recap, out of 88 times that Mel gave a 30% chance of rain tomorrow, it rained the next day 30 times and didn’t rain the next day 58 times. You can see why it’s hard to judge the meteorologist based on a single observation, even though we are often personally annoyed when the event (rain/no-rain) does not coincide with the choice we made about whether to wear galoshes. Decisions Aside from subjectivity, which is a thorny topic among statisticians, there is really no practical difference between the interpretation of 30% probability as a frequency of occurrence in an ensemble of possible worlds or as a degree of belief about this world. It won’t change what you do about it. If you take this forecast of rain seriously, you have decisions to make. It could be whether or not to take an umbrella with you when you leave the house tomorrow, or whether to cancel your plans to have a barbecue outside. These decisions may not seem very high stakes. The worst case scenario is that you (and others at your barbecue) get wet. But other decisions you have to make on a daily basis can have more serious consequences for your health or even your life. You often have to make those decisions based on probabilistic and maybe subjective information. Death End of warm-up. It’s time to talk about when you will die. I highly recommend this data visualization called Years You Have Left to Live, Probably. Here is a screenshot, although it’s not nearly as interesting when you can’t interact with the simulation and watch the little balls drop. Figure 8.2: Screenshot of interactive data visualization This visualization does a number of things. The most salient feature is probably the dropping balls. Each one represents a possible future outcome. This is exactly like an ensemble of alternate universes. As you watch the balls drop, you think to yourself, “ah, nice, I lived to be 92” and then moments later, “ooh, harsh! I died at 39!” As the simulation runs, it also accumulates data in bins at the bottom, labeled “0 to 9,” “10 to 19,” and so on. (Recall the discussion of bins, frequency tables, and histograms in Chapter 4.) Note that these bins represent ranges of years-you-have-left-to-live, not age-at-death. This may be confusing, because age-at-death is what is shown along the horizonatal, or x-axis, of the figure. Also, right below the x-axis, and corresponding to age-at-death is a set of gray bars that grow as the balls drop. In the screenshot, the simulation has been running for a little while, so that the following counts have been accumulated. bin counts 0 to 9 1 10 to 19 1 20 to 29 3 30 to 39 3 40 to 49 13 50 or more 72 Notice that by the time this screenshot was taken, 93 balls had dropped. The visualization took the counts, converted them into proportions of total counts (e.g., 72/93 = 0.774; 3/93 = 0.33), and represented each of these proportions as a probability, expressed as a percent (e.g., 77%; 3%). Another thing that you will notice if you play around a bit is that as the balls drop, the probabilities change. In the beginning, when the number of samples (balls dropped) is small, the numbers change rapidly and sometimes by a large amount. However, after a couple of hundred samples, the changes are much smaller. By watching the balls drop on this simulation (which I, for one, find mesmerizing), you may actually be meditating on some profound ideas in statistics. Every time you restart the simulation, you begin the sampling process. Each sample is a draw from some distribution of possible life outcomes. Your future life bounces around in this distribution from sample to sample. And in the beginning, when you have only collected a small number of samples, the distribution itself seems unstable. For example, if you put in 24 as the current age and start the simulation in slow mode, the estimated probability of living 40-49 more years fluctuates a lot. However, as you accumulate samples, the shape of the distribution literally comes into view as a pattern among the gray bars just below the x-axis. As the sample size increases, the probabilities becomes more stable. Eventually, if you let it run long enough, you end up with the same values, regardless of how things started out. (This increasing stability at large samples is why I was hesitant to judge Mel the meteorologist until I had more data.) Although we are now talking about probabilities about your remaining years left to live, the interpretation of probabilities is similar to that in our discussion of rain predictions. In the case of rain, there were only two possibilities, rain or no-rain. (A dichotomy!) In the death simulation, there are six bins, each of which represents a range of years. In the case of rain, we understood the meaning of a 30% chance (i.e., probability) of rain by imagining a large number of possible worlds, where it rains in 30% of them. Or one universe where there are a lot of opportunities to make forecasts and check the restults of them. Thus the probability was associated directly with a frequency of something occurring. This is known is as the frequentist interpretation of probability. In the case of death, we say you have a 77% chance of living 50+ more years if, in a large number of possible worlds, you live 50+ more years in 77% of them. You probably realize that we don’t get to see all of these alternate universes, even though we can imagine them. Therefore our probability estimates in many cases are based on things that we have observed happen to other people. For example, among 100,000 people that we do observe from the moment of birth, suppose 78% of them lived into or past their 70s. We convert that observed frequency into a probability for you. You could say that we treat the other people we observed as alternate-universe versions of you. "],["conditional-death.html", "Chapter 9 Conditional Death", " Chapter 9 Conditional Death How does the death (simulation) work? The Flowing Data animated visualization is based on data collected in “life tables,” which can be found online from sources like the National Center for Health Statistics (NCHS) and the Social Security Administration (SSA). Different life tables are produced every year, as life expectancy continues to evolve along with changes in health science and nutrition. Figure 9.1 plots data for age-at-death (for Americans) as of 2010. There is a bar for each age from 0 to 120, and the height of each bar represents a count of deaths at that age per 100,000 people. Figure 9.1: How long Americans were living in 2010 If you’re like me, the first thing you notice in Figure 9.1 is that little spike at age 0, like a rattle sticking up at the end of a rattle snake’s tail. It shows us that roughly 5 out of 1000 babies don’t make it to their first birthday. After that, your odds get considerably better for a while. Another feature that you may detect is that the distribution of age-at-death is not symmetric. It has a long tail to the left. Distributions like this are also called left-skewed. So how does age-at-death relate exactly to the years you have left to live? Life tables are a bit of a strange thing. First of all, they are not tables of “raw data” for a sample of 100,000 people. Rather, they represent a summary of data from many more deaths. According to the SSA source, “the life table represents a hypothetical cohort of 100,000 persons born at the same instant who experience the rate of mortality represented by qx, the probability that a person age x will die within one year, for each age x throughout their lives.” Most of us don’t think about our lives in terms of questions like, are we going to die this year? But that is technically how the life table works. The life table is a set of numbers—including deaths-at-age-x and expected-years-left-to-live-at-age-x—that are all derived from one initial set of numbers which represent the probability that a person age x will die within one year. If you’re curious what that initial set of numbers looks like, I’ve plotted them in Figure 9.2. Figure 9.2: Mortality rate per year of age Looking at Figure 9.2, you can say that the probability of dying within one year gets higher as you grow older, which comes as a surprise to no one. If you’re under 65, say, that probability doesn’t even feel that high. It’s less than 0.01 or 1%. The probability that you will die this year only passes 50% after age 100. That’s reassuring, right? Well, don’t get too optimistic. Your chances of dying every year may be small, but every year is another draw from this morbid lottery. If your chances of dying were 1 out of 2000, then in 2000 universes, you died in one of them. In the other 1999, you live on to another year, but then you have to press your luck again. This happens every year, and the chances slowly get worse. But what if you wanted to know your chances, at birth, of dying in your 60s, that is between 60-69. For now, we will try to answer this question using only the life table and assuming that we know nothing else about you. The rows of the life table corresponding to this age range are these Table 9.1: Life Table Age qx lx dx L Tx ex 60-61 0.008732 88745.98 774.97 88358.50 2051875 23.1 61-62 0.009335 87971.02 821.18 87560.42 1963516 22.3 62-63 0.009983 87149.84 870.00 86714.84 1875956 21.5 63-64 0.010715 86279.84 924.46 85817.61 1789241 20.7 64-65 0.011568 85355.38 987.39 84861.68 1703423 20.0 65-66 0.012586 84367.98 1061.84 83837.06 1618562 19.2 66-67 0.013763 83306.15 1146.57 82732.86 1534724 18.4 67-68 0.015057 82159.58 1237.07 81541.05 1451992 17.7 68-69 0.016380 80922.51 1325.52 80259.75 1370451 16.9 69-70 0.017756 79596.98 1413.34 78890.31 1290191 16.2 This is a lot of numbers. Recall that each qx is the mortality rate for age x, the probability of dying within one year of age x. So should you add up the qx-values for each age in the interval 60 to 69? Maybe pause here to think about this question for a moment before reading on. Here is a partial answer. You can die at 62 and you can die at 64, but you can’t die at both ages. In that sense, it was okay to add the probabilities of these events because they are disjoint, i.e., they can’t both happen and you are interested in whether any one of them does happen. However, if you add up these probabilities, you will still over-estimate the probability for a different reason. Can you guess what you’ve left out? Here is the rest of the answer. You’ve left out the fact that these probabilities assume that you have already made it to 60, and there’s a chance (at birth) that you won’t. To answer the original question, you want to add up the following probabilities: (Probability of making it to 60 and then dying at 60) + (Probability of making it to 61 and then dying at 61) + ... + (Probability of making it to 69 and then dying at 69) + How do you figure out the probability of making it to 60 without dying? It sounds a little bit like a riddle whose answer is “one year at a time.” Indeed, to make it to 60 without dying, you need to not die every year for the first 59 years of your life. Note that, while death can occur in only one year of your life, to survive into your sixties you need ALL of the following to be true: NOT dying at 0 AND NOT dying at 1 AND … NOT dying at 59. The probability of each event (not dying in each year) is independent, and the probability that all of them happen is the product of the individual probabilities. Probability of NOT dying at 0 * Probability of NOT dying at 1 having made it to 1 * ... * Probability of NOT dying at 59 having made it to 59 Since in any given year, you either die or don’t die, these two probabilities must add up to 1, so having gotten to any age x, the probability of surviving it is (1-qx). Now we can take the product of (that is, multiply) all of the survival probabilities (1 - qx) for each x from 0 up to age 59. (I will include the code here. The data table I have loaded from the National Center for Health Statistics is called “lifetableNCHS”). prod(1-lifetableNCHS[1:60,&quot;qx&quot;]) ## [1] 0.887458 You may notice that this probability had already been calculated for you in the life table, but it had been presented slightly differently as column lx, which is the number of persons (in a cohort of 100,000) surviving to exact age x. If we multiply our rate by 100000, we get 88745.8, which (up to a rounding error) is the same as the number in Table 9.1. Okay, so now we are ready to complete the probability calculation. Recall we wanted to add up ten things: Probability of making it to 60 and then dying at 60, etc. We know that the probability of making it to age x is the same as the value of column lx in the table divided by 100,000. And the probability of dying is qx. So we need to multiply these two numbers in each row and add them up. The result is 0.1056. An American child born in 2010 has a 10.5% chance of dying in their 60s (and a 20.7% chance of dying in their 70s). So, we’ve figured out how to do that. And we’re almost ready to move on, but it is worth noticing something. The product of the value qx and lx in each row of the life table is the value dx, which is the number of deaths at age x (or between x and x+1). So when we multiplied and added before, we were really just adding up the number of deaths (dx) at ages 60-69 and dividing by 100,000. Now hopefully that makes sense to you that this should give us the answer we were originally looking for, namely what are the chances, at birth, of dying in your 60s. We could have looked at our hypothetical cohort of 100,000 people all born at the same time and asked: how many of them will die in their 60s. Well, that would be the sum of the dx-values, namely 10562. It wouldn’t be a probability, though, unless we divided it by the total number of people (100,000). So we’ve shown that we can answer our particular question two different ways: Computing the total probability of your making it to 60 and then dying at 60 or making it to 61 and dying at 61 or making it to 62 and dying at 62 etc. up to age 69. or Computing the overall proportion, out of 100,000 people, who die in their 60s. A = B in this case. An important property of mathematical sciences is that you can arrive at the same answer in different ways. Maybe that sounds like a waste of time, but I view it as one of the most reassuring things about math. If you try something two different ways, and you do not get the same answer even though you should, then something is probably wrong with the way you are thinking about it. "],["somefacts.html", "Chapter 10 Some facts about Probabilities Conditional Probabilities Conditional death, again", " Chapter 10 Some facts about Probabilities In this course, I have taken the strong position that ideas should be driven by questions. So I’ve tried to reason through the example above before setting up any foundations on the basic rules of probability. A standard introduction to these topics can be found in many books, for example OpenIntro Stats, Chapter 3. Now is probably a good time to recap some of what we have established about probabilities. We will also introduce the most basic notation P(A) for the probability that event A happens. For example, event A can stand for “you die at age 64” or “it rains in New York tomorrow.” When possibilities are disjoint, or mutually exclusive, the probability that either one of them happens is the sum of the individual probabilities P(A or B) = P(A) + P(B) An example of this was dying at age 62 or dying at age 64. A special case of this addition rule applies when one or the other MUST happen. For example, in logic, either something happens or it doesn’t happen. Either A or NOT A. Since these possibilities are disjoint: P(A) + P(not A) = 1 P(not A) = 1 - P(A) An example of this was the probability that you do not die at age 0. We found it by subtracting out the probability that you will die from 1. The last fact we used is The joint probability rule for independent events that BOTH occur is the product of the individual probabilities of each event occurring. P(A and B) = P(A) * P(B) We used that to figure out how you survive by not dying every year. Notice that I’ve snuck in the word independent (well, I snuck it in boldy, so it wasn’t that sneaky). There is an intuitive reason why it is important to make a distinction about independent events. In the last module, we said that two events (we were talking about responses to questions) are independent if knowing about one of them does not give you any information about what the other one might be. But remember bizarro world where the toilet paper orientation and peanut butter preference were deterministically related, and specifically everyone is either under-chunky or over-smooth? I’ve reproduced this result in Table 10.1. If I told you that 53% of the total population prefers smooth, then what proportion of the total population prefers smooth AND likes to over-hang? Also 53%. What proportion prefers smooth AND under-hangs? 0! Table 10.1: Bizarro world chunky smooth over 0 23 under 17 0 In bizarro world, toilet paper orientation and peanut butter preference are NOT independent, because knowing one of them DOES give you information about the other. P(tp = over AND pb = smooth) does NOT equal to P(tp = over) * P(pb = smooth) This will become even more clear in the next section. Conditional Probabilities Recall that we would NOT have gotten the right answer to the probability of dying in your 60s if we added up the mortality rates qx for all ages x in [60-69]. (Exercise: verify this.) Rather, we had to multiply these numbers first by the probability of living to age x. Another way to say this is that the mortality rate qx was actually a conditional probability. It was the probability of dying at age x on condition that you have survived to age x. To be absolutely clear, we are measuring x in whole numbers, like birthdays, but we don’t mean dying on your xth birthday. Rather, we mean dying anytime between turning age x and turning x+1. We need a special notation to distinguish conditional probabilities. We write, qx = P(You die at age x | You survived to age x) and we read this as “qx is the probability that you die at age x given that you survived to age x” or as “qx is the probability that you die at age x conditional on your surviving to age x.” These are equivalent, but they differ from P(You die at age x) which is the unconditional probability that you die at age x. This is also different from P(You die at age x AND You survived to age x) which is called the joint probablity of the two events. (Note that the joint probability of events A and B is also often written as P(A, B). The comma functions like the word “and.”) We calculated exactly this joint probability above when we wanted to add up the probabilities that you die at some point in your 60s. The way we computed the joint probability for each year was by application of this general rule for conditional probabilities P(A and B) = P(A|B) P(B) which we read as “the probability of both A and B happening is equal to the probability of A conditional on B multiplied by the probability of B.” Note that this rule always holds. That’s because what I’ve called the general rule is equivalently just the definition of conditional probability. For example, I could have written it this way: P(A|B) = P(A and B) / P(B) This is just a rearrangement of the formula, but we have a tendency of seeing whatever is on the left side of an equation as being defined by what is on the right. As far as death is concerned, the following are all true: P(die at x AND survived to x) = P(die at x | survived to x) * P(survived to x) P(die at x AND survived to x) = qx * P(survived to x) qx = P(die at x AND survived to x) / P(survived to x) where in the second line I substituted the mortality rate qx for the conditional probability that defines it. In the last line, you can see how the mortality rate could be estimated from data if you actually observed a whole bunch of people. You would count how many of the die at age, say, 62, and divide that number by the number who survived to age 62. You can also probably see why the following is true: P(survived to x | die at x) = 1 That is, if you died at 62 then you must have survived to that age. That may seem too obvious for words, but it helps to show clearly that for conditional probabilities, it is not generally true that P(A|B) = P(B|A). Considering toilet paper in bizarro world, we can see explicitly why the rule for joint probabilities of independent events P(A and B) = P(A) * P(B) did not hold. The conditional probability relationship always holds, but independence is a special case. We can see what it is now: P(A and B) = P(A|B) P(B) = {only in special cases} = P(A) * P(B) Thus, when A and B are independent, it must be true that P(A|B) = P(A) # for independent events which reads as “the probability of A conditional on B is equal to the probability of A (regardless of B).” Another way to say this is that no matter what we know about B, it doesn’t tell us anything informative about A. In the real world, this is true about toilet paper and peanut butter. P(tp = over | pb = smooth) = P(tp = over) ## real world But that was NOT true in bizarro world, where knowing peanut butter preference told us EVERYTHING about toilet paper orientation. If A is the probability that a person is an over-hanger, and B is the probability that they prefer smooth peanut butter, then P(tp = over | pb = smooth) = 1 ## bizarro world P(tp = over | pb = chunky) = 0 However, regardless of whether we stay in the real world or in bizarro world, the following will always be true, P(tp = over AND pb = smooth) = P(tp = over | pb = smooth) * P(pb = smooth), because this is the definition of conditional probability. Now you might notice that if the above is true, then the following should also be true: P(pb = smooth AND tp = over) = P(pb = smooth | tp = over) * P(tp = over). And it is true. For the same reason that this follows from the definition of conditional probability. The order of the two joint events on the left-hand-side does not matter. There is no difference between P(A and B) and P(B and A). If both A and B happen, they both happen. There is no implied chronology when we write “A and B” that A came before B. So, that said, it is also true that P(tp = over | pb = smooth) * P(pb = smooth) = P(pb = smooth | tp = over) * P(tp = over). By combining the two equations from above. Or, in general P(A | B) * P(B) = P(B | A) * P(A) Notice that is not generally true about conditional probabilities that P(A|B) = P(B|A). Even though it is always true about joint probabilities that P(A,B) = P(B,A) (I used commas instead of “and” here.) Here’s an example. All NYU students are New Yorkers (at least, honorary New Yorkers while in town.) But not all New Yorkers are NYU students. The probability of being a New Yorker conditional on (or given that) you are currently an NYU student is 1. However the probability of being an NYU student conditional on being a New Yorker is clearly not 1. Conditional death, again Earlier I said we would use the life table to answer questions about when you will die assuming nothing else about you. Now, you might be aware that life expectancy is not the same for males and females. Indeed, there are separate life tables for each sex. I’ve plotted the death column dx from both tables in Figure 10.1. Females are shown in light blue bars, and males using orange. Unfortunately for the males, their mortality rate is higher not only in their later years, but even in their late teens and twenties. (We’ll come back to that when we consider how you will die.) Figure 10.1: Deaths by age for male and female (2010) Suppose I was interested purely in the likelihood (at birth) of living into ones 90s or beyond, conditional on sex. I can find out the proportions from separate life tables for each sex as follows. (These single-sex life tables go from 0 to 119, and the first row is dying before your 1st birthday. I need to look at the 91st row to see death after age 90.): #proportion of females dying from at ages 80-119 sum(lifetableFemale[91:120,&quot;dx&quot;])/sum(lifetableFemale[,&quot;dx&quot;]) ## [1] 0.2446278 #proportion of males dying from at ages 80-119 sum(lifetableMale[91:120,&quot;dx&quot;])/sum(lifetableMale[,&quot;dx&quot;]) ## [1] 0.1347719 I can make a two way table using these proportions. I will base my table on a sample of 1000 females and 1000 males. This is not real sample data. I am just using the life table proportions here to construct an idealized sample. sex_age80 &lt;- data.frame(dieBefore90 = c(755,865), livePast90 = c(245,135), row.names=c(&quot;Females&quot;,&quot;Males&quot;)) The table looks like this Table 10.2: Will you live into your 90s? dieBefore90 livePast90 Females 755 245 Males 865 135 The unconditional probability of living past 90 is (245+135)/2000 = 0.19 or 19%. This would be your betting chances if a baby were born and you did not know its sex. We can write this as P(live past 90) = 0.19. But if you knew it was born female, then you need to compute P(live past 90 | sex = female). How would you do this? Well, you can use the table. Of the 1000 females, 245 live past 90, so the answer is 24.5%. For males, it is 13.5%. Sorry, males. Just to make things weird Now suppose, for arguments’ sake, that the numbers in the life table apply to everyone born in the last 120 years as of today (they don’t; life expectancy has changed over the years). If I told you that someone was over 90, but nothing else about them, what is the probability that the person was female at birth (we will make the simplifying assumption that the life table corresponds to sex at birth)? The two-way table I constructed had equal numbers of males and females. We will assume that the birth rates are indeed the same. So the unconditional probability of being born female knowing nothing about a person’s status as living or dead is 50%. However, among those alive in their 90s, 245/(245+135) = 0.64 of them are female. Almost 2 to 1. Given a two-way table, with variables representing events A and B, it is possible to derive conditional probabilities in both directions. P(A|B) might be the probability of living past 90 given sex at birth. P(B|A) would then be probability of sex at birth given present age over 90. In the next chapters, we will start to examine what kinds of things might kill you. This will give us another chance to look at association in two way tables (as distinguished from independence). We will also explore what it means to say that something causes early death. In the example above, we saw that sex is associated with early death. Females live longer; males die younger. Would we say that sex causes early death? "],["how-you-will-die-causes-or-conditions.html", "Chapter 11 How You Will Die: Causes or Conditions? Causes of death, colloquially Hold my beer Non-deterministic Causation Testing for an association between two variables", " Chapter 11 How You Will Die: Causes or Conditions? Causes of death, colloquially We now pause our inquiry into when you will die and concentrate for some time on how it might happen. Let us pay another visit to Nathan Yau’s series of (interactive) visualizations for Flowing Data. We already discussed Years You Have Left to Live, Probably. Another of them is called Causes of Death and the last How you will die (links work if you download the PDF but do not work within Perusall). These visualizations are really a treasure trove. So much going on. I’ve reproduced an image from Causes of Death below, but you really should interact with it on the web. Figure 11.1: Screenshot of Causes of Death visualization Take a close look at the image in Figure 11.1. How many variables are represented here? (Pause and think this over.) There are fifteen color bands representing causes of death, but these are not each variables. Rather, they are categories of a single variable: cause-of-death, as operationalized by the Centers for Disease Control and Prevention.7 These include cancer, congenital defects, and external causes (e.g., pianos falling on your head), among others. Along the bottom of the image we see age, which is another variable. Indeed, the most salient aspect of this visualization is how much the relative importance of different causes of death changes over the course of one’s life span. If you die in your 20s, it is likely that you died from external causes. While if you died in your 80s, it is more likely because of a circulatory or respiratory disease. This intuitive sense-making is an example of conditional probabilities. It may not be a two-way table, but Figure 11.1 is in many ways analogous to a two-way table. The two variables are age-at-death and cause-of-death. Although we are not putting numbers to the probabilities we can read this image as indicating that P(cause = external | age in 20s) &gt; P(cause = circ or resp | age in 20s) (read, the probability that cause of death is external given that age of death is in ones 20s…). While the opposite is true in your 80s: P(cause = external | age in 80s) &lt; P(cause = circ or resp | age in 80s) There is an important difference between this particular figure and a two-way table of counts. This figure shows relative proportions of death for each age, not absolute numbers. As we know, and even visualized explicitly in Chapter 9, the count of deaths is much higher among adults in their 70s and 80s than among those in their 20s. Figure 11.1 does not communicate that. Instead, by ignoring the overall scale, the figure allows us to focus on the shifting importance of different causes. A two dimensional figure is used here to represent relationships between two variables (age-at-death and cause-of-death), but there are more than two variables here. The additional variables require using the tabs at the top. Sex is here, as well as race. Note that although sex and race categories are all switched between by using tabs, they are not, of course, categories belonging to a single variable. This is something that I slightly dislike about this visualization. The way it is designed, it feels as though one cannot observe, or condition on, sex and race at the same time. So there are four variables (did you get it right?). We can, in principle, imagine conditioning on any or all of them. For example, we could ask what is the probability of death by endocrine disease for (i.e., given, conditioned on) an African-American male aged 65-70. We might write something like this in probability notation as, P(cause = endocrine | race = African-American, sex= male, age = 65-70) = ? Yau also draws your attention to another feature of this visualization: “When you select races, you might notice that the smaller groups, American Indian and Asian, appear to be more jagged, whereas cause of death for the larger groups, black and white, appear to be smoother. This is likely due to population size more than anything else. It’s a smaller sample, and there’s higher variance.” The sample size factors in here, because CDC data are being used to estimate a proportion. For example, what proportion of 65-year-old deaths are the result of circulatory disease. Suppose we knew the true proportion in the total population. If we took only a sample from this population, we will not always find exactly the same proportion. For small samples, the proportion in each sample will appear to vary a lot. While for large samples, the proportion will vary but in a smaller range. We will revisit this idea again when we discuss making bets. Yau’s final visualization in the trio, How You Will Die, combines elements of causes-of-death and years-you-have-left-to-live. The visualization is dynamic, and the passing of time is like a sped up version of your life. In Figures 11.2-11.4, I have reproduced still snapshots taken at three different “ages,” starting with the simulation settings: Asian female, age 21. Figure 11.2: Screenshot 1 of How You Will Die visualization Figure 11.3: Screenshot 2 of How You Will Die visualization Figure 11.4: Screenshot 3 of How You Will Die visualization In this visualization, you can tell that death becomes more probable (and eventually certain) as you get older. The dots fill up slowly (except at the very beginning, when early childhood disease can play a role) and then more quickly. The colors represent causes. In the two-dimensional plane of dots, we get a sense of the randomness of the draw. But we also perceive the relative importance of causes by the areas taken up by purple and red. These are collected into a bar plot on the right side. At the base of each bar, the proportion is reported as a percentage of all deaths by this age. Hold my beer So far, we have conditioned on things that you can’t change, like your age, race, or sex-at-birth. But what if I tell you that I’m about to do something really stupid risky. Like suppose I told you I was going to eat a bunch of Pop Rocks and drink a lot of Coke, which we all know killed Little Mikey, the child star from the Life cereal commercials?8 If I am about to engage in highly risky behavior, you might not want to put the probability that I die this year at the 1% or 2% base rate from the life tables. You might think I have a 37% chance of dying from an exploded stomach. Which, by the way, is an external cause. So you might think that the conditional probability of death by external causes should go up, but not the probability of death by respiratory disease. When there is something that you can change, or manipulate, such as engaging in a particular behavior (e.g., driving, rock climbing, or eating Pop Rocks and Coke), then we can imagine what might have happened if you didn’t do something that you did (or did something that in reality you didn’t do). This is what is known as a counterfactual claim, such as “if I didn’t check my email this morning before leaving the house, I would not have missed the train.” Maybe. But how can we know? Our knowledge about the world is based on a combination of observations. (I am distinguishing knowledge from belief, but epistemology is a big topic and beyond our scope at the moment!) Some of those observations are of the form where we do not deliberately manipulate conditions in the world, such as treatments, behaviors, or environmental factors. We just observe the joint occurrences of events in samples, such as health outcomes, behaviors, pollution levels, etc. Analysis of these kinds of data are called observational studies. We can also do experiments, sometimes, where we gather a sample of people and assign half of them to do something (e.g., take medicine, or sleep in a cabin) and half of them to do something else (e.g., take a placebo, or watch an informational video). These are experimental studies. It is often said that experimental studies are the best way to develop knowledge about causation, that is, answers to questions of the form, does A cause B? Does eating meat cause heart disease? Does smoking cause lung cancer? And are experiments the only way to answer such questions? What does it mean to say A causes B? The philosopher David Hume was one of the first to shed light on the question of how we conceive of causation. In the past twenty years, statistical thinking about causality has also changed a lot. In this course, we are going to take a pragmatic approach and focus on how we use the concept of causation in everyday life. It will be helpful to review our distinction between deterministic and stochastic processes and between associated and independent events. Non-deterministic Causation If I hit a porcelain tea cup hard with hammer and the tea cup breaks, we can safely say that hitting the teacup with a hammer caused the cup to break. We don’t really feel the need to say that if you hit a teacup hard with a hammer, there is a 99.9997% chance that it will break. Even if that’s actually true. And we don’t feel the need to define “hard” in this case either. We use an example like a teacup and hammer when we want to focus on the common-sense big picture and not the details. And the big picture here says that hitting a teacup with a hammer deterministically causes the teacup to break. Let us also assert that if we do not hit the teacup, and it just sits there, then it will not spontaneously break. In the case of the physics of hammers and teacups, we feel that we know this much is true. What about buying a lottery ticket? Does buying a lottery ticket cause one to win the lottery? Well, you certainly are not guaranteed to win the lottery if you buy a ticket. (In fact, your chances will be very low. The subject of making money is the next Big Question). But you can’t possibly win if you don’t buy a ticket. So, strictly speaking, buying a ticket does influence the probability of winning. We’ve now discussed two examples. In the first case (hammer and teacup): If A (hammer hits teacup) then definitely B (teacup breaks) If not A (hammer does not hit teacup) then definitely not B (teacup does not break) In table form: Teacup breaks Teacup doesn’t break Hammer hits teacup Always* Never Hammer does not hit teacup Never Always *pretty much; we’re not splitting hairs here. In the second case (lottery ticket): If A (buy lottery ticket) then maybe B (win lottery) and maybe not B (do not win lottery) If not A (do not buy lottery ticket) then definitely not B (do not win lottery) Win lottery Do not win lottery Buy lottery ticket Rarely Probably Do not buy ticket Never Always What about this question: does smoking cause cancer? Does it fit either of these two cases? Unfortunately the question about smoking does not. It belongs to a yet another case. In the third case (smoking): If A (smoke) then maybe B (cancer) and maybe not B (no cancer) If not A (do not smoke) then maybe B (cancer) and maybe not B (no cancer) Get cancer Do not get cancer Smoke Maybe Maybe Do not smoke Maybe Maybe Now I’m not saying that the chances of cancer are the same whether you smoke or not. That remains an open question so far as our present argument goes. But even thus far, we can see that the smoking causality question, posed this way, invites some more questions. How big a difference does there have to be between the cancer rates for smokers and non-smokers for us to be convinced that there is an association between smoking and cancer? And if there is an association between smoking and cancer, what would drive us to call this a causal relationship, to say that smoking causes cancer? Could there be a third variable? Could causality go the other way? At minimum, we may say that there is no causation without association. If two events are independent (that is, they co-occur with frequencies that are consistent with their being independent events), then it does not make sense to say that one causes the other. Testing for an association between two variables Let’s focus on the first question: How big a difference does there have to be between the cancer rates for smokers and non-smokers for us to be convinced that there is an association between smoking and cancer? Our approach to answering this question will be very similar to the one we used in Chapter 3 where we were considering associations between two-types-of-people questions. The method for establishing association is quite general. Caveat: we are going to vastly oversimplify the problem here and use made up numbers. Smoking is not a dichotomous variable. People can smoke more or less heavily and for different durations. They may have already quit or be active smokers. For this worked example, we will imagine that we are sampling people between the ages of 50 and 75. We will operationalize smokers as heavy smokers: people who smoked the equivalent of at least 10 cigarettes a day for at least 20 years. Suppose that we are able to obtain a randomly selected sample of 1000 people in this age group. For each one, the following information is available: a) whether the person is/was a heavy smoker and b) whether the person has ever been diagnosed with cancer. The beginning of our dataset might look something like this: Cancer? Smoke? Person 1 No No Person 2 No Yes Person 3 Yes No Person 4 No No Person 5 No Yes Person 6 Yes Yes As a first step, you tabulate the data and get the following contingency table: Cancer: Yes Cancer: No Smoke: Yes 46 204 Smoke: No 93 657 Then, you use the table to estimate the following: \\[P(\\text{Cancer}|\\text{Smoke})=\\frac{46}{46+204}=0.184\\] \\[P(\\text{Cancer}|\\text{Not Smoke})=\\frac{93}{93+657}=0.124\\] You might say that these numbers suggest an association (i.e., dependence) between smoking and cancer: Within this sample, a higher proportion of smokers were diagnosed with cancer than non-smokers. But is this enough of a difference to convince you that, if you went out and found 1000 new (random) people, you would still observe a difference of this magnitude? One way to start trying to answer this question is to consider the following thought experiment: imagine that, among all people in the world, there is NOT a higher incidence of cancer among smokers (as compared to non-smokers). If that were the case, you would expect to see \\[P(\\text{Cancer}|\\text{Smoke})=P(\\text{Cancer}|\\text{Not Smoke}).\\] Or, written slightly differently: \\[P(\\text{Cancer}|\\text{Smoke})-P(\\text{Cancer}|\\text{Not Smoke})=0.\\] In comparison, you observed the following in your sample: \\[P(\\text{Cancer}|\\text{Smoke})-P(\\text{Cancer}|\\text{Not Smoke})=0.184-0.124=0.06.\\] So, you could pose the following question: what is the probability that, among the whole population, smokers do not have higher risk of cancer; but, among the random sample of 1000 people that you observed, there is a 6% (or greater) increased incidence of cancer among smokers as compared to non-smokers? This type of question is the basis for hypothesis testing. Often, in hypothesis testing, we form a null hypothesis (in this case, the null hypothesis might be that smokers and non-smokers have equal cancer incidence among the full population) and alternative hypothesis (in this case, the alternative hypothesis might be that smokers have at least 6% higher risk of cancer than non-smokers). Based on the sample you observed, you could estimate that approximately \\(\\frac{46+204}{1000}*100=25\\) percent of the population smokes and approximately \\(\\frac{46+93}{1000}*100=13.9\\) percent of the population has been diagnosed with cancer. If there is no real difference in cancer incidence among smokers and non-smokers, then these two variables are independent: as though 25% of your sample randomly decided to smoke, and 13.9% were randomly diagnosed with cancer. It is easy to simulate datasets under this assumption. In two completely separate (independent) steps, we will randomly assign to each of our 1000 people, a 25% chance of being a heavy smoker and a 13.9% chance of getting cancer. In Chapter 3, we imagined starting with a fixed number of responses (e.g., smoker/non-smoker), shuffling those responses, and then redistributing the values among our virtual subjects. The code below actually does something different. It will draw values with replacement for each of the two variables. This means that in each replication, we will not actually have exactly 250 smokers and 139 cancers. The numbers will fluctuate somewhat in each simulated dataset. After we generate our simulated datasets, we can calculate \\(P(\\text{Cancer}|\\text{Smoke})-P(\\text{Cancer}|\\text{Not Smoke})\\) and observe what range of values occurs. We will be particularly interested in the proportion of the time that this difference is greater than or equal to \\(0.06\\). That was our observed value. And in this simulation, we know that the two variables are independent. Thus, we are quantifying the chance of observing such a difference in outcomes due to random chance. set.seed(5123) #set some number of iterations/replications nIter = 100 #create vector to save differences in proportions differences = vector(length = nIter) for(i in 1:nIter){ #repeat the following process nIter times #create some fake data and save it as &quot;fakedata&quot; fakedata = data.frame(Smoke = sample(c(&quot;Y&quot;, &quot;N&quot;), size=1000, prob=c(.25, .75), replace = T), Cancer = sample(c(&quot;Y&quot;, &quot;N&quot;), size=1000, prob=c(.139, .861), replace = T)) #use the fake data to calculate P(cancer|smoke) CgivenS = table(fakedata)[2,2]/sum(table(fakedata)[2,]) #use the fake data to calculate P(cancer|not smoke) CgivenNS = table(fakedata)[1,2]/sum(table(fakedata)[1,]) #save P(cancer|smoke) - P(cancer|not smoke) in the ith location of differences differences[i] &lt;- CgivenS - CgivenNS } #calculate proportion of differences greater than or equal to .06 propGreater &lt;- sum(differences &gt;= .06)/nIter #plot a histogram of the differences with a red vertical line at .06 hist(differences, main=&quot;Histogram of P(cancer|smoke) - P(cancer|not smoke)&quot;, xlim=c(-0.1,0.1), breaks=seq(-0.1,0.1,0.01)) abline(v=.06, lwd=2, col=2) text(0.09, nIter/10, paste0(propGreater*100,&quot;% of cases&quot;)) As you might expect, the histogram of simulated differences (\\(P(\\text{Cancer}|\\text{Smoke}) - P(\\text{Cancer}|\\text{Not Smoke})\\)) is centered around zero. If there’s no real difference, then you should expect to observe (close to) zero differences among any random sample of 1000 people. That said, you’ll see from the histogram that it is still possible, by random chance, to observe a difference as large or greater than 6%. In fact, in our simulation, this happened 3 times (3% of the time). Remember that our simulation was carried out assuming independence, in which case there should be no difference. So we might interpret our findings as follows: there is a relatively low probability of observing \\(P(\\text{Cancer}|\\text{Smoke}) - P(\\text{Cancer}|\\text{Not Smoke})\\ge .06\\) in a sample of 1000 people if it were truly the case that this difference was zero. Operationalization of a variable is a fancy and more specific word for defining it. It doesn’t seem necessary to define “cause of death” in the common-sense use of the word define. But when we decide how many and which categories to include in our use of the variable, we operationalize this definition. For example, we could operationalize cause of death as “natural causes” or “other.” This would be a dichotomous operationalization. And it leaves a lot to the imagination. The Center for Disease Control is particularly interested in diseases and not in external causes. So it makes sense that all external causes are banded together in this figure. However, don’t be fooled. There are many subcategories, including very specific ones like “Pedal cyclist injured in collision with heavy transport vehicle or bus.”↩︎ Okay, yeah, this is an urban legend: https://www.snopes.com/fact-check/pop-rocks-soda/↩︎ "],["will-you-make-money.html", "Will You Make Money? Battle of the Bills", " Will You Make Money? No one can win at roulette unless he steals money from the table while the croupier isn’t looking. — Albert Einstein (possibly) The development of probability theory is historically linked to attempts to understand games of chance, especially ones in which money was involved (see for example, here). Sometimes betting money on an uncertain outcome falls under the name of gambling; other times it’s dignified with the name investment or “smart business decision.” But regardless of the label, there are smarter and less smart ways to play money games. Battle of the Bills Let’s recall a distinction we made earlier in this course about deterministic and stochastic, or random, processes. This time, we’ll think about two different bets you make with your friend. In the first bet, you and your friend are debating whether it was Bill Paxton or Bill Pullman in the movie Apollo 13. To make the game interesting, you bet two dollars. You look it up on the internet, and find that it was indeed Paxton. One of you wins. Do you feel the need to check again? Probably not. This particular question, although you may not have known the answer for sure, has only one possible answer, no matter how many times you check. Now consider another bet, this time for three bucks! You and your friend are walking down the street debating the “merits” of mint chocolate chip vs. cookies and cream as ice cream flavors. You claim that mint chocolate chip is the more popular flavor, and decide to ask the first passer-by which flavor they think is better. Suppose they don’t just ignore you, thinking you’re a nutcase, and they answer cookies and cream. Are you satisfied with this one answer? Or do you feel the need to ask another pedestrian? And how many? We might say that the variable “BPA,” which stands for “which Bill P. starred in Apollo 13?” has a deterministic answer. It is always the same. But the variable “MCCoCAC” which stands for “mint chocolate chip &gt; cookies and cream?” can take on different answer (one of two; no ties allowed) depending on whom we ask. Because it is a random or stochastic variable, we have to talk about it using different terms. We might say something like, what proportion of people (in this neighborhood, say) prefer cookies and cream? Or what are the chances that the first person we ask will express that particular preference. This may all sound like silly bets that are really just games between friends. But people make small and large money bets all the time, in everything from business and life decisions, to recreational games. In this module, we explore probability calculations that inform things like advertising, airplane booking, the job market, and march madness. "],["betting-on-beer-or-ice-cream.html", "Chapter 12 Betting on Beer (or Ice Cream) How do statisticians solve problems like this?", " Chapter 12 Betting on Beer (or Ice Cream) This section makes reference to Chapter 5 of Naked Statistics by Charles Wheelan. In 1981, Schlitz brewing company, now defunct but at one time the largest beer producer in the US, ran a bold advertising campaign. During the Super Bowl, Schlitz ran a live blind taste test against one of its competitors, Michelob. 100 Michelob drinkers participated in the taste test, which aired LIVE. The advertisement slot itself cost a lot of money. Schlitz could have just run a funny ad involving puppies on the beach, so why take a risk with a taste test that could conceivably have gone badly. How could Schlitz have been so confident that their beer would be preferred? THINK ABOUT IT QUESTION: What information would you need to know to advise the Schlitz brewing company about running such an ad? (Take a few minutes before continuing on, to try to list this information on your own). As discussed in Wheelan’s chapter, some things we would need to know are: Actual proportion of Michelob drinkers who would prefer Schlitz in a blind taste test Acceptable outcome of live taste test for promoting Schlitz beer Intended sample size for taste test Rules of mathematical probability Wheelan adds a lot of context to this particular story, which is part of the fun. In particular, he asserts that Schlitz and Michelob are probably indistinguishable to most beer drinkers. This puts the chances of anyone prefering one beer over the other at 50%. He also points out that the marketing campaign works well for a range of outcomes, because the taste test is conducted with Michelob drinkers. Schlitz executives will be quite happy to be able to say that even 40% of Michelob drinkers prefer Schlitz, which sounds (and is) very different from saying that 40% of all beer drinkers prefer Schlitz over Michelob. Wheelan invokes the “law of large numbers” to argue that for a given sample size, and if the actual proportion is 50%, that the results of the live taste test can be almost guaranteed to be satisfactory for Schlitz (we define satisfactory, for now, as at least 40% preferred). The larger the sample size, the greater the probability that the taste test will be a success. We have created a Schlitz simulation for you to explore this for yourself. In his book, Wheelan claims that (a) for 10 blind tast testers, the probability of a happy outcome is 0.83 and (b) for 100 blind taste testers, the probability is 0.98. If you don’t want to take this assertion at face value, you might try convincing yourself by opening the simulation, running 100 simulated experiments of sample size 10 or 100, and inspecting the proportion of those experiments that led to a favorable outcome. You should see values around .83 and .98 for sample sizes of 10 and 100, respectively. For a moment, let’s pull back the curtain on the Schlitz simulation and see how it works. The following code walks through the process of repeatedly surveying 10 people, recording the proportion who preferred Schlitz (under the assumption that each person has a 50% chance of preferring Schlitz), and calculating the proportion of those 10-person surveys that led to an acceptable outcome. If we collect 10,000 samples of 10 people and calculate the proportion of those 10 person samples where at least 4 out of 10 people preferred Schlitz, we can estimate the probability of an acceptable outcome very accurately: numExpts = 10000 #set some number of repeated experiments to run sampleSize = 10 #set the sample size trueProb = .5 #set the (true) probability of preferring Schlitz acceptableOutcome = .4 #set an acceptable proportion of Schlitz preferrers results = vector(length = numExpts) #create a vector of length nIter for(i in 1:numExpts){ #repeat the following process numExpts times #Choose sampleSize-many values from the set (0,1) with replacement #where the probability of drawing a 1 is equal to trueProb #save the results in a vector called drawResults drawResults = sample(c(0,1), size=sampleSize, prob=c(1-trueProb, trueProb), replace=TRUE) #In the ith location of &quot;results&quot;, calculate the proportion of 1s in Samp results[i] = sum(drawResults)/sampleSize # NB: this would also have worked } #Calculate the proportion of random experiments that were &quot;acceptable&quot; sum(results &gt;= acceptableOutcome)/numExpts ## [1] 0.834 Feel free to copy this code over into your own script in RStudio and play with the parameters to see what happens. If you decrease numExpts to 1000 and re-run the simulation a few times, you might see that there is more variation in the estimated probability; however, if you increase numExpts to 100000, you are more likely to observe values very close to .83 every time. That is, the sampling variance is larger for small samples and smaller for large samples. How do statisticians solve problems like this? In this course, I have tried to emphasize conceptual understanding through simulation and discussion. In the example above, you can, for example, run a bunch of simulations of the experiment and (very accurately) estimate the probability of an acceptable outcome. But, you’ll get slightly different answers each time you run the simulation. If this bothers you, read on. Mathematical statistics does have precise answers that depend on properties of continuous distributions like the normal distribution and the binomial distribution. The term data distribution comes from trying to describe how data or observations are distributed across the values that they can attain. For example, are all outcomes equally likely? (A uniform distribution.) Or are values near the “middle” more likely than values farther away? A histogram is one way to visualize a distribution, and we often speak of the “shape of a distribution.” The normal and binomial distributions (which are both bell-shaped, by the way) are idealizations that are realized if we have an infinite amount of data. In reality we only have finite samples. But when we have large enough samples, even though they are not infinite, their data distributions start to look more and more like their idealized versions. The Schlitz commercial is exactly the kind of scenario that is explained using a binomial distribution (more on that later). If we ran the simulation (always with samples of 10), taking more and more observations (i.e., 100 samples of 10, 1000 samples of 10, etc.) and checked our success rate (defined by at least 4/10 preferring Schlitz), we would see that indeed this proportion does converge. This is plotted in Figure 12.1. The x-axis is the number of samples, but note that the x-axis is shown using logarithmic scales. We need to use this scale, or else all of the points at smaller values would be bunched together. (Try this at home: draw an x-axis and label one end 0 and the other end 100,000. Now try placing tick marks at the values 10, 100, 1000, and 10,000.) Figure 12.1: Convergence of successful tasting proportions So we see that there is some convergence: If we run more and more experiments, we find that the proportion of “successful” experiments converges to a stable value. But how can you calculate that value precisely? To find the analytical (i.e., exact) answer, instead of using a simulation to estimate it (empirically), it helps to start with a small sample size, say 2. Now that we’ve reduced our scope, we have some hope of writing all possible outcomes of this experiment and their probabilities. What are all of the possible outcomes of 2 independent taste tests? We are using the word independent here, because we believe that one person’s preference in a blind taste test does not influence the preference expressed by another person. If the same person were asked to do the taste test on two occasions, it might not be appropriate to treat those two results as independent. Unless the beers are truly indistinguishable. In any case, we are assuming that we have two strangers. Aside: To be more precise, we should say that the two strangers’ answers are conditionally independent, because their preference does, in principle, reflect the quality of the beer. Suppose I told you that I had two beers, a horrible one and a delicious one. And that I was going to put them to a blind taste test. I don’t tell you which is labeled beer A and whcih is beer B. I tell you that one person preferred beer A. Now another person comes to take the test. Which do you think they will pick? Probably beer A, because that one is probably the delicious one. So it looks like knowing what one person chose provides information about what the other one will choose. Which seems to violate the definition of independence. We can condition this association away. The answers are not causally associated, but rather indirectly associated by the fact that they are both explained by the “true” taste of the beer. That is our third, or lurking, variable here. I didn’t tell you whether beer A or B was the delicious one. You tried guess which it is, based on the one observation of preference for beer A. However, if I told you that in fact beer B is the delicious one, but one out of twenty people seem to prefer beer A anyway, then this would change things. Knowing that one person came along and picked A would not make you predict that the next person will do the same. You will still, if you take my word for it, predict that the next person (and almost all of the people to come) will choose beer B. The formal way to put all of this together is that conditioning on the known desirability of the beer, each individual taste test is independent of the others. In the example above, we assumed that the chances are 50/50, but they didn’t have to be for the conditional independence of trials to hold. There are two possibilities for each taster (either Schlitz or Michelob), so if we represent each possible outcome as (taster 1’s choice, taster 2’s choice), we get 4 possibilities. {(Michelob, Michelob), (Michelob, Schlitz), (Schlitz, Michelob), (Schlitz, Schlitz)} The possible outcomes of a random experiment form a set, which is called the sample space. As per convention, we use curly brackets to indicate the set. The elements of the set are the outcomes, and we use ordinary parentheses to indicate the ordered pair of (Taster1, Taster2). At this point, you might find it helpful to look back at Chapter 10 to refresh your memory. Because each individual taste test is independent, we already know how to express the (joint) probability of an outcome involving multiple tasters in terms of the individual probabilities. For example, the probability of (Michelob, Schlitz) is: P(Michelob, Schlitz) = P(taster 1 prefers Michelob AND taster 2 prefers Schlitz) = P(taster 1 prefers Michelob) * P(taster 2 prefers Schlitz) If we assume that every taste tester is equally likely to prefer either beer, then P(taster 1 prefers Michelob) = 0.5 = P(taster 2 prefers Schlitz) and so P(Michelob, Schlitz) = (.5)*(.5)=.25. In other words, there is a 25% chance that the outcome of the experiment is that the first taster prefers Michelob and the second taster prefers Schlitz. In fact, assuming a 50% chance of preferring either beer, we’ll get the same probability for any of the four outcomes. Notice that the four outcomes listed above are disjoint (only one can occur) and complete (one of them MUST occur). Therefore, we also knew that their probabilities must add up to 1. And since they are also equiprobable, we could have known that each probability must by 0.25. Now we can ask: which of the four possible outcomes will meet our requirement that at least 40% of respondents prefer Schlitz? Probability textbooks often use the word event to describe some subset of the sample space for a random experiment. An event can be one outcome, a subset of one, but it can also span several outcomes. In this case, we could describe the event that at least 40% of respondents prefer Schlitz as the set: Acceptable = {(Michelob, Schlitz), (Schlitz, Michelob), (Schlitz, Schlitz)}. If the probability of each outcome is 0.25, then because the outcomes are disjoint, the probability of “Acceptable” is P(Acceptable) = P(Michelob, Schlitz) + P(Schlitz, Michelob) + P(Schlitz, Schlitz) =0.25 + 0.25 + 0.25 = 0.75 Notice that because all of the outcomes are equally probably, the probability of an acceptable outcome reduces to the number of acceptable outcomes divided by the total number of possible outcomes. In this case, 3 out of 4, or 75%. Excercise: How would the probability of an acceptable outcome change if we believed that each taster had only a 40% chance of preferring Schlitz (and a 60% chance of preferring Michelob)? After all this work, you might be thinking: well that’s all fine and good, but it’s a lot of work to write out the sample space and list of acceptable outcomes for a sample size of 10 or even 100. You’d be right. For larger sample sizes, we need to employ some new techniques and R functions (which are only briefly covered in this text). But, for the sake of completeness, let’s briefly examine two ways to conceptualize the problem and calculate the analytical probabilities in R. Don’t worry if it doesn’t make perfect sense yet; we won’t make you do this by hand! The solution by counting, or combinatorics First consider the simple case above, where tasters are equally likely to prefer Schlitz or Michelob. Because all individual probabilities are equal, all joint probabilities are equal—e.g., the probability of (Michelob, Michelob, Schlitz, Michelob) is the same as that of (Schlitz, Michelob, Schlitz, Michelob). We can calculate the probability of an acceptable outcome (at least 40% preferring Schlitz) among 10 tasters by figuring out (i.e., counting) how many of the possible outcomes are acceptable outcomes. The total number of possible outcomes is \\(2^{10}\\) for a 10 person sample size (this is the same math as in the calculation of how many types of people you could observe by asking \\(10\\) independent, dichotomous questions). One acceptable outcome would be if everyone chose Schlitz. But it would still be acceptable if 9/10 of the tasters chose Schlitz, and there are 10 different ways that could happen, because the Michelob chooser could be in position 1…10 in the order. We are going to have to follow this logic down to 8, 7, 6, 5, and 4 out of 10 (since all would be acceptable). So we nned to dust off the old choose() function. If you’ve never seen it before: choose(n,k) (read “n choose k” and often written \\(n\\choose{k}\\)) is the number of possible ways to choose k items out of a group of n total items. In this context, choose(n=10,k=4) is the number of unique groups of 4 tasters among a total pool of 10 tasters (i.e., number of ways that exactly 4/10 tasters could prefer Schlitz). In R: #calculate choose(n=10,k=4) in R choose(n=10, k=4) ## [1] 210 #calculate choose(n=10,k= all the numbers between 4 and 10) choose(n=10, k=4:10) ## [1] 210 252 210 120 45 10 1 #add up all of the values above using sum() and then divide by 2^10 sum(choose(n=10, k=4:10))/2^10 ## [1] 0.828125 Excercise: Can you modify the above code to calculate the empirical probability of an acceptable outcome for a sample size of 100 (again assuming preference for Schlitz and Michelob are equally likely)? The solution by distribution, or statistics The above combinatoric method won’t work if the outcomes are not equally probable. We may want to account for different probabilities of preferring Schlitz or Michelob. Maybe it’s 43% to 57% and not 50/50. Now we will need to call upon the powers of the binomial probability distribution. This function and its relatives can answer questions such as what is the probability of 42 successes in 89 independent trials, each of which has a 0.44 chance of being successful. Of course those numbers are arbitrary. In R, the function dbinom(x,n,p) gives the probability of x “successes” in n independent random trials, where each random trial has probability of success = p. For example, dbinom(4,10,0.5) is the probability that exactly 4 out of 10 people prefer Schlitz if the probability of any individual preferring Schlitz is 0.5. Using this function, we can now repeat the calculation above using some slightly different code: # calculate a few individual values, e.g., dbinom(4,10,.5) dbinom(4,10,0.5) ## [1] 0.2050781 dbinom(5,10,0.5) ## [1] 0.2460938 dbinom(6,10,0.5) ## [1] 0.2050781 #add up the probabilities of 4,5,6,7,8,9, or 10 people preferring Schlitz sum(dbinom(4:10,10,0.5)) ## [1] 0.828125 Exercise 1: Can you explain why choose(n=10, k=4)/2^k is equal to dbinom(4,10,.5)? Exercise 2: Can you modify the code above to calculate the probability of an acceptable outcome if each taster only has a 40% chance of preferring Schlitz? If these calculations feel a little overwhelming and confusing at this point, don’t fear! Instead, revel in the fact that you just got (approximately) the same number using three different conceptualizations of the same problem. The point is: there are many ways to answer probabilistic questions, and simulation can be a powerful tool to side-step advanced probability calculations. "],["sec-randomness.html", "A Randomness A.1 Defining random processes A.2 Predictions about random events A.3 Modification for continuous observations", " A Randomness Whereas in common language, we may use the word “random” to mean surprising or unfamiliar, the concept has a more precise meaning in data science and statistics. Interestingly, there is no (formal) idea of randomness without probability. Just like there is no idea of straightness without the idea of space. What are the odds, right? The concept of randomness needs probability to define it, as we shall see. Moreover, randomness is an idea, like straightness, that has a pure ideal form. But what we observe in practice may be less than perfectly random (or straight). Consider straightness first. A straight line made using a ruler and a pencil is easily distinguished by eye from a wiggly hand-drawn line or a swooping curvy one. However, if we examine the pencil markings of a “straight line” under a magnifying glass, we can observe tiny wiggles at the edge. These wiggles may be due to the texture of the paper or the imperfections in the graphite (the pencil’s “lead”) or both. So the straight line is not actually perfectly straight. Nevertheless, we are able to hold in our minds a mathematical idea of a straight line. For example, we can define a line using math, specifically coordinate algebra and the \\((x,y)\\) Euclidean plane. You likely can recognize an equation of this form: \\[ y = 2 x + 3.\\] This formula, which expresses a linear functional relationship, assumes a fair amount of prior knowledge, which we typically learn in school. For example, that \\(x\\) and \\(y\\) can take on continuous, real-number values, that the \\(y\\) axis is perpendicular to the x-axis, etc. The line expressed in the formula is perfectly straight. Even if we draw it using only an approximately straight line with a pencil or chalk. YouTuber Vsauce’s video on “What is Random?” explores whether things that we take for granted as being random (e.g., coin flips) really are. If you watch this video, you may come away convinced the outcome of tossing a coin or rolling a die is not truly random, and that quantum mechanics is the only truly random mechanism in nature. Alternately you may be satisfied that the fact of having limited information is sufficient to justify the treatment of a coin toss as a random event. On this argument, as long as you can’t tell the difference between a coin and a truly random coin, you may, for all intents and purposes, treat is as perfectly random. A.1 Defining random processes As with straightness, we can at least define an ideal random process. For example, we can define an ideal coin toss using mathematical notation, although this notation is less commonly learned in school. Here’s one way to do it: \\[ x \\in \\{H, T\\} \\qquad P(x=H) = p\\] In this definition, \\(x\\) is not a real number but rather a discrete outcome of a dichotomous random process, i.e., the coin toss. The first part of the formula specifies that \\(x\\) can take on two possible values, heads (H) or tails (T). This is abbreviated here in set notation. We can read it as “\\(x\\) is an element of the set containing H and T.” The second part defines the probability that \\(x\\) is observed to have a value of H. The probability is \\(p\\), which is a parameter that stands in for some number between 0 and 1. For a fair coin, we would put \\(p=0.5\\). Although it is not written out explicitly, the probability of a coin coming up tails must be \\(1-p\\) due to the axioms of probability and the fact that there are only two possible outcomes. The axioms of probability say that the the probability of all possible disjoint outcomes must sum up to one. We could have used any discrete process besides a coin toss. For another example, we might think that whether we will pass the course we are taking or not is a random process. Suppose we give ourselves a 60% chance of passing. Then we can write: \\[ x \\in \\{Pass, Fail\\} \\qquad P(x=Pass) = 0.6\\] That’s all there is to it. We defined a random process, so it’s random! But that doesn’t mean all of this is intuitive. When we observe the outcome of a coin flip, we observe heads or tails, not both. We either pass the class or not. So what’s the evidence that the process was random? The only way to collect such evidence is to be able to observe the process occur many many times. Or at least imagine observing in many times. Randomness requires probability. By the way, we don’t have to stop at binary or dichotomous outcomes. For a fair six-sided die, we can write \\[ x \\in \\{1, 2, \\dots, 6\\} \\qquad P(x=n) = 1/6 \\quad \\forall n \\in \\{1, 2, \\dots, 6\\}. \\] The “forall” symbol (an upside down A) is a shorthand we use to indicate that all outcomes are equally probable, with probability of 1/6. We can read this part as, “the probability that \\(x\\) is observed to take the value \\(n\\) is equal to 1/6 for all \\(n\\) in the set containing the values 1, 2, 3, 4, 5, and 6.” What will the color be of the next car (excluding taxicabs) that crosses your path on the road? An idealized random description of that observation might look like this. Notice that I do not have a compact shortcut “forall” because the probabilities are not the same. \\[ x \\in \\{\\mbox{white, black, gray, silver, red, blue, other}\\} \\] \\[ P(x=\\mbox{white}) = 0.24 \\] \\[ P(x=\\mbox{black}) = 0.23 \\] \\[P(x=\\mbox{gray}) = 0.16 \\] \\[P(x=\\mbox{silver}) = 0.15 \\] \\[P(x=\\mbox{red}) = 0.10 \\] \\[P(x=\\mbox{blue}) = 0.09 \\] \\[P(x=\\mbox{other}) = 0.03 \\] A.2 Predictions about random events Perfectly random processes may not be very predictable on a case-by-case basis (unless the probability is close to 0 or 1), but good predictions can be made about the outcomes of many observations taken together. Suppose I predict that you will not be struck by lightning while reading this sentence. Was I right? Probably, because even though getting struck by lightning may be random, the probability is very close to 0. But if I predict that the next car that crosses your path will be black, I may be wrong most of the time even if my description above is an accurate one! If instead I predict that of the first 1000 people reading this sentence (and residing in the United States, on which I based my model) 23% will see a black car cross their path first, then my prediction should be close. How close? Well, quantifying error is part of what statistics is all about… The examples above, using two-sided coins, pass/fail, six-sided dice, and car colors might suggest to you how a general random process can be defined whenever there are a finite number of possible outcomes. We first define the set of all possible outcomes. Then, for each one, we define the probability that it occurs. The probabilities must still add up to one. This is a formal way of declaring that in our ideal random process, something, i.e., one of the possible outcomes, must occur. A.3 Modification for continuous observations When outcomes are continuous and not discrete, the above definitions need to be modified slightly. For example, suppose you were interested in exactly how long it will take before a black car crosses your path. (And suppose you have a clock so precise that there is no practical limit on how fine a time difference you can observe. Any fraction of a second is possible.) By the very nature of continuous measures, you cannot enumerate them, so you can’t go one by one and declare what the probability of each possible outcome is. Think about it: you can count seconds (one, two, three,…) but you can’t count infinitessimal fractions of seconds. Instead, you have to limit what you can say to things like, “the probability that the outcome (time to crossing by black car) is in some range is…” The choice of range can be totally arbitrary, say, the probability that it will take between 37 and 49 seconds. Or, you can turn the continous outcome into a discrete set of ranges (e.g., less than a minute, between one and five minutes, five to ten minutes, or longer than ten minutes) and then proceed as before. You will see this kind of thing done a lot. The formal definition of random processes for continuous variables uses calculus. "],["references.html", "References", " References American Educational Research Association and American Psychological Association and National Council on Measurement in Education and Joint Committee on Standards for Educational and Psychological Testing. 1999. Standards for Educational and Psychological Testing. Amer Educational Research Assn. Kaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in Data: An Introduction to Cluster Analysis. Vol. 344. John Wiley &amp; Sons. MacCallum, Robert C, Shaobo Zhang, Kristopher J Preacher, and Derek D Rucker. 2002. “On the Practice of Dichotomization of Quantitative Variables.” Psychological Methods 7 (1): 19. Pittenger, David J. 1993. “Measuring the MBTI... And Coming up Short.” Journal of Career Planning and Employment 54 (1): 48–52. "]]
